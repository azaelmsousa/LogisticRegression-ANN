{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a multi class classification of the samples\n",
    "# regarding an optimized theta\n",
    "# ToDo\n",
    "def ClassifyMultiClass(theta,X,th,nmodels):\n",
    "\tX = np.insert(X,0,1,axis=1)\n",
    "\ty = Hypothesis(theta,X)\n",
    "\ty[y >= th] = 1\n",
    "\ty[y < th] = 0\n",
    "\tX = np.delete(X,0,axis=1)\n",
    "\treturn y\n",
    "\n",
    "# Apply the binary classification of the samples\n",
    "# regarding an optimized theta\n",
    "def Classify(theta,X,th):\n",
    "\tX = np.insert(X,0,1,axis=1)\n",
    "\ty = Hypothesis(theta,X)\n",
    "\ty[y >= th] = 1\n",
    "\ty[y < th] = 0\n",
    "\tX = np.delete(X,0,axis=1)\n",
    "\treturn y\n",
    "\n",
    "def Predict(theta,X):\n",
    "\tX = np.insert(X,0,1,axis=1)\n",
    "\ty = Hypothesis(theta,X)\n",
    "\tX = np.delete(X,0,axis=1)\n",
    "\treturn y\n",
    "\n",
    "'''\n",
    " Since we are dealing with logistic regression,\n",
    " the hypothesis is defined as:\n",
    "\n",
    "                    1\n",
    "       F(x) = --------------\n",
    "                1 + exp^(-x)\n",
    "\n",
    " However, its implementation may result in overflow\n",
    " if x is too large, then, the version implemented \n",
    " here is more stable with similar results.\n",
    "'''\n",
    "def Hypothesis(theta,X):\n",
    "\tcX = np.copy(X)\n",
    "\tcX[X > 0] = -cX[X > 0]\n",
    "\th = 1 / (1 + np.exp(np.dot(cX,theta)))\n",
    "\treturn h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------\n",
    "#   Evaluation Metrics and Loss Functions\n",
    "#-----------------------------------\n",
    "\n",
    "def CrossEntropyLoss(h,y):\n",
    "\treturn np.multiply(np.log(h),y) + np.multiply((1-np.log(h)),(1-y))\n",
    "\n",
    "def Cost(theta,X,y):\n",
    "\th = Hypothesis(theta,X)\n",
    "\tcost = CrossEntropyLoss(h,y)\n",
    "\tmean_cost = cost.sum()/-y.shape[0]\n",
    "\treturn mean_cost\n",
    "\n",
    "def AccuracyScore(predY,Y):\n",
    "\tTP = ((predY == Y) & (predY == 1.)).sum()\n",
    "\tTN = ((predY == Y) & (predY == 0.)).sum()\t\n",
    "\tacc = (TP + TN) / predY.shape[0]\n",
    "\treturn acc\n",
    "\n",
    "def PrecisionScore(predY,Y):\n",
    "\tTP = ((predY == Y) & (predY == 1)).sum()\n",
    "\tFP = ((predY != Y) & (predY == 1)).sum()\n",
    "\tprecision = TP / (TP + FP)\n",
    "\treturn precision\n",
    "\n",
    "def RecallScore(predY, Y):\n",
    "\tTP = ((predY == Y) & (predY == 1)).sum()\n",
    "\tFN = ((predY != Y) & (predY == 0)).sum()\n",
    "\trecall = TP / (TP + FN)\n",
    "\treturn recall\n",
    "\n",
    "def FbScore(predY, Y, beta):\n",
    "\tprecision = PrecisionScore(predY,Y)\n",
    "\trecall = RecallScore(predY,Y)\n",
    "\tfscore = (1 + beta*beta)*((precision*recall)/((beta*beta*precision)+recall))\n",
    "\treturn fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------\n",
    "#   Gradient Descent\n",
    "#-----------------------------------\n",
    "\n",
    "def BGD(X,y,alpha,iterations):\n",
    "\n",
    "\tX = np.insert(X,0,1,axis=1)\n",
    "\n",
    "\tnsamples = X.shape[0]\n",
    "\tnfeatures = X.shape[1]\n",
    "\ttheta = np.zeros(nfeatures)\n",
    "\tJ=[]\t\n",
    "\n",
    "\tfor i in range(iterations):\n",
    "\n",
    "\t\th = Hypothesis(theta,X)\n",
    "\n",
    "\t\terror = h - y\n",
    "\n",
    "\t\tgrad = np.dot(X.transpose(),error)/nsamples\n",
    "\n",
    "\t\ttheta = theta - alpha*grad\n",
    "\n",
    "\t\tJ.append(Cost(theta,X,y))\t\t\n",
    "\n",
    "\tX = np.delete(X,0,axis=1)\n",
    "\n",
    "\tplt.plot(J)\n",
    "\tplt.ylabel('Error')\n",
    "\tplt.xlabel('iterations')\n",
    "\tplt.show()\n",
    "\n",
    "\treturn theta,J[iterations-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  34   34   34]\n",
      " [  45   45   45]\n",
      " [  88   88   88]\n",
      " ...\n",
      " [9200 9200 9200]\n",
      " [2800 2800 2800]\n",
      " [4000 4000 4000]]\n",
      "\n",
      "--- X\n",
      "[[  34   34   34]\n",
      " [  45   45   45]\n",
      " [  88   88   88]\n",
      " ...\n",
      " [9200 9200 9200]\n",
      " [2800 2800 2800]\n",
      " [4000 4000 4000]]\n",
      "\n",
      "--- y\n",
      "[0. 0. 0. ... 1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Theta\n",
      "[-2.81974846  0.00393608  0.00393608  0.00393608]\n",
      "\n",
      "--- Error\n",
      "-1.6520852565401747\n",
      "\n",
      "--- Hypothesis\n",
      "[1.         1.         1.         1.         0.99999977 1.\n",
      " 1.         0.99998707 0.99797821 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.99992834 1.         0.96432299 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.99725365 1.         1.         1.\n",
      " 0.06792505 1.         0.99870824 1.         1.         0.99999994\n",
      " 0.99481655 1.         0.94703133 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99999999\n",
      " 1.         1.         1.         0.99974047 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.13849385 0.98514345 1.         1.         1.         1.\n",
      " 0.99999503 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.9999995  1.\n",
      " 1.         1.         0.99897969 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         0.49765318 1.         1.\n",
      " 1.         1.         1.         0.99911437 0.99999999 1.\n",
      " 1.         1.         1.         1.         0.64671741 0.9595745\n",
      " 0.6911244  1.         1.         1.         0.99999984 1.\n",
      " 1.         1.         1.         0.99965951 0.99992918 1.\n",
      " 1.         1.         1.         1.         0.99995531 1.\n",
      " 0.95473704 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.98251944 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99999999\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.99999014 1.         0.99956885 1.\n",
      " 0.08821633 1.         1.         0.99999999 1.         1.\n",
      " 1.         1.         1.         0.99068412 1.         1.\n",
      " 0.99999434 1.         1.         1.         1.         0.98479381\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.66008709 1.\n",
      " 0.99999998 1.         0.99705261 1.         1.         1.\n",
      " 0.99668442 1.         1.         1.         1.         0.78605828\n",
      " 0.91122172 1.         0.42155296 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.99698239 1.         1.         1.         0.99999996\n",
      " 0.63585263 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.86067287 0.99999999 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.22902446 0.99999999 1.         1.         1.\n",
      " 0.99995315 1.         1.         1.         1.         1.\n",
      " 0.99999962 1.         1.         0.99999999 1.         1.\n",
      " 1.         1.         1.         1.         0.25499843 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.20496829 0.99928383 0.9999998  1.         0.93458035 1.\n",
      " 1.         1.         1.         1.         1.         0.99999275\n",
      " 1.         1.         0.96924387 0.77800781 1.         1.\n",
      " 1.         1.         0.99777835 1.         1.         1.\n",
      " 1.         1.         1.         0.99672321 0.71091809 1.\n",
      " 1.         1.         1.         1.         0.99999967 1.\n",
      " 1.         0.89799449 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99889186\n",
      " 1.         0.10800779 0.99996029 1.         1.         1.\n",
      " 1.         1.         1.         0.50355723 0.99999503 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.99999612 1.         1.         0.9999997\n",
      " 1.         1.         0.99999995 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.27101653 1.         1.         1.         1.\n",
      " 0.61929543 1.         1.         1.         0.99999092 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99999993\n",
      " 1.         0.11745151 0.99761563 0.99999995 0.9999866  1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.54183602 1.\n",
      " 0.99999715 1.         1.         1.         0.9662995  1.\n",
      " 0.97665669 1.         1.         1.         1.         0.99664517\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.99999675 0.99937839 1.         1.         1.\n",
      " 1.         1.         0.21876652 1.         1.         1.\n",
      " 1.         1.        ]\n",
      "\n",
      "--- Classification\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n",
      "\n",
      "--- Expected Output\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "--- Accuracy\n",
      "0.972\n",
      "\n",
      "--- Precision\n",
      "1.0\n",
      "\n",
      "--- Recall\n",
      "0.9717171717171718\n",
      "\n",
      "--- F1Score\n",
      "0.9856557377049181\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------\n",
    "#   Logistic Regression - Toy Example\n",
    "#-----------------------------------\n",
    "\n",
    "size=5000\n",
    "size2=size//2\n",
    "size10=size//10\n",
    "size102=size10//2\n",
    "X = np.random.randint(low=1,high=99, size=(size,1))\n",
    "X = np.concatenate((X,X,X),axis=1)\n",
    "X = np.multiply(X,np.concatenate((np.full((size2,1),1),np.full((size2,1),100))))\n",
    "print(X)\n",
    "y = np.empty((size))\n",
    "y[X[:,1] <= 99]=0\n",
    "y[X[:,1] >= 100]=1\n",
    "\n",
    "print(\"\\n--- X\")\n",
    "print(X)\n",
    "print(\"\\n--- y\")\n",
    "print(y)\n",
    "\n",
    "theta,error = BGD(X,y,0.001,10000)\n",
    "print(\"\\n--- Theta\")\n",
    "print(theta)\n",
    "print(\"\\n--- Error\")\n",
    "print(error)\n",
    "\n",
    "\n",
    "X_val = np.random.randint(low=1,high=9900, size=(size10,1))\n",
    "X_val = np.concatenate((X_val,X_val,X_val),axis=1)\n",
    "y_val = np.empty((size10))\n",
    "y_val[X_val[:,1] <= 99]=0\n",
    "y_val[X_val[:,1] >= 100]=1\n",
    "\n",
    "\n",
    "h = Predict(theta,X_val)\n",
    "print(\"\\n--- Hypothesis\")\n",
    "print(h)\n",
    "predY = Classify(theta,X_val,0.7)\n",
    "print(\"\\n--- Classification\")\n",
    "print(predY)\n",
    "print(\"\\n--- Expected Output\")\n",
    "print(y_val)\n",
    "\n",
    "acc = AccuracyScore(predY,y_val)\n",
    "pre = PrecisionScore(predY,y_val)\n",
    "recall = RecallScore(predY,y_val)\n",
    "f = FbScore(predY,y_val,1)\n",
    "\n",
    "print(\"\\n--- Accuracy\")\n",
    "print(acc)\n",
    "print(\"\\n--- Precision\")\n",
    "print(pre)\n",
    "print(\"\\n--- Recall\")\n",
    "print(recall)\n",
    "print(\"\\n--- F1Score\")\n",
    "print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------\n",
    "# MultiClass Classification - Toy Example\n",
    "#-----------------------------------\n",
    "\n",
    "# Not ready, still has a bug\n",
    "\n",
    "size=500\n",
    "size4=size//4\n",
    "size10=size//10\n",
    "size104=size10//4\n",
    "X = np.random.randint(low=0,high=7999, size=(size,1))\n",
    "X = np.concatenate((X,X,X),axis=1)\n",
    "y = np.empty((size),dtype=np.float128)\n",
    "y[X[:,1] <= 1999]=0\n",
    "y[((X[:,1] <= 3999) & (X[:,1]>1999))]=1\n",
    "y[((X[:,1] <= 5999) & (X[:,1]>3999))]=2\n",
    "y[X[:,1]>5999]=3\n",
    "\n",
    "classes = np.unique(y)\n",
    "print(classes)\n",
    "theta = {}\n",
    "\n",
    "for c in classes:\n",
    "\n",
    "\tcy = np.copy(y)\n",
    "\n",
    "\tcy[y != c] = 0\n",
    "\tcy[y == c] = 1\n",
    "\n",
    "\ttheta[c],acc = BGD(X,cy,0.001,500)\n",
    "\n",
    "X_val = np.random.randint(low=1,high=7999, size=(size10,1))\n",
    "X_val = np.concatenate((X_val,X_val,X_val),axis=1)\n",
    "y_val = np.empty((size10))\n",
    "y_val[X_val[:,1] <= 1999]=0\n",
    "y_val[((X_val[:,1] <= 3999) & (X_val[:,1]>1999))]=1\n",
    "y_val[((X_val[:,1] <= 5999) & (X_val[:,1]>3999))]=2\n",
    "y_val[X_val[:,1]>5999]=3\n",
    "\n",
    "print(\"\\n--- Theta\")\n",
    "print(theta)\n",
    "\n",
    "predY,pred = ClassifyMultiClass(theta,X_val)\n",
    "print(\"\\n--- Hypothesis\")\n",
    "print(pred)\n",
    "print(\"\\n--- Classification\")\n",
    "print(predY)\n",
    "print(\"\\n--- Expected Output\")\n",
    "print(y_val)\n",
    "\n",
    "acc = AccuracyScore(predY,y_val)\n",
    "pre = PrecisionScore(predY,y_val)\n",
    "recall = RecallScore(predY,y_val)\n",
    "f = FbScore(predY,y_val,1)\n",
    "\n",
    "print(\"\\n--- Accuracy\")\n",
    "print(acc)\n",
    "print(\"\\n--- Precision\")\n",
    "print(pre)\n",
    "print(\"\\n--- Recall\")\n",
    "print(recall)\n",
    "print(\"\\n--- F1Score\")\n",
    "print(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
