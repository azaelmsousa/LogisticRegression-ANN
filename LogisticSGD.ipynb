{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " Since we are dealing with logistic regression,\n",
    " the hypothesis is defined as:\n",
    "\n",
    "                    1\n",
    "       F(x) = ----------------\n",
    "                1 + exp^(-x)\n",
    "\n",
    " However, its implementation may result in overflow\n",
    " if x is too large, then, the version implemented \n",
    " here is more stable with similar results, and is\n",
    " defined as:\n",
    " \n",
    "                  exp^(x)\n",
    "       F(x) = ----------------, if x < 0\n",
    "                1 + exp^(x) \n",
    "                \n",
    "                    1\n",
    "       F(x) = ----------------, if x >= 0\n",
    "                1 + exp^(-x) \n",
    "'''\n",
    "def hypothesis(theta,X,stable=False):\n",
    "    \n",
    "    dot = np.dot(X,theta)\n",
    "    \n",
    "    #Regular Sigmoid Function        \n",
    "    if (stable == False):        \n",
    "        h = 1 / (1 + np.exp(-dot))\n",
    "    \n",
    "    else:\n",
    "    #Stable Sigmoid Function\n",
    "        num = (dot >= 0).astype(np.float128)\n",
    "        dot[dot >= 0] = -dot[dot >= 0]\t\n",
    "        exp = np.exp(dot)\n",
    "        num = np.multiply(num,exp)\n",
    "        h = num / (1 + exp)\n",
    "    \n",
    "    return h\n",
    "\n",
    "# Apply a multi class classification of the samples\n",
    "# regarding an optimized theta\n",
    "# ToDo\n",
    "def classify_multiclass(theta , X , th, nmodels):\n",
    "    # inserting the X^0 coeficient\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    # Given a theta collection calculate the hyphotesis\n",
    "    y = hypothesis(theta, X)\n",
    "    y[y >= th] = 1\n",
    "    y[y < th] = 0\n",
    "    X = np.delete(X, 0, axis=1)\n",
    "    return y\n",
    "\n",
    "# Given a threshold apply a \n",
    "# binary classification of the samples\n",
    "# regarding an optimized theta\n",
    "def classify(theta, X, th):\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    y = hypothesis(theta, X)\n",
    "    y[y >= th] = 1\n",
    "    y[y < th] = 0\n",
    "    X = np.delete(X, 0, axis=1)\n",
    "    return y\n",
    "\n",
    "def predict(theta,X):\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    y = hypothesis(theta, X)\n",
    "    X = np.delete(X, 0, axis=1)\n",
    "    return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------\n",
    "#   Evaluation Metrics and Loss Functions\n",
    "#-----------------------------------\n",
    "\n",
    "def cross_entropy_loss(h, y):\n",
    "    # y.log(h) + (1-log(h) . 1-y)\n",
    "    # log probability * inverse of the log probabality \n",
    "\teps = np.finfo(np.float).eps\n",
    "\th[h < eps] = eps\n",
    "\th[h > 1.-eps] = 1.-eps\n",
    "\treturn np.multiply(np.log(h),y) + np.multiply((np.log(1-h)),(1-y))\n",
    "\n",
    "def cost(theta, X, y):\n",
    "    h = hypothesis(theta, X)\n",
    "    cost = cross_entropy_loss(h, y)\n",
    "    mean_cost = cost.sum() / -y.shape[0]\n",
    "    return mean_cost\n",
    "\n",
    "def accuracy_score(predY, Y):\n",
    "    TP = ((predY == Y) & (predY == 1.)).sum()\n",
    "    TN = ((predY == Y) & (predY == 0.)).sum()\n",
    "    acc = (TP + TN) / predY.shape[0]\n",
    "    return acc\n",
    "\n",
    "def precision_score(predY,Y):\n",
    "    TP = ((predY == Y) & (predY == 1)).sum()\n",
    "    FP = ((predY != Y) & (predY == 1)).sum()\n",
    "    precision = TP / (TP + FP)\n",
    "    return precision\n",
    "\n",
    "def recall_score(predY, Y):\n",
    "    TP = ((predY == Y) & (predY == 1)).sum()\n",
    "    FN = ((predY != Y) & (predY == 0)).sum()\n",
    "    recall = TP / (TP + FN)\n",
    "    return recall\n",
    "\n",
    "def f1_score(predY, Y, beta):\n",
    "    precision = precision_score(predY, Y)\n",
    "    recall = recall_score(predY, Y)\n",
    "    fscore = (1 + beta * beta) * ((precision * recall) / ((beta * beta * precision) + recall))\n",
    "    return fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------\n",
    "#   Gradient Descent\n",
    "#-----------------------------------\n",
    "\n",
    "def BGD(X, y, alpha, iterations):\n",
    "\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "    nsamples = X.shape[0]\n",
    "    nfeatures = X.shape[1]\n",
    "    theta = np.zeros(nfeatures)\n",
    "    J=[]\n",
    "\n",
    "    for i in range(iterations):\n",
    "\n",
    "        h = hypothesis(theta,X)\n",
    "\n",
    "        error = h - y\n",
    "\n",
    "        grad = np.dot(X.transpose(),error)/nsamples\n",
    "\n",
    "        theta = theta - alpha * grad\n",
    "\n",
    "        J.append(cost(theta,X,y))\n",
    "#         J.append(error)\n",
    "\n",
    "    X = np.delete(X,0,axis=1)\n",
    "    plt.figure()\n",
    "    plt.plot(J)    \n",
    "    plt.ylabel('Error')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.show()\n",
    "\n",
    "    return theta, J[iterations-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- X\n",
      "[[  43   43   43]\n",
      " [  31   31   31]\n",
      " [  81   81   81]\n",
      " ...\n",
      " [4600 4600 4600]\n",
      " [4100 4100 4100]\n",
      " [7200 7200 7200]]\n",
      "\n",
      "--- y\n",
      "[0. 0. 0. ... 1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Theta\n",
      "[-0.02409624  0.0006008   0.0006008   0.0006008 ]\n",
      "\n",
      "--- Error\n",
      "0.38661647595835424\n",
      "\n",
      "--- Hypothesis\n",
      "[0.99410172 0.99880846 0.76879855 0.9999996  0.98456682 0.99999995\n",
      " 0.99999996 0.99976617 0.9999991  0.99999856 0.65089313 0.99989701\n",
      " 0.95975023 0.85526501 0.99997766 0.99999997 0.94834934 0.99997068\n",
      " 0.98183383 0.99997511 0.99999992 0.96422697 0.99999083 0.9999984\n",
      " 0.96441301 0.91642471 0.9999999  0.99999947 0.99818119 0.99975229\n",
      " 0.99997315 0.96296253 0.99629368 0.98080994 0.99784224 0.99999547\n",
      " 0.99997489 0.99867733 0.99383154 0.9999696  0.99999352 0.99997057\n",
      " 0.99997125 0.99999998 0.99999997 0.98952887 0.9997007  0.99985469\n",
      " 0.99995672 0.99999734 0.999997   0.53133474 0.99919675 0.99999997\n",
      " 0.99986284 0.98939733 0.99999898 0.92459493 0.99999384 0.99999911\n",
      " 0.99937127 0.99736559 0.5083944  0.99655679 0.97464769 0.99952014\n",
      " 0.99999721 0.97603452 0.99999922 0.9378336  0.9999994  0.99997429\n",
      " 0.9999959  0.99087339 0.54656101 0.99999616 0.99998839 0.85704079\n",
      " 0.99999916 0.99991116 0.99841366 0.99999891 0.99962374 0.99999994\n",
      " 0.99982818 0.99860143 0.89647125 0.9995421  0.99999993 0.9148936\n",
      " 0.89134412 0.99999989 0.99999987 0.99999992 0.99999871 0.97047067\n",
      " 0.79985445 0.99996313 0.99970178 0.99992727 0.99999964 0.99999876\n",
      " 0.99999996 0.99999976 0.69754852 0.99999998 0.99999988 0.93416344\n",
      " 0.99979938 0.99999962 0.99997301 0.99983634 0.99482514 0.99999996\n",
      " 0.99878682 0.99999934 0.89829695 0.99986722 0.96064569 0.98881482\n",
      " 0.99999444 0.99987215 0.99942231 0.99996526 0.9999996  0.99999857\n",
      " 0.99999976 0.99002128 0.99374251 0.99999997 0.99961759 0.9999959\n",
      " 0.99962644 0.99999581 0.93075879 0.79016256 0.99999966 0.9999999\n",
      " 0.79578428 0.9999933  0.99993959 0.99998794 0.99995382 0.99999995\n",
      " 0.99874912 0.94131201 0.99992984 0.99999883 0.99999487 0.99999889\n",
      " 0.98025956 0.99999881 0.99993085 0.99999994 0.99970604 0.99999957\n",
      " 0.99999929 0.63809208 0.99667829 0.9999491  0.7550701  0.99537805\n",
      " 0.99999844 0.7493592  0.94646354 0.99236716 0.99999975 0.86204519\n",
      " 0.99267484 0.99999845 0.99996412 0.99968064 0.99999975 0.76911877\n",
      " 0.99762219 0.99999452 0.84913618 0.93959623 0.97387946 0.99997595\n",
      " 0.99999497 0.75772696 0.97881505 0.99999941 0.99999792 0.99999909\n",
      " 0.99999544 0.99998181 0.99987076 0.99998445 0.99998298 0.73662666\n",
      " 0.57584608 0.99999933 0.99999998 0.99999993 0.99998937 0.99571358\n",
      " 0.99586472 0.99999997 0.99339714 0.91531368 0.97722722 0.99869859\n",
      " 0.99999964 0.99999998 0.70547381 0.99655679 0.96777561 0.99968464\n",
      " 0.99883603 0.99991523 0.99843352 0.99981198 0.99992281 0.96838822\n",
      " 0.99994627 0.99360657 0.99996267 0.98941622 0.91752248 0.9999084\n",
      " 0.99999957 0.99999998 0.80725126 0.95101868 0.98863398 0.72779193\n",
      " 0.99999778 0.99914449 0.9988506  0.99999739 0.9999999  0.99960994\n",
      " 0.99767713 0.91860711 0.99449003 0.99979499 0.99999975 0.72600289\n",
      " 0.99952444 0.99522637 0.99997573 0.99999867 0.99999834 0.99352592\n",
      " 0.99978477 0.96657408 0.78806295 0.9996886  0.99989867 0.99999971\n",
      " 0.9998909  0.65782311 0.99970498 0.99999991 0.97805451 0.99651948\n",
      " 0.99997182 0.99947585 0.78043936 0.99999989 0.51334863 0.99995272\n",
      " 0.99610957 0.99999455 0.95954083 0.99999989 0.78290016 0.99963902\n",
      " 0.99999985 0.90328922 0.99999945 0.61660672 0.99999411 0.9999581\n",
      " 0.99999971 0.99998063 0.99999988 0.99995423 0.99999714 0.96794382\n",
      " 0.99947396 0.99999987 0.53088588 0.97738711 0.9999645  0.99714878\n",
      " 0.99992714 0.99376489 0.99995521 0.99999951 0.68757118 0.99801359\n",
      " 0.99999994 0.999994   0.99785385 0.96882666 0.99981232 0.97364923\n",
      " 0.81768465 0.99973378 0.74868154 0.999772   0.58155869 0.99999994\n",
      " 0.99999973 0.98491885 0.99981896 0.99999993 0.99999649 0.9986239\n",
      " 0.99982156 0.99999905 0.99772263 0.89134412 0.99999848 0.99938471\n",
      " 0.94507696 0.56214436 0.99565939 0.9718338  0.99999995 0.99764771\n",
      " 0.99999975 0.98240354 0.99999977 0.95434854 0.98502557 0.99999414\n",
      " 0.99999065 0.80976272 0.99999477 0.99999813 0.99999997 0.64513764\n",
      " 0.99994694 0.99999944 0.99999937 0.99999997 0.99890119 0.98703328\n",
      " 0.99700702 0.99982347 0.99999992 0.91670039 0.99994928 0.99943872\n",
      " 0.99999984 0.98437393 0.99996192 0.99999946 0.99553296 0.87161253\n",
      " 0.99788831 0.99999966 0.99999909 0.99999994 0.99775919 0.98996772\n",
      " 0.99999913 0.70808853 0.99999998 0.99999964 0.99958455 0.99922378\n",
      " 0.99997869 0.99999774 0.99998524 0.99981567 0.96030357 0.99999992\n",
      " 0.99999651 0.99999955 0.99299532 0.99999966 0.99999973 0.9846215\n",
      " 0.99990538 0.6339195  0.99999925 0.9766999  0.99999977 0.99996023\n",
      " 0.99999298 0.99999876 0.99059208 0.99997914 0.99999626 0.50704292\n",
      " 0.99999982 0.99999923 0.99999177 0.99999801 0.49487732 0.68485447\n",
      " 0.99992921 0.99999986 0.99999936 0.99999993 0.99991052 0.66629136\n",
      " 0.50163616 0.95059717 0.99972796 0.99911631 0.96580871 0.99983634\n",
      " 0.8174158  0.99999881 0.99999338 0.86311344 0.99957008 0.99999972\n",
      " 0.99997872 0.9999987  0.999999   0.99998704 0.9999991  0.99999111\n",
      " 0.94488955 0.9999449  0.99999162 0.99981634 0.99798842 0.99999701\n",
      " 0.99998589 0.95940065 0.55681371 0.90249909 0.64307178 0.99999926\n",
      " 0.93504472 0.51604989 0.99999704 0.99159345 0.99999681 0.99838485\n",
      " 0.78655386 0.99999678 0.99993305 0.94010569 0.99999652 0.99494438\n",
      " 0.99999986 0.99999128 0.99985574 0.99994529 0.99999997 0.99919386\n",
      " 0.99988061 0.99999975 0.99953293 0.80300956 0.94346464 0.99999994\n",
      " 0.99996333 0.99999798 0.53178354 0.98570131 0.99983456 0.98877488\n",
      " 0.99997542 0.99969361 0.99999567 0.99999025 0.99996893 0.99623329\n",
      " 0.99999981 0.99999923 0.99997207 0.94682768 0.97289914 0.99986793\n",
      " 0.99960356 0.69754852 0.99997461 0.99999907 0.97599232 0.99865088\n",
      " 0.99999138 0.9999823  0.99999893 0.99999983 0.99998976 0.99999996\n",
      " 0.99994704 0.99475978 0.9999978  0.99999839 0.99995521 0.99974824\n",
      " 0.99999288 0.99999464 0.99999992 0.9999999  0.99999319 0.99999985\n",
      " 0.9378336  0.99262223]\n",
      "\n",
      "--- Classification\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "--- Expected Output\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "--- Accuracy\n",
      "0.966\n",
      "\n",
      "--- Precision\n",
      "1.0\n",
      "\n",
      "--- Recall\n",
      "0.9653767820773931\n",
      "\n",
      "--- F1Score\n",
      "0.9823834196891191\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------\n",
    "#   Logistic Regression - Toy Example\n",
    "#-----------------------------------\n",
    "\n",
    "size=5000\n",
    "size2=size//2\n",
    "size10=size//10\n",
    "size102=size10//2\n",
    "\n",
    "X = np.random.randint(low=1, high=99, size=(size,1))\n",
    "X = np.concatenate((X, X, X), axis=1)\n",
    "X = np.multiply(X,np.concatenate((np.full((size2,1),1), np.full((size2,1),100))))\n",
    "\n",
    "y = np.empty((size))\n",
    "y[X[:,1] <= 99]=0\n",
    "y[X[:,1] >= 100]=1\n",
    "\n",
    "print(\"\\n--- X\")\n",
    "print(X)\n",
    "print(\"\\n--- y\")\n",
    "print(y)\n",
    "\n",
    "theta,error = BGD(X, y, 0.00001, 10000)\n",
    "print(\"\\n--- Theta\")\n",
    "print(theta)\n",
    "print(\"\\n--- Error\")\n",
    "print(error)\n",
    "\n",
    "\n",
    "X_val = np.random.randint(low=1, high=9900, size=(size10,1))\n",
    "X_val = np.concatenate((X_val, X_val, X_val),axis=1)\n",
    "y_val = np.empty((size10))\n",
    "y_val[X_val[:,1] <= 99]=0\n",
    "y_val[X_val[:,1] >= 100]=1\n",
    "\n",
    "\n",
    "h = predict(theta,X_val)\n",
    "print(\"\\n--- Hypothesis\")\n",
    "print(h)\n",
    "predY = classify(theta,X_val,0.7)\n",
    "print(\"\\n--- Classification\")\n",
    "print(predY)\n",
    "print(\"\\n--- Expected Output\")\n",
    "print(y_val)\n",
    "\n",
    "acc = accuracy_score(predY, y_val)\n",
    "pre = precision_score(predY, y_val)\n",
    "recall = recall_score(predY, y_val)\n",
    "f = f1_score(predY, y_val, 1)\n",
    "\n",
    "print(\"\\n--- Accuracy\")\n",
    "print(acc)\n",
    "print(\"\\n--- Precision\")\n",
    "print(pre)\n",
    "print(\"\\n--- Recall\")\n",
    "print(recall)\n",
    "print(\"\\n--- F1Score\")\n",
    "print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------\n",
    "# MultiClass Classification - Toy Example\n",
    "#-----------------------------------\n",
    "\n",
    "# Not ready, still has a bug\n",
    "\n",
    "size=500\n",
    "size4=size//4\n",
    "size10=size//10\n",
    "size104=size10//4\n",
    "X = np.random.randint(low=0,high=7999, size=(size,1))\n",
    "X = np.concatenate((X,X,X),axis=1)\n",
    "y = np.empty((size),dtype=np.float128)\n",
    "y[X[:,1] <= 1999]=0\n",
    "y[((X[:,1] <= 3999) & (X[:,1]>1999))]=1\n",
    "y[((X[:,1] <= 5999) & (X[:,1]>3999))]=2\n",
    "y[X[:,1]>5999]=3\n",
    "\n",
    "classes = np.unique(y)\n",
    "print(classes)\n",
    "theta = {}\n",
    "\n",
    "for c in classes:\n",
    "\n",
    "\tcy = np.copy(y)\n",
    "\n",
    "\tcy[y != c] = 0\n",
    "\tcy[y == c] = 1\n",
    "\n",
    "\ttheta[c],acc = BGD(X,cy,0.001,500)\n",
    "\n",
    "X_val = np.random.randint(low=1,high=7999, size=(size10,1))\n",
    "X_val = np.concatenate((X_val,X_val,X_val),axis=1)\n",
    "y_val = np.empty((size10))\n",
    "y_val[X_val[:,1] <= 1999]=0\n",
    "y_val[((X_val[:,1] <= 3999) & (X_val[:,1]>1999))]=1\n",
    "y_val[((X_val[:,1] <= 5999) & (X_val[:,1]>3999))]=2\n",
    "y_val[X_val[:,1]>5999]=3\n",
    "\n",
    "print(\"\\n--- Theta\")\n",
    "print(theta)\n",
    "\n",
    "predY, pred = classify_multiclass(theta,X_val)\n",
    "print(\"\\n--- Hypothesis\")\n",
    "print(pred)\n",
    "print(\"\\n--- Classification\")\n",
    "print(predY)\n",
    "print(\"\\n--- Expected Output\")\n",
    "print(y_val)\n",
    "\n",
    "acc = accuracy_score(predY, y_val)\n",
    "pre = precision_score(predY, y_val)\n",
    "recall = recall_score(predY, y_val)\n",
    "f = f1_score(predY, y_val, 1)\n",
    "\n",
    "print(\"\\n--- Accuracy\")\n",
    "print(acc)\n",
    "print(\"\\n--- Precision\")\n",
    "print(pre)\n",
    "print(\"\\n--- Recall\")\n",
    "print(recall)\n",
    "print(\"\\n--- F1Score\")\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
