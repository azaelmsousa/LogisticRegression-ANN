{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " Since we are dealing with logistic regression,\n",
    " the hypothesis is defined as:\n",
    "\n",
    "                    1\n",
    "       F(x) = --------------\n",
    "                1 + exp^(-x)\n",
    "\n",
    " However, its implementation may result in overflow\n",
    " if x is too large, then, the version implemented \n",
    " here is more stable with similar results.\n",
    "'''\n",
    "def hypothesis(theta,X):\n",
    "    cX = np.copy(X)\n",
    "    '''\n",
    "                    1\n",
    "       F(x) = --------------\n",
    "                1 + exp^(-x)\n",
    "    '''    \n",
    "    cX[X > 0] = -cX[X > 0]\n",
    "    h = 1 / (1 + np.exp(np.dot(cX,theta)))\n",
    "\n",
    "    return h\n",
    "\n",
    "# Apply a multi class classification of the samples\n",
    "# regarding an optimized theta\n",
    "# ToDo\n",
    "def classify_multiclass(theta , X , th, nmodels):\n",
    "    # inserting the X^0 coeficient\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    # Given a theta collection calculate the hyphotesis\n",
    "    y = hypothesis(theta, X)\n",
    "    y[y >= th] = 1\n",
    "    y[y < th] = 0\n",
    "    X = np.delete(X, 0, axis=1)\n",
    "    return y\n",
    "\n",
    "# Given a threshold apply a \n",
    "# binary classification of the samples\n",
    "# regarding an optimized theta\n",
    "def classify(theta, X, th):\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    y = hypothesis(theta, X)\n",
    "    y[y >= th] = 1\n",
    "    y[y < th] = 0\n",
    "    X = np.delete(X, 0, axis=1)\n",
    "    return y\n",
    "\n",
    "def predict(theta,X):\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    y = hypothesis(theta, X)\n",
    "    X = np.delete(X, 0, axis=1)\n",
    "    return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------\n",
    "#   Evaluation Metrics and Loss Functions\n",
    "#-----------------------------------\n",
    "\n",
    "def cross_entropy_loss(h, y):\n",
    "    # y.log(h) + (1-log(h) . 1-y)\n",
    "    # log probability * inverse of the log probabality \n",
    "    return np.multiply(np.log(h),y) + np.multiply((1-np.log(h)),(1-y))\n",
    "\n",
    "def cost(theta, X, y):\n",
    "    h = hypothesis(theta, X)\n",
    "    cost = cross_entropy_loss(h, y)\n",
    "    mean_cost = cost.sum() / -y.shape[0]\n",
    "    return mean_cost\n",
    "\n",
    "def accuracy_score(predY, Y):\n",
    "    TP = ((predY == Y) & (predY == 1.)).sum()\n",
    "    TN = ((predY == Y) & (predY == 0.)).sum()\n",
    "    acc = (TP + TN) / predY.shape[0]\n",
    "    return acc\n",
    "\n",
    "def precision_score(predY,Y):\n",
    "    TP = ((predY == Y) & (predY == 1)).sum()\n",
    "    FP = ((predY != Y) & (predY == 1)).sum()\n",
    "    precision = TP / (TP + FP)\n",
    "    return precision\n",
    "\n",
    "def recall_score(predY, Y):\n",
    "    TP = ((predY == Y) & (predY == 1)).sum()\n",
    "    FN = ((predY != Y) & (predY == 0)).sum()\n",
    "    recall = TP / (TP + FN)\n",
    "    return recall\n",
    "\n",
    "def f1_score(predY, Y, beta):\n",
    "    precision = precision_score(predY, Y)\n",
    "    recall = recall_score(predY, Y)\n",
    "    fscore = (1 + beta * beta) * ((precision * recall) / ((beta * beta * precision) + recall))\n",
    "    return fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------\n",
    "#   Gradient Descent\n",
    "#-----------------------------------\n",
    "\n",
    "def BGD(X, y, alpha, iterations):\n",
    "\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "    nsamples = X.shape[0]\n",
    "    nfeatures = X.shape[1]\n",
    "    theta = np.zeros(nfeatures)\n",
    "    J=[]\n",
    "\n",
    "    for i in range(iterations):\n",
    "\n",
    "        h = hypothesis(theta,X)\n",
    "\n",
    "        error = h - y\n",
    "\n",
    "        grad = np.dot(X.transpose(),error)/nsamples\n",
    "\n",
    "        theta = theta - alpha * grad\n",
    "\n",
    "        J.append(cost(theta,X,y))\n",
    "#         J.append(error)\n",
    "\n",
    "    X = np.delete(X,0,axis=1)\n",
    "    plt.figure()\n",
    "    plt.plot(J)    \n",
    "    plt.ylabel('Error')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.show()\n",
    "\n",
    "    return theta, J[iterations-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  90   90   90]\n",
      " [  92   92   92]\n",
      " [  29   29   29]\n",
      " ...\n",
      " [ 200  200  200]\n",
      " [3400 3400 3400]\n",
      " [7300 7300 7300]]\n",
      "\n",
      "--- X\n",
      "[[  90   90   90]\n",
      " [  92   92   92]\n",
      " [  29   29   29]\n",
      " ...\n",
      " [ 200  200  200]\n",
      " [3400 3400 3400]\n",
      " [7300 7300 7300]]\n",
      "\n",
      "--- y\n",
      "[0. 0. 0. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------\n",
    "#   Logistic Regression - Toy Example\n",
    "#-----------------------------------\n",
    "\n",
    "size=5000\n",
    "size2=size//2\n",
    "size10=size//10\n",
    "size102=size10//2\n",
    "\n",
    "X = np.random.randint(low=1, high=99, size=(size,1))\n",
    "X = np.concatenate((X, X, X), axis=1)\n",
    "X = np.multiply(X,np.concatenate((np.full((size2,1),1), np.full((size2,1),100))))\n",
    "\n",
    "print(X)\n",
    "y = np.empty((size))\n",
    "y[X[:,1] <= 99]=0\n",
    "y[X[:,1] >= 100]=1\n",
    "\n",
    "print(\"\\n--- X\")\n",
    "print(X)\n",
    "print(\"\\n--- y\")\n",
    "print(y)\n",
    "\n",
    "theta,error = BGD(X, y, 0.001, 10000)\n",
    "print(\"\\n--- Theta\")\n",
    "print(theta)\n",
    "print(\"\\n--- Error\")\n",
    "print(error)\n",
    "\n",
    "\n",
    "X_val = np.random.randint(low=1, high=9900, size=(size10,1))\n",
    "X_val = np.concatenate((X_val, X_val, X_val),axis=1)\n",
    "y_val = np.empty((size10))\n",
    "y_val[X_val[:,1] <= 99]=0\n",
    "y_val[X_val[:,1] >= 100]=1\n",
    "\n",
    "\n",
    "h = predict(theta,X_val)\n",
    "print(\"\\n--- Hypothesis\")\n",
    "print(h)\n",
    "predY = classify(theta,X_val,0.7)\n",
    "print(\"\\n--- Classification\")\n",
    "print(predY)\n",
    "print(\"\\n--- Expected Output\")\n",
    "print(y_val)\n",
    "\n",
    "acc = accuracy_score(predY, y_val)\n",
    "pre = precision_score(predY, y_val)\n",
    "recall = recall_score(predY, y_val)\n",
    "f = f1_score(predY, y_val, 1)\n",
    "\n",
    "print(\"\\n--- Accuracy\")\n",
    "print(acc)\n",
    "print(\"\\n--- Precision\")\n",
    "print(pre)\n",
    "print(\"\\n--- Recall\")\n",
    "print(recall)\n",
    "print(\"\\n--- F1Score\")\n",
    "print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------\n",
    "# MultiClass Classification - Toy Example\n",
    "#-----------------------------------\n",
    "\n",
    "# Not ready, still has a bug\n",
    "\n",
    "size=500\n",
    "size4=size//4\n",
    "size10=size//10\n",
    "size104=size10//4\n",
    "X = np.random.randint(low=0,high=7999, size=(size,1))\n",
    "X = np.concatenate((X,X,X),axis=1)\n",
    "y = np.empty((size),dtype=np.float128)\n",
    "y[X[:,1] <= 1999]=0\n",
    "y[((X[:,1] <= 3999) & (X[:,1]>1999))]=1\n",
    "y[((X[:,1] <= 5999) & (X[:,1]>3999))]=2\n",
    "y[X[:,1]>5999]=3\n",
    "\n",
    "classes = np.unique(y)\n",
    "print(classes)\n",
    "theta = {}\n",
    "\n",
    "for c in classes:\n",
    "\n",
    "\tcy = np.copy(y)\n",
    "\n",
    "\tcy[y != c] = 0\n",
    "\tcy[y == c] = 1\n",
    "\n",
    "\ttheta[c],acc = BGD(X,cy,0.001,500)\n",
    "\n",
    "X_val = np.random.randint(low=1,high=7999, size=(size10,1))\n",
    "X_val = np.concatenate((X_val,X_val,X_val),axis=1)\n",
    "y_val = np.empty((size10))\n",
    "y_val[X_val[:,1] <= 1999]=0\n",
    "y_val[((X_val[:,1] <= 3999) & (X_val[:,1]>1999))]=1\n",
    "y_val[((X_val[:,1] <= 5999) & (X_val[:,1]>3999))]=2\n",
    "y_val[X_val[:,1]>5999]=3\n",
    "\n",
    "print(\"\\n--- Theta\")\n",
    "print(theta)\n",
    "\n",
    "predY, pred = classify_multiclass(theta,X_val)\n",
    "print(\"\\n--- Hypothesis\")\n",
    "print(pred)\n",
    "print(\"\\n--- Classification\")\n",
    "print(predY)\n",
    "print(\"\\n--- Expected Output\")\n",
    "print(y_val)\n",
    "\n",
    "acc = accuracy_score(predY, y_val)\n",
    "pre = precision_score(predY, y_val)\n",
    "recall = recall_score(predY, y_val)\n",
    "f = f1_score(predY, y_val, 1)\n",
    "\n",
    "print(\"\\n--- Accuracy\")\n",
    "print(acc)\n",
    "print(\"\\n--- Precision\")\n",
    "print(pre)\n",
    "print(\"\\n--- Recall\")\n",
    "print(recall)\n",
    "print(\"\\n--- F1Score\")\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
