{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the models with the Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train-images-idx3-ubyte.gz', 'train-labels-idx1-ubyte.gz', 't10k-images-idx3-ubyte.gz', 't10k-labels-idx1-ubyte.gz']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azael/Trabalhos/MO444/MO444/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "\n",
    "import time\n",
    "import timeit\n",
    "from NN import activation_functions, loss_functions\n",
    "import NN.network as network\n",
    "\n",
    "from utils import dataset_helper\n",
    "from utils import custom_scores\n",
    "from importlib import reload \n",
    "\n",
    "\n",
    "base_dir = '../data/fashion'\n",
    "print(os.listdir(base_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import mnist_reader, dataset_helper\n",
    "X, y = mnist_reader.load_mnist('../data/fashion', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist('../data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.copy() / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the dtypes, there is no possibility of negative values in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclasses =10\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.05, random_state=42)    \n",
    "y_train = dataset_helper.one_hot_encode(y_train, nclasses)\n",
    "y_val = dataset_helper.one_hot_encode(y_val, nclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((57000, 784), (3000, 784), (57000, 10), (3000, 10))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "b_sz = 256\n",
    "eps = np.finfo(np.float64).eps\n",
    "nfeatures  = X.shape[1]\n",
    "epoch_sz = X.shape[0]\n",
    "max_iter = 200 * (epoch_sz // b_sz) \n",
    "print_interval = 10 * (epoch_sz // b_sz) \n",
    "decay_iteractions= 300 * (epoch_sz // b_sz) \n",
    "decay_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary\n",
      "-------------------------------\n",
      "H1      (input=784, neurons=512, activation=relu)\n",
      "soft    (input=512, neurons=10, activation=softmax)\n",
      "-------------------------------\n",
      "\n",
      "Shuffled\n",
      "It: 2340 Batch: 110 Epoch 10 Train Loss: 0.07748090 lr: 0.000100 Val Loss: 0.06985190 Val Acc 0.85833333\n",
      "It: 4680 Batch: 220 Epoch 20 Train Loss: 0.06029129 lr: 0.000100 Val Loss: 0.07058425 Val Acc 0.85666667\n",
      "It: 7020 Batch: 107 Epoch 31 Train Loss: 0.05452196 lr: 0.000100 Val Loss: 0.06000391 Val Acc 0.88233333\n",
      "It: 9360 Batch: 217 Epoch 41 Train Loss: 0.05074901 lr: 0.000100 Val Loss: 0.06125652 Val Acc 0.87600000\n",
      "It: 11700 Batch: 104 Epoch 52 Train Loss: 0.04772342 lr: 0.000100 Val Loss: 0.05715281 Val Acc 0.88966667\n",
      "It: 14040 Batch: 214 Epoch 62 Train Loss: 0.04527599 lr: 0.000100 Val Loss: 0.05525437 Val Acc 0.89300000\n",
      "It: 16380 Batch: 101 Epoch 73 Train Loss: 0.04308628 lr: 0.000100 Val Loss: 0.05378358 Val Acc 0.89633333\n",
      "It: 18720 Batch: 211 Epoch 83 Train Loss: 0.04112578 lr: 0.000100 Val Loss: 0.05500848 Val Acc 0.88733333\n",
      "It: 21060 Batch: 98 Epoch 94 Train Loss: 0.03939186 lr: 0.000100 Val Loss: 0.05340238 Val Acc 0.89033333\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-550593e34b20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m           \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_sz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_sz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m           \u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m           print_interval=print_interval)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0miteraction_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_iteration_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Trabalhos/MO444/T2/LogisticRegression-ANN/NN/network.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, lr, max_iter, lr_optimizer, epsilon, power_t, t, print_interval, b_sz, decay_iteractions, decay_rate, X_val, Y_val)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_error\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Trabalhos/MO444/T2/LogisticRegression-ANN/NN/network.py\u001b[0m in \u001b[0;36mbackpropagate\u001b[0;34m(self, Y, y_pred, lr)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         layer.backpropagate(last_layer=None, output=True,\n\u001b[0;32m--> 169\u001b[0;31m                             loss_gradient=loss_derivative, lr=lr)\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ml_idx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0ml_idx\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Trabalhos/MO444/T2/LogisticRegression-ANN/NN/network.py\u001b[0m in \u001b[0;36mbackpropagate\u001b[0;34m(self, last_layer, output, loss_gradient, lr)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_gradient\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;31m# print(self.label,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import timeit\n",
    "from NN import activation_functions, loss_functions\n",
    "import NN.network as network\n",
    "\n",
    "from utils import dataset_helper\n",
    "from utils import custom_scores\n",
    "from importlib import reload \n",
    "reload(custom_scores)\n",
    "reload(dataset_helper)\n",
    "reload(loss_functions)\n",
    "reload(activation_functions)\n",
    "reload(network)\n",
    "reload(dataset_helper)\n",
    "\n",
    "\n",
    "h1 = network.Layer(nfeatures, 512, 'relu',  label=\"H1\")\n",
    "o1 = network.Layer(512, nclasses, 'softmax', label=\"soft\")\n",
    "\n",
    "model = network.NN(loss='cross_entropy')\n",
    "model.add_layer(h1)\n",
    "model.add_layer(o1)\n",
    "model.summary()\n",
    "\n",
    "print(\"\")\n",
    "model.fit(X_train, y_train, max_iter=max_iter, \n",
    "          lr=lr, epsilon=eps, b_sz = b_sz,   \n",
    "          X_val=X_val, Y_val=y_val,\n",
    "          print_interval=print_interval)\n",
    "iteraction_log = network.get_iteration_log()\n",
    "\n",
    "Y_ = np.array(model.predict(X_test)).argmax(axis=-1)\n",
    "reload(custom_scores)\n",
    "custom_scores.evaluate_multiclass(y_test=y_test.argmax(axis=-1), y_pred=Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(custom_scores)\n",
    "reload(dataset_helper)\n",
    "reload(loss_functions)\n",
    "reload(activation_functions)\n",
    "reload(network)\n",
    "reload(dataset_helper)\n",
    "\n",
    "\n",
    "h1 = network.Layer(nfeatures, 512, 'relu',  label=\"H1\")\n",
    "o1 = network.Layer(512, nclasses, 'softmax', label=\"soft\")\n",
    "\n",
    "model = network.NN(loss='cross_entropy')\n",
    "model.add_layer(h1)\n",
    "model.add_layer(h2)\n",
    "model.add_layer(h3)\n",
    "model.add_layer(o1)\n",
    "model.summary()\n",
    "\n",
    "print(\"\")\n",
    "model.fit(X_train, y_train, max_iter=max_iter, \n",
    "          lr=lr, epsilon=eps, b_sz = b_sz,\n",
    "          X_val=X_val, Y_val=y_val,\n",
    "          print_interval=print_interval)\n",
    "iteraction_log = network.get_iteration_log()\n",
    "\n",
    "Y_ = np.array(model.predict(X_val)).argmax(axis=-1)\n",
    "reload(custom_scores)\n",
    "custom_scores.evaluate_multiclass(y_val=y_val.argmax(axis=-1), y_pred=Y_)\n",
    "iteraction_log.index = iteraction_log.it\n",
    "iteraction_log.error_train.plot()\n",
    "iteraction_log.error_val.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
