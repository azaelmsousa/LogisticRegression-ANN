{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the models with the Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train-labels-idx1-ubyte.gz', 'train-images-idx3-ubyte.gz', 't10k-images-idx3-ubyte.gz', 't10k-labels-idx1-ubyte.gz']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "\n",
    "import time\n",
    "import timeit\n",
    "from NN import activation_functions, loss_functions\n",
    "import NN.network as network\n",
    "\n",
    "from utils import dataset_helper\n",
    "from utils import custom_scores\n",
    "from importlib import reload \n",
    "\n",
    "\n",
    "base_dir = '../data/fashion'\n",
    "print(os.listdir(base_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import mnist_reader, dataset_helper\n",
    "X, y = mnist_reader.load_mnist('../data/fashion', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist('../data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.copy() / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the dtypes, there is no possibility of negative values in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclasses =10\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.05, random_state=42)    \n",
    "y_train = dataset_helper.one_hot_encode(y_train, nclasses)\n",
    "y_val = dataset_helper.one_hot_encode(y_val, nclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((57000, 784), (3000, 784), (57000, 10), (3000, 10))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "b_sz = 256\n",
    "eps = np.finfo(np.float64).eps\n",
    "nfeatures  = X_train.shape[1]\n",
    "epoch_sz = X_train.shape[0]\n",
    "max_iter = 1000 * (epoch_sz // b_sz) \n",
    "print_interval = 10 * (epoch_sz // b_sz) \n",
    "decay_iteractions= 300 * (epoch_sz // b_sz) \n",
    "decay_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary\n",
      "-------------------------------\n",
      "H1      (input=784, neurons=128, activation=relu)\n",
      "H2      (input=128, neurons=256, activation=relu)\n",
      "H3      (input=256, neurons=100, activation=relu)\n",
      "soft    (input=100, neurons=10, activation=softmax)\n",
      "-------------------------------\n",
      "\n",
      "Shuffled\n",
      "It: 2220 Batch: 213 Epoch 9 Train Loss: 0.09767297 lr: 0.000100 Val Loss: 0.07559946 Val Acc 0.85100000\n",
      "It: 4440 Batch: 203 Epoch 19 Train Loss: 0.06763070 lr: 0.000100 Val Loss: 0.06760438 Val Acc 0.86400000\n",
      "It: 6660 Batch: 193 Epoch 29 Train Loss: 0.05983446 lr: 0.000100 Val Loss: 0.06252444 Val Acc 0.87433333\n",
      "It: 8880 Batch: 183 Epoch 39 Train Loss: 0.05450625 lr: 0.000100 Val Loss: 0.05914234 Val Acc 0.87933333\n",
      "It: 11100 Batch: 173 Epoch 49 Train Loss: 0.05053760 lr: 0.000100 Val Loss: 0.06079744 Val Acc 0.88066667\n",
      "It: 13320 Batch: 163 Epoch 59 Train Loss: 0.04700024 lr: 0.000100 Val Loss: 0.05775611 Val Acc 0.88833333\n",
      "It: 15540 Batch: 153 Epoch 69 Train Loss: 0.04398248 lr: 0.000100 Val Loss: 0.05638815 Val Acc 0.89100000\n",
      "It: 17760 Batch: 143 Epoch 79 Train Loss: 0.04142932 lr: 0.000100 Val Loss: 0.05748332 Val Acc 0.88466667\n",
      "It: 19980 Batch: 133 Epoch 89 Train Loss: 0.03880456 lr: 0.000100 Val Loss: 0.05641216 Val Acc 0.89033333\n",
      "It: 22200 Batch: 123 Epoch 99 Train Loss: 0.03636110 lr: 0.000100 Val Loss: 0.06230658 Val Acc 0.88166667\n",
      "It: 24420 Batch: 113 Epoch 109 Train Loss: 0.03429414 lr: 0.000100 Val Loss: 0.05812263 Val Acc 0.89133333\n",
      "It: 26640 Batch: 103 Epoch 119 Train Loss: 0.03232187 lr: 0.000100 Val Loss: 0.05963821 Val Acc 0.88700000\n",
      "It: 28860 Batch: 93 Epoch 129 Train Loss: 0.03057294 lr: 0.000100 Val Loss: 0.05968776 Val Acc 0.89433333\n",
      "It: 31080 Batch: 83 Epoch 139 Train Loss: 0.02874217 lr: 0.000100 Val Loss: 0.06050463 Val Acc 0.88933333\n",
      "It: 33300 Batch: 73 Epoch 149 Train Loss: 0.02695326 lr: 0.000100 Val Loss: 0.06086499 Val Acc 0.88966667\n",
      "It: 35520 Batch: 63 Epoch 159 Train Loss: 0.02545676 lr: 0.000100 Val Loss: 0.06167861 Val Acc 0.88900000\n",
      "It: 37740 Batch: 53 Epoch 169 Train Loss: 0.02379942 lr: 0.000100 Val Loss: 0.07303250 Val Acc 0.88066667\n",
      "It: 39960 Batch: 43 Epoch 179 Train Loss: 0.02238307 lr: 0.000100 Val Loss: 0.06683190 Val Acc 0.88566667\n",
      "It: 42180 Batch: 33 Epoch 189 Train Loss: 0.02089017 lr: 0.000100 Val Loss: 0.07156954 Val Acc 0.87933333\n",
      "It: 44400 Batch: 23 Epoch 199 Train Loss: 0.01956045 lr: 0.000100 Val Loss: 0.06827412 Val Acc 0.89166667\n",
      "It: 46620 Batch: 13 Epoch 209 Train Loss: 0.01826025 lr: 0.000100 Val Loss: 0.07389559 Val Acc 0.88300000\n",
      "It: 48840 Batch: 3 Epoch 219 Train Loss: 0.01678512 lr: 0.000100 Val Loss: 0.07266729 Val Acc 0.88866667\n",
      "It: 51060 Batch: 216 Epoch 228 Train Loss: 0.01590899 lr: 0.000100 Val Loss: 0.07852540 Val Acc 0.88366667\n",
      "It: 53280 Batch: 206 Epoch 238 Train Loss: 0.01450375 lr: 0.000100 Val Loss: 0.08267126 Val Acc 0.87433333\n",
      "It: 55500 Batch: 196 Epoch 248 Train Loss: 0.01313123 lr: 0.000100 Val Loss: 0.07923533 Val Acc 0.89233333\n",
      "It: 57720 Batch: 186 Epoch 258 Train Loss: 0.01277564 lr: 0.000100 Val Loss: 0.08389964 Val Acc 0.88366667\n",
      "It: 59940 Batch: 176 Epoch 268 Train Loss: 0.01144603 lr: 0.000100 Val Loss: 0.08498344 Val Acc 0.88933333\n",
      "It: 62160 Batch: 166 Epoch 278 Train Loss: 0.01097470 lr: 0.000100 Val Loss: 0.08718615 Val Acc 0.88833333\n",
      "It: 64380 Batch: 156 Epoch 288 Train Loss: 0.01009308 lr: 0.000100 Val Loss: 0.08940401 Val Acc 0.88533333\n",
      "It: 66600 Batch: 146 Epoch 298 Train Loss: 0.00812768 lr: 0.000100 Val Loss: 0.09211013 Val Acc 0.88766667\n",
      "It: 68820 Batch: 136 Epoch 308 Train Loss: 0.00423581 lr: 0.000050 Val Loss: 0.09721536 Val Acc 0.88800000\n",
      "It: 71040 Batch: 126 Epoch 318 Train Loss: 0.00375763 lr: 0.000050 Val Loss: 0.09923832 Val Acc 0.89000000\n",
      "It: 73260 Batch: 116 Epoch 328 Train Loss: 0.00338535 lr: 0.000050 Val Loss: 0.10164890 Val Acc 0.88833333\n",
      "It: 75480 Batch: 106 Epoch 338 Train Loss: 0.00305087 lr: 0.000050 Val Loss: 0.10378742 Val Acc 0.88633333\n",
      "It: 77700 Batch: 96 Epoch 348 Train Loss: 0.00274256 lr: 0.000050 Val Loss: 0.10640891 Val Acc 0.88533333\n",
      "It: 79920 Batch: 86 Epoch 358 Train Loss: 0.00250596 lr: 0.000050 Val Loss: 0.10861939 Val Acc 0.88700000\n",
      "It: 82140 Batch: 76 Epoch 368 Train Loss: 0.00226070 lr: 0.000050 Val Loss: 0.11089703 Val Acc 0.88466667\n",
      "It: 84360 Batch: 66 Epoch 378 Train Loss: 0.00205358 lr: 0.000050 Val Loss: 0.11475684 Val Acc 0.88633333\n",
      "It: 86580 Batch: 56 Epoch 388 Train Loss: 0.00187557 lr: 0.000050 Val Loss: 0.11455383 Val Acc 0.89033333\n",
      "It: 88800 Batch: 46 Epoch 398 Train Loss: 0.00168461 lr: 0.000050 Val Loss: 0.11685393 Val Acc 0.88700000\n",
      "It: 91020 Batch: 36 Epoch 408 Train Loss: 0.00155765 lr: 0.000050 Val Loss: 0.12002988 Val Acc 0.88766667\n",
      "It: 93240 Batch: 26 Epoch 418 Train Loss: 0.00141551 lr: 0.000050 Val Loss: 0.12134830 Val Acc 0.88666667\n",
      "It: 95460 Batch: 16 Epoch 428 Train Loss: 0.00128449 lr: 0.000050 Val Loss: 0.12382792 Val Acc 0.88566667\n",
      "It: 97680 Batch: 6 Epoch 438 Train Loss: 0.00118144 lr: 0.000050 Val Loss: 0.12539109 Val Acc 0.88666667\n",
      "It: 99900 Batch: 219 Epoch 447 Train Loss: 0.00109197 lr: 0.000050 Val Loss: 0.12645726 Val Acc 0.88766667\n",
      "It: 102120 Batch: 209 Epoch 457 Train Loss: 0.00100951 lr: 0.000050 Val Loss: 0.12822413 Val Acc 0.88666667\n",
      "It: 104340 Batch: 199 Epoch 467 Train Loss: 0.00093481 lr: 0.000050 Val Loss: 0.13011073 Val Acc 0.88800000\n",
      "It: 106560 Batch: 189 Epoch 477 Train Loss: 0.00086789 lr: 0.000050 Val Loss: 0.13123005 Val Acc 0.88766667\n",
      "It: 108780 Batch: 179 Epoch 487 Train Loss: 0.00081484 lr: 0.000050 Val Loss: 0.13307387 Val Acc 0.88700000\n",
      "It: 111000 Batch: 169 Epoch 497 Train Loss: 0.00076022 lr: 0.000050 Val Loss: 0.13362018 Val Acc 0.88833333\n",
      "It: 113220 Batch: 159 Epoch 507 Train Loss: 0.00071352 lr: 0.000050 Val Loss: 0.13582296 Val Acc 0.88666667\n",
      "It: 115440 Batch: 149 Epoch 517 Train Loss: 0.00066702 lr: 0.000050 Val Loss: 0.13728015 Val Acc 0.88600000\n",
      "It: 117660 Batch: 139 Epoch 527 Train Loss: 0.00062941 lr: 0.000050 Val Loss: 0.13799808 Val Acc 0.88666667\n",
      "It: 119880 Batch: 129 Epoch 537 Train Loss: 0.00059264 lr: 0.000050 Val Loss: 0.13985959 Val Acc 0.88466667\n",
      "It: 122100 Batch: 119 Epoch 547 Train Loss: 0.00056259 lr: 0.000050 Val Loss: 0.14073707 Val Acc 0.88833333\n",
      "It: 124320 Batch: 109 Epoch 557 Train Loss: 0.00053214 lr: 0.000050 Val Loss: 0.14203958 Val Acc 0.88600000\n",
      "It: 126540 Batch: 99 Epoch 567 Train Loss: 0.00050674 lr: 0.000050 Val Loss: 0.14319417 Val Acc 0.88600000\n",
      "It: 128760 Batch: 89 Epoch 577 Train Loss: 0.00048062 lr: 0.000050 Val Loss: 0.14436793 Val Acc 0.88766667\n",
      "It: 130980 Batch: 79 Epoch 587 Train Loss: 0.00045946 lr: 0.000050 Val Loss: 0.14461657 Val Acc 0.88633333\n",
      "It: 133200 Batch: 69 Epoch 597 Train Loss: 0.00043803 lr: 0.000050 Val Loss: 0.14619441 Val Acc 0.88733333\n",
      "It: 135420 Batch: 59 Epoch 607 Train Loss: 0.00040603 lr: 0.000025 Val Loss: 0.14668303 Val Acc 0.88700000\n",
      "It: 137640 Batch: 49 Epoch 617 Train Loss: 0.00039660 lr: 0.000025 Val Loss: 0.14738868 Val Acc 0.88666667\n",
      "It: 139860 Batch: 39 Epoch 627 Train Loss: 0.00038801 lr: 0.000025 Val Loss: 0.14750751 Val Acc 0.88733333\n",
      "It: 142080 Batch: 29 Epoch 637 Train Loss: 0.00038115 lr: 0.000025 Val Loss: 0.14798145 Val Acc 0.88766667\n",
      "It: 144300 Batch: 19 Epoch 647 Train Loss: 0.00037308 lr: 0.000025 Val Loss: 0.14850770 Val Acc 0.88600000\n",
      "It: 146520 Batch: 9 Epoch 657 Train Loss: 0.00036600 lr: 0.000025 Val Loss: 0.14890952 Val Acc 0.88800000\n",
      "It: 148740 Batch: 222 Epoch 666 Train Loss: 0.00035856 lr: 0.000025 Val Loss: 0.14968880 Val Acc 0.88700000\n",
      "It: 150960 Batch: 212 Epoch 676 Train Loss: 0.00035202 lr: 0.000025 Val Loss: 0.14977674 Val Acc 0.88733333\n",
      "It: 153180 Batch: 202 Epoch 686 Train Loss: 0.00034476 lr: 0.000025 Val Loss: 0.15013393 Val Acc 0.88666667\n",
      "It: 155400 Batch: 192 Epoch 696 Train Loss: 0.00033828 lr: 0.000025 Val Loss: 0.15079097 Val Acc 0.88633333\n",
      "It: 157620 Batch: 182 Epoch 706 Train Loss: 0.00033217 lr: 0.000025 Val Loss: 0.15090341 Val Acc 0.88800000\n",
      "It: 159840 Batch: 172 Epoch 716 Train Loss: 0.00032649 lr: 0.000025 Val Loss: 0.15147958 Val Acc 0.88666667\n",
      "It: 162060 Batch: 162 Epoch 726 Train Loss: 0.00032059 lr: 0.000025 Val Loss: 0.15180120 Val Acc 0.88633333\n",
      "It: 164280 Batch: 152 Epoch 736 Train Loss: 0.00031408 lr: 0.000025 Val Loss: 0.15232236 Val Acc 0.88666667\n",
      "It: 166500 Batch: 142 Epoch 746 Train Loss: 0.00031016 lr: 0.000025 Val Loss: 0.15270642 Val Acc 0.88633333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 168720 Batch: 132 Epoch 756 Train Loss: 0.00030367 lr: 0.000025 Val Loss: 0.15322101 Val Acc 0.88633333\n",
      "It: 170940 Batch: 122 Epoch 766 Train Loss: 0.00029937 lr: 0.000025 Val Loss: 0.15360567 Val Acc 0.88733333\n",
      "It: 173160 Batch: 112 Epoch 776 Train Loss: 0.00029405 lr: 0.000025 Val Loss: 0.15369592 Val Acc 0.88666667\n",
      "It: 175380 Batch: 102 Epoch 786 Train Loss: 0.00028864 lr: 0.000025 Val Loss: 0.15430726 Val Acc 0.88666667\n",
      "It: 177600 Batch: 92 Epoch 796 Train Loss: 0.00028437 lr: 0.000025 Val Loss: 0.15457418 Val Acc 0.88700000\n",
      "It: 179820 Batch: 82 Epoch 806 Train Loss: 0.00027972 lr: 0.000025 Val Loss: 0.15502829 Val Acc 0.88600000\n",
      "It: 182040 Batch: 72 Epoch 816 Train Loss: 0.00027563 lr: 0.000025 Val Loss: 0.15536553 Val Acc 0.88666667\n",
      "It: 184260 Batch: 62 Epoch 826 Train Loss: 0.00027062 lr: 0.000025 Val Loss: 0.15587903 Val Acc 0.88766667\n",
      "It: 186480 Batch: 52 Epoch 836 Train Loss: 0.00026637 lr: 0.000025 Val Loss: 0.15637974 Val Acc 0.88700000\n"
     ]
    }
   ],
   "source": [
    "reload(custom_scores)\n",
    "reload(dataset_helper)\n",
    "reload(loss_functions)\n",
    "reload(activation_functions)\n",
    "reload(network)\n",
    "reload(dataset_helper)\n",
    "\n",
    "\n",
    "h1 = network.Layer(nfeatures, 128, 'relu',  label=\"H1\")\n",
    "h2 = network.Layer(128, 256, 'relu',  label=\"H2\")\n",
    "h3 = network.Layer(256, 100, 'relu',  label=\"H3\")\n",
    "o1 = network.Layer(100, nclasses, 'softmax', label=\"soft\")\n",
    "\n",
    "model = network.NN(loss='cross_entropy')\n",
    "model.add_layer(h1)\n",
    "model.add_layer(h2)\n",
    "model.add_layer(h3)\n",
    "model.add_layer(o1)\n",
    "model.summary()\n",
    "\n",
    "print(\"\")\n",
    "model.fit(X_train, y_train, max_iter=max_iter, \n",
    "          lr=lr, epsilon=eps, b_sz = b_sz,\n",
    "          X_val=X_val, Y_val=y_val,\n",
    "          decay_iteractions= decay_iteractions, decay_rate = decay_rate,\n",
    "          print_interval=print_interval)\n",
    "iteraction_log = network.get_iteration_log()\n",
    "\n",
    "Y_ = np.array(model.predict(X_val)).argmax(axis=-1)\n",
    "reload(custom_scores)\n",
    "custom_scores.evaluate_multiclass(y_val=y_val.argmax(axis=-1), y_pred=Y_)\n",
    "iteraction_log.index = iteraction_log.it\n",
    "iteraction_log.error_train.plot()\n",
    "iteraction_log.error_val.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(custom_scores)\n",
    "reload(dataset_helper)\n",
    "reload(loss_functions)\n",
    "reload(activation_functions)\n",
    "reload(network)\n",
    "reload(dataset_helper)\n",
    "\n",
    "\n",
    "h1 = network.Layer(nfeatures, 512, 'relu',  label=\"H1\")\n",
    "o1 = network.Layer(512, nclasses, 'softmax', label=\"soft\")\n",
    "\n",
    "model = network.NN(loss='cross_entropy')\n",
    "model.add_layer(h1)\n",
    "model.add_layer(h2)\n",
    "model.add_layer(h3)\n",
    "model.add_layer(o1)\n",
    "model.summary()\n",
    "\n",
    "print(\"\")\n",
    "model.fit(X_train, y_train, max_iter=max_iter, \n",
    "          lr=lr, epsilon=eps, b_sz = b_sz,\n",
    "          X_val=X_val, Y_val=y_val,\n",
    "          print_interval=print_interval)\n",
    "iteraction_log = network.get_iteration_log()\n",
    "\n",
    "Y_ = np.array(model.predict(X_val)).argmax(axis=-1)\n",
    "reload(custom_scores)\n",
    "custom_scores.evaluate_multiclass(y_val=y_val.argmax(axis=-1), y_pred=Y_)\n",
    "iteraction_log.index = iteraction_log.it\n",
    "iteraction_log.error_train.plot()\n",
    "iteraction_log.error_val.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
