{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows the development of an Artificial Neural Network with focus on classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.datasets as sk_datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function:\n",
    "\t\n",
    "\t#\n",
    "\t# Sigmoid function for the activation\n",
    "\t# of a neuron, where h is the dot\n",
    "\t# product of X (input) and theta (weights)\n",
    "\t#\n",
    "\tdef act_Sigmoid(h):\n",
    "\t\tsig = 1. / (1. + np.exp(-h))\n",
    "\t\treturn sig\n",
    "\n",
    "\t#\n",
    "\t# Derivative of the sigmoid function. It\n",
    "\t# is used as part of the backpropagation\n",
    "\t# algorithm\n",
    "\t#\n",
    "\tdef act_Sigmoid_derivative(h):\n",
    "\t\tsig = act_Sigmoid(h)\n",
    "\t\tderivative = sig*(1-sig)\n",
    "\t\treturn derivative\n",
    "\n",
    "\t#\n",
    "\t# Hyberbolic tangent function for the\n",
    "\t# acivation of a neuron, where h is the\n",
    "\t# dot product of X (input) and theta (weights)\n",
    "\t#\t\n",
    "\tdef act_Tanh(h):\n",
    "\t\ttanh = (2 / (1+np.exp(-2*h)))-1\n",
    "\n",
    "\t#\n",
    "\t# Deivative of the hyperbolic tangent\n",
    "\t# function. It is used as part of the\t\n",
    "\t# back propagation algorithm.\n",
    "\t#\n",
    "\tdef act_Tanh_derivative(h):\n",
    "\t\ttanh_l = (4*np.exp(-2*h))/((1+np.exp(-2*h))**2)\n",
    "\t\treturn tanh_l\n",
    "\n",
    "\t#\n",
    "\t# Cross entropy loss function, where h\n",
    "\t# is the activation of the last layer.\n",
    "\t# It computes the error of the predicted\n",
    "\t# class and the correct one.\n",
    "\t#\n",
    "\tdef err_CrossEntropy(predY,y):\n",
    "\t\teps = np.finfo(np.float128).eps\n",
    "\t\tpredY[predY < eps] = eps\n",
    "\t\tpredY[predY > 1.-eps] = 1.-eps\n",
    "\t\treturn -np.multiply(np.log(predY),y) - np.multiply((np.log(1-predY)),(1-y))\n",
    "\n",
    "\t#\n",
    "\t# Derivative of the SMD loss function.\n",
    "\t# It is used in the back propagation\n",
    "\t# algorithm.\n",
    "\t#\n",
    "\tdef err_CrossEntropy_derivative(X,predY,y):\n",
    "\t\terror = (predY - y)\n",
    "\t\tgrad = np.dot(X.transpose(),error)\n",
    "\t\treturn grad\n",
    "\n",
    "\t#\n",
    "\t# Sum of the squared differences (SMD) loss\n",
    "\t# function, where h is the activation of the\n",
    "\t# last layer. It computes the error of the\n",
    "\t# predicted class and the correct one.\n",
    "\t#\n",
    "\tdef err_SMD(predY,y):\n",
    "\t\terror = np.square((predY - y)).sum()\n",
    "\t\treturn error/2\n",
    "\n",
    "\t#\n",
    "\t# Sum of the squared differences (SMD) loss\n",
    "\t# function, where h is the activation of the\n",
    "\t# last layer. It computes the error of the\n",
    "\t# predicted class and the correct one.\n",
    "\t#\n",
    "\tdef err_SMD_derivative(X,predY,y):\n",
    "\t\terror = (predY - y)\n",
    "\t\tgrad = np.dot(X.transpose(),error)\n",
    "\t\treturn grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "\n",
    "\tdef __init__(self, activation, error):\n",
    "\t\tself.act_func = Function.__dict__[activation]\n",
    "\t\tself.act_derivative = Function.__dict__[activation+\"_derivative\"]\n",
    "\t\tself.err_func = Function.__dict__[error]\n",
    "\t\tself.err_derivative = Function.__dict__[error+\"_derivative\"]\n",
    "\t\tself.activations = []\n",
    "\t\tself.dot_product = []\n",
    "\n",
    "\tdef initialize_random_weights(self, n_input, n_perceptron, n_classes):\n",
    "\t\tself.n_hidden_layers = len(n_perceptron)\n",
    "\n",
    "\t\tself.hidden_layers = []\t\t\n",
    "\n",
    "\t\tfor l in range(self.n_hidden_layers):\n",
    "\t\t\tif (l == 0):\n",
    "\t\t\t\tw = np.random.rand(n_input,n_perceptron[l])\t\t\t\t\n",
    "\t\t\telse:\n",
    "\t\t\t\tw = np.random.rand(n_perceptron[l-1],n_perceptron[l])\n",
    "\t\t\tself.hidden_layers.append(w)\n",
    "\t\t\t\n",
    "\n",
    "\t\tif (self.n_hidden_layers == 0):\n",
    "\t\t\tself.output_layer = np.random.rand(n_input,n_classes)\n",
    "\t\telse:\n",
    "\t\t\tself.output_layer = np.random.rand(n_perceptron[self.n_hidden_layers-1],n_classes)\n",
    "\n",
    "\n",
    "\tdef initialize_fixed_weights(self, w):\n",
    "\t\tself.hidden_layers = w[:-1]\n",
    "\t\tself.output_layer = w[-1]\n",
    "\t\tself.n_hidden_layers = len(w)-1\n",
    "\n",
    "\n",
    "\tdef show_weights(self):\n",
    "\n",
    "\t\tfor l in range(self.n_hidden_layers):\n",
    "\t\t\tprint(\"Hidden Layer \",str(l+1))\n",
    "\t\t\tprint(self.hidden_layers[l],\"\\n\")\n",
    "\n",
    "\t\tprint(\"Output Layer \")\n",
    "\t\tprint(self.output_layer,\"\\n\")\n",
    "\n",
    "\tdef show_activations(self):\n",
    "\n",
    "\t\tfor l in range(self.n_hidden_layers):\n",
    "\t\t\tprint(\"Hidden Layer \",str(l+1))\n",
    "\t\t\tprint(self.activations[l],\"\\n\")\n",
    "\n",
    "\t\tprint(\"Output Layer \")\n",
    "\t\tprint(self.activations[self.n_hidden_layers],\"\\n\")\n",
    "\n",
    "\tdef show_setup(self):\n",
    "\t\t\t\t\n",
    "\t\tprint(\"--- Input size: \",str(self.hidden_layers[0].shape[0]-1))\n",
    "\t\tprint(\"--- Number of hidden layers: \",str(self.n_hidden_layers))\n",
    "\t\tprint(\"--- Number of perceptrons at each layer: \")\n",
    "\t\tfor l in range(self.n_hidden_layers):\n",
    "\t\t\tprint(\"------ HL \"+str(l+1)+\": \"+str(self.hidden_layers[l].shape[1]))\n",
    "\t\tprint(\"--- Number of classes: \"+str(self.output_layer.shape[1]),\"\\n\")\n",
    "\n",
    "\tdef foward_propagation(self, X):\n",
    "\n",
    "\t\tdel self.activations[:]\n",
    "\n",
    "\t\tself.activations.append(X)\n",
    "\t\tinp = X #np.insert(X,0,1,axis=1)\n",
    "\n",
    "\t\tfor l in range(self.n_hidden_layers):\n",
    "\t\t\tout = np.matmul(inp, self.hidden_layers[l])\n",
    "\t\t\tself.dot_product.append(out)\n",
    "\t\t\tsig = self.act_func(out)\n",
    "\t\t\tself.activations.append(sig)\n",
    "\t\t\tinp = sig #np.insert(sig,0,1,axis=1)\t\t\t\n",
    "\n",
    "\t\tout = np.matmul(inp, self.output_layer)\n",
    "\t\tself.dot_product.append(out)\n",
    "\t\tsig = self.act_func(out)\n",
    "\t\tself.activations.append(sig)\n",
    "\n",
    "\t\treturn sig\n",
    "\n",
    "\tdef backpropagation(self, x, y):\n",
    "\t\tsig = self.foward_propagation([x])\n",
    "\t\t#dErr/dAct * dAct/dDot\n",
    "\t\tdelta1 = self.err_derivative(sig,y) * self.act_derivative(self.dot_product[-1])\t\n",
    "\t\n",
    "\t\t#input current layer = output previous layer (inserting bias)\n",
    "\t\tact = np.insert(self.activations[-2],0,1,axis=1).transpose()\n",
    "\n",
    "\t\t#dErr/dAct * dAct/dDot * dDot/dWl\n",
    "\t\tgrad1 = np.matmul(act,delta1)\n",
    "\n",
    "\t\t#Layer (l-1)\n",
    "\n",
    "\t\t#dDotK/dAct (no bias)\n",
    "\t\tdot_act = self.output_layer[1:]\n",
    "\t\t#dAct/dDot\n",
    "\t\tact_dot = self.act_derivative(self.dot_product[-2])\n",
    "\t\tdelta2 = np.multiply(dot_act, act_dot.transpose())\n",
    "\t\tdelta12 = np.multiply(delta1,delta2)\n",
    "\t\tdelta12s = delta12.sum(axis=1)\n",
    "\t\tdelta12s = np.expand_dims(delta12s, axis=0)\n",
    "\n",
    "\t\tact = np.insert(self.activations[-3],0,1,axis=1).transpose()\n",
    "\t\tgrad2 = np.multiply(delta12s, act)\n",
    "\n",
    "\t\tgrad3 = None\n",
    "\t\t#Layer (l-2)\n",
    "\t\tif(len(self.hidden_layers) > 1):\n",
    "\t\t\t#dDotK/dAct (no bias)\n",
    "\t\t\tdot_act = self.hidden_layers[-1][1:]\n",
    "\t\t\t#dAct/dDot\n",
    "\t\t\tact_dot = self.act_derivative(self.dot_product[-3])\n",
    "\t\t\tdelta3 = np.multiply(dot_act, act_dot.transpose())\t\t\n",
    "\n",
    "\t\t\tdelta123 = np.array([])\n",
    "\t\t\tfor l,c in zip(delta12,delta3.transpose()):\n",
    "\t\t\t\tl = np.expand_dims(l,axis=0)\n",
    "\t\t\t\tc = np.expand_dims(c,axis=1)\n",
    "\t\t\t\tmatrix = np.matmul(c,l)\n",
    "\t\t\t\tdelta123 = delta123+matrix if delta123.size else matrix\n",
    "\t\t\tdelta123s = delta123.sum(axis=1)\n",
    "\n",
    "\t\t\tact = np.insert([x],0,1,axis=1).transpose()\n",
    "\t\t\tgrad3 = np.multiply(delta123s,act)\n",
    "\t\t\n",
    "\t\treturn grad1,grad2,grad3\n",
    "\n",
    "\n",
    "\tdef stochastic_training(self, X, Y, alpha, epochs):\n",
    "\t\tJ = []\n",
    "\t\tfor e in range(epochs):\n",
    "\t\t\tfor x,y in zip(X,Y):\n",
    "\t\t\t\tgrad1, grad2, grad3 = self.backpropagation(x,y)\t\t\t\n",
    "\t\t\t\n",
    "\t\t\t\t#update weights\n",
    "\t\t\t\tself.output_layer      = self.output_layer      - alpha*grad1\n",
    "\t\t\t\tself.hidden_layers[-1] = self.hidden_layers[-1] - alpha*grad2\n",
    "\t\t\t\tif(len(self.hidden_layers) > 1): self.hidden_layers[-2] = self.hidden_layers[-2] - alpha*grad3\n",
    "\n",
    "\t\t\t\tsig = self.foward_propagation([x])\n",
    "\t\t\t\terr = self.err_func(sig,y)\n",
    "\t\t\t\tJ.append(err)\n",
    "\t\t\t\t\n",
    "\t\tplt.plot(J)\t\n",
    "\t\tplt.ylabel('Error')\n",
    "\t\tplt.xlabel('iterations')\n",
    "\t\tplt.show()\n",
    "\n",
    "\tdef batch_training(self, X, Y, alpha, epochs):\n",
    "\t\tJ = []\n",
    "\t\tfor e in range(epochs):\n",
    "\t\t\tgrad1s, grad2s, grad3s = np.array([]),np.array([]),np.array([])\n",
    "\t\t\tfor x,y in zip(X,Y):\n",
    "\t\t\t\tgrad1, grad2, grad3 = self.backpropagation(x,y)\t\t\t\n",
    "\t\t\t\tgrad1s = grad1s+grad1 if grad1s.size else grad1\n",
    "\t\t\t\tgrad2s = grad2s+grad2 if grad2s.size else grad2\n",
    "\t\t\t\tif grad3: grad3s = grad3s+grad3 if grad3s.size else grad3\n",
    "\n",
    "\t\t\tgrad1s = grad1s/X.shape[0]\n",
    "\t\t\tgrad2s = grad2s/X.shape[0]\n",
    "\t\t\tgrad3s = grad3s/X.shape[0]\n",
    "\t\t\t\n",
    "\t\t\t#update weights\n",
    "\t\t\tself.output_layer      = self.output_layer      - alpha*grad1s\n",
    "\t\t\tself.hidden_layers[-1] = self.hidden_layers[-1] - alpha*grad2s\n",
    "\t\t\tif(len(self.hidden_layers) > 1): self.hidden_layers[-2] = self.hidden_layers[-2] - alpha*grad3s\n",
    "\n",
    "\t\t\tsig = self.foward_propagation(X)\n",
    "\t\t\terr = self.err_func(sig,Y)\n",
    "\t\t\tJ.append(err.sum()/X.shape[0])\n",
    "\t\t\t\t\n",
    "\t\tplt.plot(J)\t\n",
    "\t\tplt.ylabel('Error')\n",
    "\t\tplt.xlabel('iterations')\n",
    "\t\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer  1\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]] \n",
      "\n",
      "Output Layer \n",
      "[[ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]] \n",
      "\n",
      "--- Input size:  2\n",
      "--- Number of hidden layers:  1\n",
      "--- Number of perceptrons at each layer: \n",
      "------ HL 1: 2\n",
      "--- Number of classes: 2 \n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2,2) and (3,2) not aligned: 2 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b0edd2e26e65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteste\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfoward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Y\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\na(Y)\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-0a2eb54bb179>\u001b[0m in \u001b[0;36mfoward_propagation\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot_product\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                         \u001b[0msig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,2) and (3,2) not aligned: 2 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "#----------------------------------\n",
    "#        ANN Initialization\n",
    "#----------------------------------\n",
    "\n",
    "# Random Init\n",
    "teste = ANN(\"act_Sigmoid\", 'err_SMD')\n",
    "teste.initialize_random_weights(2, [2], 2)\n",
    "W1 = np.array([[0.15, 0.2], [0.25, 0.3]], np.float64).T\n",
    "W1 = np.array([[0.4, 0.45], [0.50, 0.55]],  np.float64).T\n",
    "w = [w1,w2]\n",
    "teste.initialize_fixed_weights(w)\n",
    "teste.show_weights()\n",
    "teste.show_setup()\n",
    "\n",
    "'''\n",
    "# Fixed Init\n",
    "w1 = np.array([[1,2],[3,4],[5,6]])\n",
    "w2 = np.array([[7,8],[9,10],[11,12]])\n",
    "w = [w1,w2]\n",
    "\n",
    "teste = ANN(\"act_Sigmoid\")\n",
    "teste.initialize_fixed_weights(w)\n",
    "teste.show_weights()\n",
    "teste.show_setup()\n",
    "'''\n",
    "\n",
    "#----------------------------------\n",
    "#           Toy Examples\n",
    "#----------------------------------\n",
    "\n",
    "X = np.array([[0.05, 0.10], [0.05, 0.10]], np.float64)\n",
    "Y = np.array([[0.01, 0.99], [0.01, 0.99]], np.float64)\n",
    "Y, aY = teste.foward_propagation(X)\n",
    "print(\"Y\\n\",Y)\n",
    "print(\"\\na(Y)\\n\",aY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
