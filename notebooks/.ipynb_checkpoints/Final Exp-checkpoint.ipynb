{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the models with the Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train-labels-idx1-ubyte.gz', 'train-images-idx3-ubyte.gz', 't10k-images-idx3-ubyte.gz', 't10k-labels-idx1-ubyte.gz']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "\n",
    "import time\n",
    "import timeit\n",
    "from NN import activation_functions, loss_functions\n",
    "import NN.network as network\n",
    "\n",
    "from utils import dataset_helper\n",
    "from utils import custom_scores\n",
    "from importlib import reload \n",
    "\n",
    "\n",
    "base_dir = '../data/fashion'\n",
    "print(os.listdir(base_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import mnist_reader, dataset_helper\n",
    "X, y = mnist_reader.load_mnist('../data/fashion', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist('../data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.copy() / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the dtypes, there is no possibility of negative values in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclasses =10\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.05, random_state=42)    \n",
    "y_train = dataset_helper.one_hot_encode(y_train, nclasses)\n",
    "y_val = dataset_helper.one_hot_encode(y_val, nclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((57000, 784), (3000, 784), (57000, 10), (3000, 10))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "b_sz = 256\n",
    "eps = np.finfo(np.float64).eps\n",
    "nfeatures  = X_train.shape[1]\n",
    "epoch_sz = X_train.shape[0]\n",
    "max_iter = 1000 * (epoch_sz // b_sz) \n",
    "print_interval = 10 * (epoch_sz // b_sz) \n",
    "decay_iteractions= 300 * epoch_sz\n",
    "decay_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary\n",
      "-------------------------------\n",
      "H1      (input=784, neurons=128, activation=relu)\n",
      "H2      (input=128, neurons=256, activation=relu)\n",
      "H3      (input=256, neurons=100, activation=relu)\n",
      "soft    (input=100, neurons=10, activation=softmax)\n",
      "-------------------------------\n",
      "\n",
      "Shuffled\n",
      "It: 2220 Batch: 213 Epoch 9 Train Loss: 0.09685530 lr: 0.000100 Val Loss: 0.07936623 Val Acc 0.84100000\n",
      "It: 4440 Batch: 203 Epoch 19 Train Loss: 0.06716714 lr: 0.000100 Val Loss: 0.06695639 Val Acc 0.86900000\n",
      "It: 6660 Batch: 193 Epoch 29 Train Loss: 0.05883200 lr: 0.000100 Val Loss: 0.06137398 Val Acc 0.88000000\n",
      "It: 8880 Batch: 183 Epoch 39 Train Loss: 0.05350302 lr: 0.000100 Val Loss: 0.05857082 Val Acc 0.88566667\n",
      "It: 11100 Batch: 173 Epoch 49 Train Loss: 0.04944792 lr: 0.000100 Val Loss: 0.05843918 Val Acc 0.88566667\n",
      "It: 13320 Batch: 163 Epoch 59 Train Loss: 0.04605790 lr: 0.000100 Val Loss: 0.05641447 Val Acc 0.88466667\n",
      "It: 15540 Batch: 153 Epoch 69 Train Loss: 0.04297932 lr: 0.000100 Val Loss: 0.05864197 Val Acc 0.87566667\n",
      "It: 17760 Batch: 143 Epoch 79 Train Loss: 0.04022384 lr: 0.000100 Val Loss: 0.06295494 Val Acc 0.88366667\n",
      "It: 19980 Batch: 133 Epoch 89 Train Loss: 0.03764026 lr: 0.000100 Val Loss: 0.05954521 Val Acc 0.88733333\n",
      "It: 22200 Batch: 123 Epoch 99 Train Loss: 0.03544444 lr: 0.000100 Val Loss: 0.05730185 Val Acc 0.89266667\n",
      "It: 24420 Batch: 113 Epoch 109 Train Loss: 0.03312580 lr: 0.000100 Val Loss: 0.05861111 Val Acc 0.88966667\n",
      "It: 26640 Batch: 103 Epoch 119 Train Loss: 0.03110050 lr: 0.000100 Val Loss: 0.06208260 Val Acc 0.88066667\n",
      "It: 28860 Batch: 93 Epoch 129 Train Loss: 0.02933049 lr: 0.000100 Val Loss: 0.05944311 Val Acc 0.89133333\n",
      "It: 31080 Batch: 83 Epoch 139 Train Loss: 0.02725928 lr: 0.000100 Val Loss: 0.06614151 Val Acc 0.88600000\n",
      "It: 33300 Batch: 73 Epoch 149 Train Loss: 0.02552157 lr: 0.000100 Val Loss: 0.07030939 Val Acc 0.87400000\n",
      "It: 35520 Batch: 63 Epoch 159 Train Loss: 0.02387119 lr: 0.000100 Val Loss: 0.06492636 Val Acc 0.89100000\n",
      "It: 37740 Batch: 53 Epoch 169 Train Loss: 0.02206183 lr: 0.000100 Val Loss: 0.06448646 Val Acc 0.89433333\n",
      "It: 39960 Batch: 43 Epoch 179 Train Loss: 0.02083026 lr: 0.000100 Val Loss: 0.07868764 Val Acc 0.88000000\n",
      "It: 42180 Batch: 33 Epoch 189 Train Loss: 0.01922379 lr: 0.000100 Val Loss: 0.07081673 Val Acc 0.88700000\n",
      "It: 44400 Batch: 23 Epoch 199 Train Loss: 0.01764750 lr: 0.000100 Val Loss: 0.07005744 Val Acc 0.88333333\n",
      "It: 46620 Batch: 13 Epoch 209 Train Loss: 0.01694691 lr: 0.000100 Val Loss: 0.08248878 Val Acc 0.88233333\n",
      "It: 48840 Batch: 3 Epoch 219 Train Loss: 0.01524980 lr: 0.000100 Val Loss: 0.07742029 Val Acc 0.89033333\n",
      "It: 51060 Batch: 216 Epoch 228 Train Loss: 0.01398987 lr: 0.000100 Val Loss: 0.08310298 Val Acc 0.88733333\n",
      "It: 53280 Batch: 206 Epoch 238 Train Loss: 0.01288636 lr: 0.000100 Val Loss: 0.08154534 Val Acc 0.88700000\n",
      "It: 55500 Batch: 196 Epoch 248 Train Loss: 0.01167357 lr: 0.000100 Val Loss: 0.09071118 Val Acc 0.88300000\n",
      "It: 57720 Batch: 186 Epoch 258 Train Loss: 0.01179848 lr: 0.000100 Val Loss: 0.09100345 Val Acc 0.89033333\n",
      "It: 59940 Batch: 176 Epoch 268 Train Loss: 0.00959341 lr: 0.000100 Val Loss: 0.09300620 Val Acc 0.89033333\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-00cc2ee95c9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m           \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_sz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_sz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m           \u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m           print_interval=print_interval)\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0miteraction_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_iteration_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/local/LogisticRegression-ANN/NN/network.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, lr, max_iter, lr_optimizer, epsilon, power_t, t, print_interval, b_sz, decay_iteractions, decay_rate, X_val, Y_val)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_error\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/local/LogisticRegression-ANN/NN/network.py\u001b[0m in \u001b[0;36mbackpropagate\u001b[0;34m(self, Y, y_pred, lr)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             layer.backpropagate(last_layer=last_layer, output=False,\n\u001b[0;32m--> 175\u001b[0;31m                                 loss_gradient=None, lr=lr)\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;31m# print(layer.label, layer.grad_w.mean())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/local/LogisticRegression-ANN/NN/network.py\u001b[0m in \u001b[0;36mbackpropagate\u001b[0;34m(self, last_layer, output, loss_gradient, lr)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             self.delta = last_layer.delta.dot(\n\u001b[0;32m---> 99\u001b[0;31m                 last_layer.weights.T) * self.act_derivative(self.net)\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reload(custom_scores)\n",
    "reload(dataset_helper)\n",
    "reload(loss_functions)\n",
    "reload(activation_functions)\n",
    "reload(network)\n",
    "reload(dataset_helper)\n",
    "\n",
    "\n",
    "h1 = network.Layer(nfeatures, 128, 'relu',  label=\"H1\")\n",
    "h2 = network.Layer(128, 256, 'relu',  label=\"H2\")\n",
    "h3 = network.Layer(256, 100, 'relu',  label=\"H3\")\n",
    "o1 = network.Layer(100, nclasses, 'softmax', label=\"soft\")\n",
    "\n",
    "model = network.NN(loss='cross_entropy')\n",
    "model.add_layer(h1)\n",
    "model.add_layer(h2)\n",
    "model.add_layer(h3)\n",
    "model.add_layer(o1)\n",
    "model.summary()\n",
    "\n",
    "print(\"\")\n",
    "model.fit(X_train, y_train, max_iter=max_iter, \n",
    "          lr=lr, epsilon=eps, b_sz = b_sz,\n",
    "          X_val=X_val, Y_val=y_val,\n",
    "          print_interval=print_interval)\n",
    "iteraction_log = network.get_iteration_log()\n",
    "\n",
    "Y_ = np.array(model.predict(X_val)).argmax(axis=-1)\n",
    "reload(custom_scores)\n",
    "custom_scores.evaluate_multiclass(y_val=y_val.argmax(axis=-1), y_pred=Y_)\n",
    "iteraction_log.index = iteraction_log.it\n",
    "iteraction_log.error_train.plot()\n",
    "iteraction_log.error_val.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.array(model.predict(X_test)).argmax(axis=-1)\n",
    "reload(custom_scores)\n",
    "custom_scores.evaluate_multiclass(y_val=y_test, y_pred=Y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(custom_scores)\n",
    "reload(dataset_helper)\n",
    "reload(loss_functions)\n",
    "reload(activation_functions)\n",
    "reload(network)\n",
    "reload(dataset_helper)\n",
    "\n",
    "\n",
    "h1 = network.Layer(nfeatures, 512, 'relu',  label=\"H1\")\n",
    "o1 = network.Layer(512, nclasses, 'softmax', label=\"soft\")\n",
    "\n",
    "model = network.NN(loss='cross_entropy')\n",
    "model.add_layer(h1)\n",
    "model.add_layer(h2)\n",
    "model.add_layer(h3)\n",
    "model.add_layer(o1)\n",
    "model.summary()\n",
    "\n",
    "print(\"\")\n",
    "model.fit(X_train, y_train, max_iter=max_iter, \n",
    "          lr=lr, epsilon=eps, b_sz = b_sz,\n",
    "          X_val=X_val, Y_val=y_val,\n",
    "          print_interval=print_interval)\n",
    "iteraction_log = network.get_iteration_log()\n",
    "\n",
    "Y_ = np.array(model.predict(X_val)).argmax(axis=-1)\n",
    "reload(custom_scores)\n",
    "custom_scores.evaluate_multiclass(y_val=y_val.argmax(axis=-1), y_pred=Y_)\n",
    "iteraction_log.index = iteraction_log.it\n",
    "iteraction_log.error_train.plot()\n",
    "iteraction_log.error_val.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.array(model.predict(X_test)).argmax(axis=-1)\n",
    "reload(custom_scores)\n",
    "custom_scores.evaluate_multiclass(y_val=y_test, y_pred=Y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
