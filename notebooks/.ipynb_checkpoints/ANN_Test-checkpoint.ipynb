{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from NN import network, activation_functions, loss_functions\n",
    "from NN import ANN\n",
    "X = np.array([[1,2],[3,4]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Code running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer  1\n",
      "[[0.40356314 0.3390639  0.98138579]\n",
      " [0.68541233 0.8509824  0.11704859]\n",
      " [0.06652712 0.84122368 0.8824393 ]] \n",
      "\n",
      "Output Layer \n",
      "[[0.33835423 0.88138579]\n",
      " [0.10516231 0.62094958]\n",
      " [0.60507321 0.20858151]\n",
      " [0.05975337 0.79489964]] \n",
      "\n",
      " Input size:  2\n",
      " Number of hidden layers:  1\n",
      " Number of perceptrons at each layer: \n",
      " HL 1: 3\n",
      " Number of classes: 2 \n",
      "\n",
      "Y\n",
      " [[1.04879415 2.31041338]\n",
      " [1.10026334 2.46115315]]\n",
      "\n",
      "a(Y)\n",
      " [[0.74054328 0.90973581]\n",
      " [0.75030944 0.92137324]]\n"
     ]
    }
   ],
   "source": [
    "# Random Init\n",
    "teste = ANN.ANN(\"sigmoid\")\n",
    "teste.initialize_random_weights(2, [3], 2)\n",
    "teste.show_weights()\n",
    "teste.show_setup()\n",
    "Y, aY = teste.foward_propagation(X)\n",
    "print(\"Y\\n\",Y)\n",
    "print(\"\\na(Y)\\n\",aY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with the new Implementation by using the same weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1 (1, 2) (2, 2)\n",
      "net [[0.3775 0.3925]]\n",
      "out [[0.59326999 0.59688438]]\n",
      "[[0.3775 0.3925]]\n",
      "[[0.59326999 0.59688438]]\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(network)\n",
    "I = np.array([[0.05, 0.10]])\n",
    "W = np.array([[0.15, 0.2], [0.25, 0.3]]).T\n",
    "h1 = network.Layer(2, 2, 'sigmoid', weights=W, bias=0.35, label=\"H1\")\n",
    "netH, outH =  h1.feed_forward(I)\n",
    "print(netH)\n",
    "print(outH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1 (1, 2) (2, 2)\n",
      "net [[1.10590597 1.2249214 ]]\n",
      "out [[0.75136507 0.77292847]]\n",
      "[[1.10590597 1.2249214 ]]\n",
      "[[0.75136507 0.77292847]]\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(network)\n",
    "I = np.array([[0.59326999, 0.59688438]])\n",
    "W = np.array([[0.4, 0.45], [0.50, 0.55]]).T\n",
    "o1 = network.Layer(2, 2, 'sigmoid', weights=W, bias=0.6, label=\"H1\")\n",
    "netO, outO =  o1.feed_forward(I)\n",
    "print(netO)\n",
    "print(outO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Weights\n",
      "-------------------------------\n",
      "H1      (input=2, neurons=2, activation=sigmoid)\n",
      "[[0.15 0.25]\n",
      " [0.2  0.3 ]]\n",
      "H1      (input=2, neurons=2, activation=sigmoid)\n",
      "[[0.4  0.5 ]\n",
      " [0.45 0.55]]\n",
      "-------------------------------\n",
      "H1 (1, 2) (2, 2)\n",
      "net [[0.3775 0.3925]]\n",
      "out [[0.59326999 0.59688438]]\n",
      "H1 (1, 2) (2, 2)\n",
      "net [[1.10590597 1.2249214 ]]\n",
      "out [[0.75136507 0.77292847]]\n",
      "Y\n",
      " [1.10590597, 1.2249214]\n",
      "\n",
      "a(Y)\n",
      " [0.75136507, 0.77292847]\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(network)\n",
    "model = network.NN(loss='smd')\n",
    "X = np.array([[0.05, 0.10]])\n",
    "\n",
    "Wh = np.array([[0.15, 0.2], [0.25, 0.3]]).T\n",
    "h = network.Layer(2, 2, 'sigmoid', weights=Wh, bias=0.35, label=\"H1\")\n",
    "\n",
    "Wo = np.array([[0.4, 0.45], [0.50, 0.55]]).T\n",
    "o = network.Layer(2, 2, 'sigmoid', weights=Wo, bias=0.6, label=\"H1\")\n",
    "model.add_layer(h)\n",
    "model.add_layer(o)\n",
    "model.show_weights()\n",
    "\n",
    "Y, aY = model.feed_forward(X)\n",
    "Y_, aY_ = ([1.10590597, 1.2249214 ], [0.75136507, 0.77292847])\n",
    "print(\"Y\\n\",Y_)\n",
    "print(\"\\na(Y)\\n\",aY_)\n",
    "\n",
    "assert((Y - Y_).sum() < np.finfo(np.float32).eps)\n",
    "assert((aY - aY_).sum() < np.finfo(np.float32).eps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on the back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Weights\n",
      "-------------------------------\n",
      "H1      (input=2, neurons=2, activation=sigmoid)\n",
      "[[0.15 0.25]\n",
      " [0.2  0.3 ]]\n",
      "H1      (input=2, neurons=2, activation=sigmoid)\n",
      "[[0.4  0.5 ]\n",
      " [0.45 0.55]]\n",
      "-------------------------------\n",
      "0.2983711087600027\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(network)\n",
    "model.show_weights()\n",
    "Y = np.array([0.01, 0.99])\n",
    "Etotal = loss_functions.smd(aY, Y)\n",
    "print(Etotal)\n",
    "assert((Etotal - 0.2983711) < np.finfo(np.float32).eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emulating the weights update for the layer O "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.74136507 -0.21707153]]\n",
      "dout [[0.1868156  0.17551005]]\n",
      "dnet [[0.59326999 0.59688438]]\n",
      "delta [[ 0.13849856 -0.03809824]]\n",
      "dw [[ 0.08216704 -0.02274024]]\n",
      "update [[0.35891648 0.51137012]\n",
      " [0.40891648 0.56137012]]\n"
     ]
    }
   ],
   "source": [
    "reload(loss_functions)\n",
    "reload(activation_functions)\n",
    "lr = 0.5\n",
    "#Done - Partial\n",
    "dEo_dw = loss_functions.smd_derivative_chain(outO, Y)\n",
    "print(dEo_dw)\n",
    "\n",
    "# Done\n",
    "dOuto_Dneto = activation_functions.sigmoid_derivative_chain(outO)\n",
    "print('dout',dOuto_Dneto)\n",
    "\n",
    "# Done - Self.input\n",
    "dNeto  = outH\n",
    "print('dnet',dNeto) \n",
    "# Done\n",
    "deltaO  = dEo_dw * dOuto_Dneto\n",
    "print('delta', deltaO)\n",
    "\n",
    "dWO = deltaO * outH\n",
    "print('dw', dWO)\n",
    "# Done\n",
    "updateO = Wo - lr * dWO\n",
    "print('update', updateO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.05539942 -0.01904912]\n",
      " [ 0.06232435 -0.02095403]]\n",
      "dETotal_dOh [0.03635031 0.04137032]\n",
      "dOuth_Dneth [[0.24130071 0.24061342]]\n",
      "deltaH [[0.00877135 0.00995425]]\n",
      "dWh [[0.00043857 0.00099543]]\n",
      "update [[0.14978072 0.24950229]\n",
      " [0.19978072 0.29950229]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Done - Partial\n",
    "dEo_dOh = deltaO * Wo\n",
    "dETotal_dOh = dEo_dOh.sum(axis=1)\n",
    "print (dEo_dOh)\n",
    "print('dETotal_dOh', dETotal_dOh)\n",
    "\n",
    "# Done\n",
    "dOuth_Dneth = activation_functions.sigmoid_derivative_chain(outH)\n",
    "print('dOuth_Dneth', dOuth_Dneth)\n",
    "\n",
    "\n",
    "# Done\n",
    "deltaH = dETotal_dOh * dOuth_Dneth \n",
    "print('deltaH', deltaH)\n",
    "\n",
    "# done\n",
    "# self.input\n",
    "dNeth_dw = X\n",
    "dWh = deltaH * dNeth_dw\n",
    "print('dWh', dWh)\n",
    "\n",
    "# Done\n",
    "updateH = Wh - lr * dWh\n",
    "print('update', updateH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-524cce6bf294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mnetH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m#     print(netH, outH)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mnetO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/local/LogisticRegression-ANN/NN/network.py\u001b[0m in \u001b[0;36mfeed_forward\u001b[0;34m(self, input_values)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mdprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_input_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_input_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/local/LogisticRegression-ANN/NN/network.py\u001b[0m in \u001b[0;36mdprint\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mdprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "... last 1 frames repeated, from the frame below ...\n",
      "\u001b[0;32m/notebooks/local/LogisticRegression-ANN/NN/network.py\u001b[0m in \u001b[0;36mdprint\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mdprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
     ]
    }
   ],
   "source": [
    "reload(loss_functions)\n",
    "reload(activation_functions)\n",
    "reload(network)\n",
    "network.DEBUG = True\n",
    "\n",
    "X = np.array([[0.05, 0.10]])\n",
    "\n",
    "Wh = np.array([[0.15, 0.2], [0.25, 0.3]]).T\n",
    "h = network.Layer(2, 2, 'sigmoid', weights=Wh, bias=0.35, label=\"H1\")\n",
    "\n",
    "Wo = np.array([[0.4, 0.45], [0.50, 0.55]]).T\n",
    "o = network.Layer(2, 2, 'sigmoid', weights=Wo, bias=0.6, label=\"H1\")\n",
    "Y = np.array([0.01, 0.99])\n",
    "\n",
    "for i in range(12):\n",
    "    netH, outH = h.feed_forward(X)\n",
    "#     print(netH, outH)\n",
    "    netO, outO = o.feed_forward(outH)\n",
    "#     print(netO, outO)\n",
    "    \n",
    "    Etotal = loss_functions.smd(outO, Y)\n",
    "    if (i % 100) == 0:\n",
    "        print(i, Etotal)\n",
    "    \n",
    "    dEo_dw = loss_functions.smd_derivative_chain(outO, Y)\n",
    "    print(dEo_dw)\n",
    "    print (\"\")\n",
    "    print (\"==========================================\")\n",
    "    print (\"Back Propagate Layer O\")\n",
    "    print (\"==========================================\")\n",
    "    o.backpropagate(dETotal_dOut=dEo_dw)\n",
    "    print (\"==========================================\")\n",
    "    print (\"\")\n",
    "\n",
    "    print (\"==========================================\")\n",
    "    print (\"Back Propagate Layer H\")\n",
    "    print (\"==========================================\")\n",
    "    h.backpropagate(output_layer=o)\n",
    "    print (\"==========================================\")\n",
    "\n",
    "print (outO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Sample and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Weights\n",
      "-------------------------------\n",
      "H1      (input=2, neurons=5, activation=sigmoid)\n",
      "[[0.14532581 0.54242371 0.32923608 0.84917855 0.17166915]\n",
      " [0.68479435 0.81146832 0.84598937 0.07393108 0.96680069]]\n",
      "output  (input=5, neurons=2, activation=sigmoid)\n",
      "[[0.83999453 0.44643036]\n",
      " [0.22607283 0.04750769]\n",
      " [0.22365104 0.24864697]\n",
      " [0.07484595 0.45393497]\n",
      " [0.37883489 0.3587275 ]]\n",
      "-------------------------------\n",
      "H1 (1, 2) (2, 5)\n",
      "net [[1.07574573 1.10826802 1.10106074 1.04985204 1.10526353]]\n",
      "out [[0.74568806 0.75180608 0.7504588  0.74074648 0.75124503]]\n",
      "output (1, 5) (5, 2)\n",
      "net [[2.30421741 2.16095665]]\n",
      "out [[0.90922572 0.89668821]]\n",
      "Y\n",
      " [[2.30421741 2.16095665]]\n",
      "\n",
      "a(Y)\n",
      " [[0.90922572 0.89668821]]\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(network)\n",
    "model = network.NN(loss='cross_entropy')\n",
    "model.add_layer(network.Layer(2, 5, 'sigmoid',  label=\"H1\"))\n",
    "model.add_layer(network.Layer(5, 2, 'sigmoid',  label=\"output\"))\n",
    "model.show_weights()\n",
    "X = np.array([[0.05, 0.10]])\n",
    "Y, aY = model.feed_forward(X)\n",
    "\n",
    "print(\"Y\\n\",Y)\n",
    "print(\"\\na(Y)\\n\",aY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the loss functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 2.7  8.1 16.2]\n",
      "1 [ 2.48  7.44 14.88]\n",
      "2 [ 2.282  6.846 13.692]\n",
      "3 [ 2.1038  6.3114 12.6228]\n",
      "4 [ 1.94342  5.83026 11.66052]\n",
      "5 [ 1.799078  5.397234 10.794468]\n",
      "6 [ 1.6691702  5.0075106 10.0150212]\n",
      "7 [1.55225318 4.65675954 9.31351908]\n",
      "8 [1.44702786 4.34108359 8.68216717]\n",
      "9 [1.35232508 4.05697523 8.11395045]\n",
      "10 [1.26709257 3.8012777  7.60255541]\n",
      "11 [1.19038331 3.57114993 7.14229987]\n",
      "12 [1.12134498 3.36403494 6.72806988]\n",
      "13 [1.05921048 3.17763145 6.35526289]\n",
      "14 [1.00328943 3.0098683  6.0197366 ]\n",
      "15 [0.95296049 2.85888147 5.71776294]\n",
      "16 [0.90766444 2.72299332 5.44598665]\n",
      "17 [0.866898   2.60069399 5.20138798]\n",
      "18 [0.8302082  2.49062459 4.98124919]\n",
      "19 [0.79718738 2.39156213 4.78312427]\n",
      "20 [0.76746864 2.30240592 4.60481184]\n",
      "21 [0.74072178 2.22216533 4.44433066]\n",
      "22 [0.7166496  2.1499488  4.29989759]\n",
      "23 [0.69498464 2.08495392 4.16990783]\n",
      "24 [0.67548617 2.02645852 4.05291705]\n",
      "25 [0.65793756 1.97381267 3.94762534]\n",
      "26 [0.6421438  1.9264314  3.85286281]\n",
      "27 [0.62792942 1.88378826 3.76757653]\n",
      "28 [0.61513648 1.84540944 3.69081888]\n",
      "29 [0.60362283 1.81086849 3.62173699]\n",
      "30 [0.59326055 1.77978164 3.55956329]\n",
      "31 [0.58393449 1.75180348 3.50360696]\n",
      "32 [0.57554104 1.72662313 3.45324626]\n",
      "33 [0.56798694 1.70396082 3.40792164]\n",
      "34 [0.56118825 1.68356474 3.36712947]\n",
      "35 [0.55506942 1.66520826 3.33041653]\n",
      "36 [0.54956248 1.64868744 3.29737487]\n",
      "37 [0.54460623 1.63381869 3.26763739]\n",
      "38 [0.54014561 1.62043682 3.24087365]\n",
      "39 [0.53613105 1.60839314 3.21678628]\n",
      "40 [0.53251794 1.59755383 3.19510765]\n",
      "41 [0.52926615 1.58779844 3.17559689]\n",
      "42 [0.52633953 1.5790186  3.1580372 ]\n",
      "43 [0.52370558 1.57111674 3.14223348]\n",
      "44 [0.52133502 1.56400507 3.12801013]\n",
      "45 [0.51920152 1.55760456 3.11520912]\n",
      "46 [0.51728137 1.5518441  3.10368821]\n",
      "47 [0.51555323 1.54665969 3.09331939]\n",
      "48 [0.51399791 1.54199372 3.08398745]\n",
      "49 [0.51259812 1.53779435 3.0755887 ]\n",
      "50 [0.51133831 1.53401492 3.06802983]\n",
      "51 [0.51020447 1.53061342 3.06122685]\n",
      "52 [0.50918403 1.52755208 3.05510416]\n",
      "53 [0.50826562 1.52479687 3.04959375]\n",
      "54 [0.50743906 1.52231719 3.04463437]\n",
      "55 [0.50669516 1.52008547 3.04017094]\n",
      "56 [0.50602564 1.51807692 3.03615384]\n",
      "57 [0.50542308 1.51626923 3.03253846]\n",
      "58 [0.50488077 1.51464231 3.02928461]\n",
      "59 [0.50439269 1.51317808 3.02635615]\n",
      "60 [0.50395342 1.51186027 3.02372054]\n",
      "61 [0.50355808 1.51067424 3.02134848]\n",
      "62 [0.50320227 1.50960682 3.01921363]\n",
      "63 [0.50288205 1.50864614 3.01729227]\n"
     ]
    }
   ],
   "source": [
    "reload(loss_functions)\n",
    "I = np.array([[1.5], [4.5], [9]])\n",
    "Y_  = np.array([[0.5], [1.5], [3]])\n",
    "W = np.array([[1.8]])\n",
    "lr = 0.01\n",
    "eps  = 0.01\n",
    "max_it = 100\n",
    "for i in range(max_it):\n",
    "    aY  = I * W\n",
    "    print(i, aY.flatten())\n",
    "    if np.absolute((Y_ - aY)).mean() < eps: \n",
    "        break\n",
    "    dC = loss_functions.mse_derivative(I, aY, Y_)\n",
    "    W = W - lr * dC\n",
    "\n",
    "assert(i > 0)\n",
    "assert(np.absolute((Y_ - aY)).mean() < eps)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 2.7  8.1 16.2]\n",
      "1 [0.423 1.269 2.538]\n",
      "2 [0.502695 1.508085 3.01617 ]\n"
     ]
    }
   ],
   "source": [
    "reload(loss_functions)\n",
    "I = np.array([[1.5], [4.5], [9]])\n",
    "Y_  = np.array([[0.5], [1.5], [3]])\n",
    "W = np.array([[1.8]])\n",
    "\n",
    "for i in range(max_it):\n",
    "    aY  = I * W\n",
    "    print(i, aY.flatten())\n",
    "    if np.absolute((Y_ - aY)).mean() < eps: \n",
    "        break\n",
    "    dC = loss_functions.cross_entropy_derivative(I, aY, Y_)\n",
    "    W = W - lr * dC\n",
    "\n",
    "assert(i > 0)\n",
    "assert(np.absolute((Y_ - aY)).mean() < eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum of Squared Errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 2.7  8.1 16.2]\n",
      "1 [0.423 1.269 2.538]\n",
      "2 [0.502695 1.508085 3.01617 ]\n"
     ]
    }
   ],
   "source": [
    "reload(loss_functions)\n",
    "I = np.array([[1.5], [4.5], [9]])\n",
    "Y_  = np.array([[0.5], [1.5], [3]])\n",
    "W = np.array([[1.8]])\n",
    "\n",
    "for i in range(max_it):\n",
    "    aY  = I * W\n",
    "    print(i, aY.flatten())\n",
    "    if np.absolute((Y_ - aY)).mean() < eps: \n",
    "        break\n",
    "    dC = loss_functions.smd_derivative(I, aY, Y_)\n",
    "    W = W - lr * dC\n",
    "assert(i > 0)\n",
    "assert(np.absolute((Y_ - aY)).mean() < eps)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
