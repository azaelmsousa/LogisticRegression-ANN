{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from NN import network, activation_functions, loss_functions\n",
    "from NN import ANN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Run Checks\n",
    "To identify if there is no code broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3775 0.3925]]\n",
      "[[0.59326999 0.59688438]]\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(network)\n",
    "I = np.array([[0.05, 0.10]])\n",
    "W = np.array([[0.15, 0.2], [0.25, 0.3]]).T\n",
    "h1 = network.Layer(2, 2, 'sigmoid', weights=W, bias=0.35, label=\"H1\")\n",
    "netH, outH =  h1.feed_forward(I)\n",
    "print(netH)\n",
    "print(outH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.10590597 1.2249214 ]]\n",
      "[[0.75136507 0.77292847]]\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(network)\n",
    "I = np.array([[0.59326999, 0.59688438]])\n",
    "W = np.array([[0.4, 0.45], [0.50, 0.55]]).T\n",
    "o1 = network.Layer(2, 2, 'sigmoid', weights=W, bias=0.6, label=\"H1\")\n",
    "netO, outO =  o1.feed_forward(I)\n",
    "print(netO)\n",
    "print(outO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergence Checkings\n",
    "Based on the sample given by the class teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y\n",
      " [1.10590597, 1.2249214]\n",
      "\n",
      "a(Y)\n",
      " [0.75136507, 0.77292847]\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(network)\n",
    "model = network.NN(loss='smd')\n",
    "X = np.array([[0.05, 0.10]])\n",
    "\n",
    "Wh = np.array([[0.15, 0.2], [0.25, 0.3]]).T\n",
    "h = network.Layer(2, 2, 'sigmoid', weights=Wh, bias=0.35, label=\"H1\")\n",
    "\n",
    "Wo = np.array([[0.4, 0.45], [0.50, 0.55]]).T\n",
    "o = network.Layer(2, 2, 'sigmoid', weights=Wo, bias=0.6, label=\"H1\")\n",
    "model.add_layer(h)\n",
    "model.add_layer(o)\n",
    "model.show_weights()\n",
    "\n",
    "Y, aY = model.feed_forward(X)\n",
    "Y_, aY_ = ([1.10590597, 1.2249214 ], [0.75136507, 0.77292847])\n",
    "print(\"Y\\n\",Y_)\n",
    "print(\"\\na(Y)\\n\",aY_)\n",
    "\n",
    "assert((Y - Y_).sum() < np.finfo(np.float32).eps)\n",
    "assert((aY - aY_).sum() < np.finfo(np.float32).eps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on the back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2983711087600027\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(network)\n",
    "model.show_weights()\n",
    "Y = np.array([0.01, 0.99])\n",
    "Etotal = loss_functions.smd(aY, Y)\n",
    "print(Etotal)\n",
    "assert((Etotal - 0.2983711) < np.finfo(np.float32).eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emulating the weights update for the layer O "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.74136507 -0.21707153]]\n",
      "dout [[0.1868156  0.17551005]]\n",
      "dnet [[0.59326999 0.59688438]]\n",
      "delta [[ 0.13849856 -0.03809824]]\n",
      "dw [[ 0.08216704 -0.02274024]]\n",
      "update [[0.35891648 0.51137012]\n",
      " [0.40891648 0.56137012]]\n"
     ]
    }
   ],
   "source": [
    "reload(loss_functions)\n",
    "reload(activation_functions)\n",
    "lr = 0.5\n",
    "#Done - Partial\n",
    "dEo_dw = loss_functions.smd_derivative_chain(outO, Y)\n",
    "print(dEo_dw)\n",
    "\n",
    "# Done\n",
    "dOuto_Dneto = activation_functions.sigmoid_derivative_chain(outO)\n",
    "print('dout',dOuto_Dneto)\n",
    "\n",
    "# Done - Self.input\n",
    "dNeto  = outH\n",
    "print('dnet',dNeto) \n",
    "# Done\n",
    "deltaO  = dEo_dw * dOuto_Dneto\n",
    "print('delta', deltaO)\n",
    "\n",
    "dWO = deltaO * outH\n",
    "print('dw', dWO)\n",
    "# Done\n",
    "updateO = Wo - lr * dWO\n",
    "print('update', updateO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.05539942 -0.01904912]\n",
      " [ 0.06232435 -0.02095403]]\n",
      "dETotal_dOh [0.03635031 0.04137032]\n",
      "dOuth_Dneth [[0.24130071 0.24061342]]\n",
      "deltaH [[0.00877135 0.00995425]]\n",
      "dWh [[0.00043857 0.00099543]]\n",
      "update [[0.14978072 0.24950229]\n",
      " [0.19978072 0.29950229]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Done - Partial\n",
    "dEo_dOh = deltaO * Wo\n",
    "dETotal_dOh = dEo_dOh.sum(axis=1)\n",
    "print (dEo_dOh)\n",
    "print('dETotal_dOh', dETotal_dOh)\n",
    "\n",
    "# Done\n",
    "dOuth_Dneth = activation_functions.sigmoid_derivative_chain(outH)\n",
    "print('dOuth_Dneth', dOuth_Dneth)\n",
    "\n",
    "\n",
    "# Done\n",
    "deltaH = dETotal_dOh * dOuth_Dneth \n",
    "print('deltaH', deltaH)\n",
    "\n",
    "# done\n",
    "# self.input\n",
    "dNeth_dw = X\n",
    "dWh = deltaH * dNeth_dw\n",
    "print('dWh', dWh)\n",
    "\n",
    "# Done\n",
    "updateH = Wh - lr * dWh\n",
    "print('update', updateH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the implemented Grad calculation for Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 error 0.2983711087600027\n",
      "1000 error 0.001114349453733746\n",
      "2000 error 0.00044486770391326887\n",
      "3000 error 0.00025152360241099405\n",
      "4000 error 0.00016427727363225536\n",
      "5000 error 0.00011622569943793622\n",
      "6000 error 8.653979465934606e-05\n",
      "7000 error 6.676957838285888e-05\n",
      "8000 error 5.288555201099971e-05\n",
      "9000 error 4.2742284255545085e-05\n",
      "[[0.01591419 0.98406371]]\n"
     ]
    }
   ],
   "source": [
    "reload(loss_functions)\n",
    "reload(activation_functions)\n",
    "reload(network)\n",
    "network.DEBUG = False\n",
    "\n",
    "X = np.array([[0.05, 0.10]], np.float64)\n",
    "\n",
    "Wh = np.array([[0.15, 0.2], [0.25, 0.3]], np.float64).T\n",
    "h = network.Layer(2, 2, 'sigmoid', weights=Wh, bias=0.35, label=\"H1\")\n",
    "\n",
    "Wo = np.array([[0.4, 0.45], [0.50, 0.55]], np.float64).T\n",
    "o = network.Layer(2, 2, 'sigmoid', weights=Wo, bias=0.6, label=\"H1\")\n",
    "Y = np.array([0.01, 0.99], np.float64)\n",
    "\n",
    "for i in range(10000):\n",
    "    netH, outH = h.feed_forward(X)\n",
    "    netO, outO = o.feed_forward(outH)\n",
    "\n",
    "    \n",
    "    Etotal = loss_functions.smd(outO, Y)\n",
    "    if (i % 1000) == 0:\n",
    "        print(i, 'error', Etotal)\n",
    "    \n",
    "    dEo_dw = loss_functions.smd_derivative_chain(outO, Y)\n",
    "    network.dprint(dEo_dw)\n",
    "    network.dprint (\"\")\n",
    "    network.dprint (\"==========================================\")\n",
    "    network.dprint (\"Back Propagate Layer O\")\n",
    "    network.dprint (\"==========================================\")\n",
    "    o.backpropagate(dETotal_dOut=dEo_dw)\n",
    "    network.dprint (\"==========================================\")\n",
    "    network.dprint (\"\")\n",
    "\n",
    "    network.dprint (\"==========================================\")\n",
    "    network.dprint (\"Back Propagate Layer H\")\n",
    "    network.dprint (\"==========================================\")\n",
    "    h.backpropagate(output_layer=o)\n",
    "    network.dprint (\"==========================================\")\n",
    "\n",
    "print (outO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Fit Methods with the full network calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMD as the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled\n",
      "It: 2000 Batch: 1 Epoch 1999 Error: 0.01027066 lr: 0.500000 \n",
      "It: 4000 Batch: 1 Epoch 3999 Error: 0.00054305 lr: 0.500000 \n",
      "It: 6000 Batch: 1 Epoch 5999 Error: 0.00025539 lr: 0.500000 \n",
      "It: 8000 Batch: 1 Epoch 7999 Error: 0.00015284 lr: 0.500000 \n",
      "It: 10000 Batch: 1 Epoch 9999 Error: 0.00010231 lr: 0.500000 \n",
      "It: 12000 Batch: 1 Epoch 11999 Error: 0.00007315 lr: 0.500000 \n",
      "It: 14000 Batch: 1 Epoch 13999 Error: 0.00005464 lr: 0.500000 \n",
      "It: 16000 Batch: 1 Epoch 15999 Error: 0.00004211 lr: 0.500000 \n",
      "It: 18000 Batch: 1 Epoch 17999 Error: 0.00003323 lr: 0.500000 \n",
      "It: 20000 Batch: 1 Epoch 19999 Error: 0.00002670 lr: 0.500000 \n",
      "It: 22000 Batch: 1 Epoch 21999 Error: 0.00002177 lr: 0.500000 \n",
      "It: 24000 Batch: 1 Epoch 23999 Error: 0.00001797 lr: 0.500000 \n",
      "It: 26000 Batch: 1 Epoch 25999 Error: 0.00001498 lr: 0.500000 \n",
      "It: 28000 Batch: 1 Epoch 27999 Error: 0.00001259 lr: 0.500000 \n",
      "It: 30000 Batch: 1 Epoch 29999 Error: 0.00001066 lr: 0.500000 \n",
      "Finished \n",
      " It: 30001 Batch: 1 Epoch 30000 Train Loss: 0.00000982 lr: 0.500000 \n",
      "[[0.01311912 0.98685128]] 0.0031339173313234357\n"
     ]
    }
   ],
   "source": [
    "from utils import dataset_helper\n",
    "reload(loss_functions)\n",
    "reload(activation_functions)\n",
    "reload(network)\n",
    "reload(dataset_helper)\n",
    "network.DEBUG = False\n",
    "eps = 0.00001\n",
    "\n",
    "X = np.array([[0.05, 0.10]], np.float64)\n",
    "Y = np.array([[0.01, 0.99]], np.float64)\n",
    "\n",
    "Wh = np.array([[0.15, 0.2], [0.25, 0.3]], np.float64).T\n",
    "h = network.Layer(2, 2, 'sigmoid', weights=Wh, bias=0.35, label=\"H1\")\n",
    "\n",
    "Wo = np.array([[0.4, 0.45], [0.50, 0.55]], np.float64).T\n",
    "o = network.Layer(2, 2, 'sigmoid', weights=Wo, bias=0.6, label=\"Output\")\n",
    "\n",
    "model = network.NN(loss='smd')\n",
    "model.add_layer(h)\n",
    "model.add_layer(o)\n",
    "model.show_weights()\n",
    "\n",
    "model.fit(X, Y, max_iter=50000, \n",
    "          lr=0.5, epsilon=eps, \n",
    "          print_interval=2000)\n",
    "\n",
    "_, Y_ = model.feed_forward(X)\n",
    "\n",
    "mae = np.absolute(Y - Y_).mean()\n",
    "print(Y_, mae)\n",
    "assert(mae < 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled\n",
      "It: 2000 Batch: 1 Epoch 1999 Error: 0.44643445 lr: 0.100000 \n",
      "It: 4000 Batch: 1 Epoch 3999 Error: 0.19864310 lr: 0.100000 \n",
      "It: 6000 Batch: 1 Epoch 5999 Error: 0.16640629 lr: 0.100000 \n",
      "It: 8000 Batch: 1 Epoch 7999 Error: 0.15169125 lr: 0.100000 \n",
      "It: 10000 Batch: 1 Epoch 9999 Error: 0.14308035 lr: 0.100000 \n",
      "It: 12000 Batch: 1 Epoch 11999 Error: 0.13738293 lr: 0.100000 \n",
      "It: 14000 Batch: 1 Epoch 13999 Error: 0.13332083 lr: 0.100000 \n",
      "It: 16000 Batch: 1 Epoch 15999 Error: 0.13027448 lr: 0.100000 \n",
      "It: 18000 Batch: 1 Epoch 17999 Error: 0.12790469 lr: 0.100000 \n",
      "It: 20000 Batch: 1 Epoch 19999 Error: 0.12600928 lr: 0.100000 \n",
      "It: 22000 Batch: 1 Epoch 21999 Error: 0.12445982 lr: 0.100000 \n",
      "It: 24000 Batch: 1 Epoch 23999 Error: 0.12317068 lr: 0.100000 \n",
      "It: 26000 Batch: 1 Epoch 25999 Error: 0.12208243 lr: 0.100000 \n",
      "It: 28000 Batch: 1 Epoch 27999 Error: 0.12115250 lr: 0.100000 \n",
      "It: 30000 Batch: 1 Epoch 29999 Error: 0.12034958 lr: 0.100000 \n",
      "It: 32000 Batch: 1 Epoch 31999 Error: 0.11965011 lr: 0.100000 \n",
      "It: 34000 Batch: 1 Epoch 33999 Error: 0.11903600 lr: 0.100000 \n",
      "It: 36000 Batch: 1 Epoch 35999 Error: 0.11849314 lr: 0.100000 \n",
      "It: 38000 Batch: 1 Epoch 37999 Error: 0.11801034 lr: 0.100000 \n",
      "It: 40000 Batch: 1 Epoch 39999 Error: 0.11757864 lr: 0.100000 \n",
      "It: 42000 Batch: 1 Epoch 41999 Error: 0.11719075 lr: 0.100000 \n",
      "It: 44000 Batch: 1 Epoch 43999 Error: 0.11684070 lr: 0.100000 \n",
      "It: 46000 Batch: 1 Epoch 45999 Error: 0.11652355 lr: 0.100000 \n",
      "It: 48000 Batch: 1 Epoch 47999 Error: 0.11623515 lr: 0.100000 \n",
      "It: 50000 Batch: 1 Epoch 49999 Error: 0.11597204 lr: 0.100000 \n",
      "Finished \n",
      " It: 50000 Batch: 1 Epoch 49999 Train Loss: 0.11597204 lr: 0.100000 \n",
      "[[0.01738492 0.98243094]] 0.007476988809858003\n"
     ]
    }
   ],
   "source": [
    "from utils import dataset_helper\n",
    "reload(loss_functions)\n",
    "reload(activation_functions)\n",
    "reload(network)\n",
    "reload(dataset_helper)\n",
    "network.DEBUG = False\n",
    "eps = 0.0001\n",
    "X = np.array([[0.05, 0.10]], np.float64)\n",
    "Y = np.array([[0.01, 0.99]], np.float64)\n",
    "\n",
    "Wh = np.array([[0.15, 0.2], [0.25, 0.3]], np.float64).T\n",
    "h = network.Layer(2, 3, 'sigmoid', weights=Wh, bias=0.35, label=\"H1\")\n",
    "\n",
    "Wo = np.array([[0.4, 0.45], [0.50, 0.55]], np.float64).T\n",
    "o = network.Layer(3, 2, 'sigmoid', weights=Wo, bias=0.6, label=\"Output\")\n",
    "\n",
    "model = network.NN(loss='cross_entropy')\n",
    "model.add_layer(h)\n",
    "model.add_layer(o)\n",
    "model.show_weights()\n",
    "\n",
    "model.fit(X, Y, max_iter=50000, \n",
    "          lr=0.1, epsilon=eps, \n",
    "          print_interval=2000)\n",
    "\n",
    "_, Y_ = model.feed_forward(X)\n",
    "\n",
    "mae = np.absolute(Y - Y_).mean()\n",
    "print(Y_, mae)\n",
    "assert(mae < 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled\n",
      "It: 2000 Batch: 2 Epoch 666 Error: 1.39215283 lr: 0.100000 \n",
      "It: 4000 Batch: 1 Epoch 1333 Error: 1.26986741 lr: 0.100000 \n",
      "It: 6000 Batch: 3 Epoch 1999 Error: 1.26065637 lr: 0.100000 \n",
      "It: 8000 Batch: 2 Epoch 2666 Error: 1.24048291 lr: 0.100000 \n",
      "It: 10000 Batch: 1 Epoch 3333 Error: 1.20126148 lr: 0.100000 \n",
      "It: 12000 Batch: 3 Epoch 3999 Error: 1.12030552 lr: 0.100000 \n",
      "It: 14000 Batch: 2 Epoch 4666 Error: 0.97339175 lr: 0.100000 \n",
      "It: 16000 Batch: 1 Epoch 5333 Error: 0.76084179 lr: 0.100000 \n",
      "It: 18000 Batch: 3 Epoch 5999 Error: 0.55122539 lr: 0.100000 \n",
      "It: 20000 Batch: 2 Epoch 6666 Error: 0.40225974 lr: 0.100000 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7b31b0a960e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m model.fit(X, Y, max_iter=50000, \n\u001b[1;32m     23\u001b[0m           \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m           print_interval=2000)\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mY_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/local/LogisticRegression-ANN/NN/network.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, lr, max_iter, lr_optimizer, epsilon, power_t, t, print_interval, decay_iteractions, decay_rate, X_val, Y_val)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/local/LogisticRegression-ANN/NN/network.py\u001b[0m in \u001b[0;36mbackpropagate\u001b[0;34m(self, Y, lr)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mdprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mdprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"============================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     def fit(self, X, Y, lr=0.5,\n",
      "\u001b[0;32m/notebooks/local/LogisticRegression-ANN/NN/network.py\u001b[0m in \u001b[0;36mbackpropagate\u001b[0;34m(self, lr, output_layer, dETotal_dOut)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dETotal_dOut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/local/LogisticRegression-ANN/NN/network.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, lr)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# Here starts the back propagation implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mdprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'self.weights'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils import dataset_helper\n",
    "reload(loss_functions)\n",
    "reload(activation_functions)\n",
    "reload(network)\n",
    "reload(dataset_helper)\n",
    "network.DEBUG = False\n",
    "\n",
    "eps = np.finfo(np.float32).eps\n",
    "\n",
    "X = np.array([[0.05, 0.10], [0.05, 0.10], [0.10, 0.05]], np.float64)\n",
    "Y = np.array([[0.0, 1], [0.0, 1], [1., 0.]], np.float64)\n",
    "\n",
    "h1 = network.Layer(2, 10, 'sigmoid',  label=\"H1\")\n",
    "h2 = network.Layer(10, 10, 'sigmoid',   label=\"H2\")\n",
    "o = network.Layer(10, 2, 'sigmoid',  label=\"Output\")\n",
    "\n",
    "model = network.NN(loss='cross_entropy')\n",
    "model.add_layer(h1)\n",
    "model.add_layer(o)\n",
    "model.show_weights()\n",
    "\n",
    "model.fit(X, Y, max_iter=50000, \n",
    "          lr=0.1, epsilon=eps,\n",
    "          print_interval=2000)\n",
    "\n",
    "Y_ = np.array(model.predict(X))\n",
    "Y_ = Y_.argmax(axis=-1).flatten()\n",
    "Y = Y.argmax(axis=-1)\n",
    "mae = np.absolute(Y - Y_).mean()\n",
    "print(mae)\n",
    "print(Y, Y_)\n",
    "assert(mae < 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi class Classification Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_one_hot(data, nb_classes):\n",
    "    \"\"\"Convert an iterable of indices to one-hot encoded labels.\"\"\"\n",
    "    targets = np.array(data).reshape(-1)\n",
    "    return np.eye(nb_classes)[targets]\n",
    "indices_to_one_hot(np.array([1,2,3,0]), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shuffled\n",
      "It: 10000 Batch: 10000 Epoch 0 Train Loss: 24.84087192 lr: 0.900000 Val Loss: 12.74812872\n",
      "It: 20000 Batch: 20000 Epoch 0 Train Loss: 6.81616909 lr: 0.900000 Val Loss: 3.08417154\n",
      "It: 30000 Batch: 30000 Epoch 0 Train Loss: 3.22120735 lr: 0.900000 Val Loss: 3.25598758\n",
      "It: 40000 Batch: 40000 Epoch 0 Train Loss: 3.21627919 lr: 0.900000 Val Loss: 2.92071930\n",
      "It: 50000 Batch: 2000 Epoch 1 Train Loss: 3.21353531 lr: 0.900000 Val Loss: 3.03072037\n",
      "It: 60000 Batch: 12000 Epoch 1 Train Loss: 3.21148551 lr: 0.900000 Val Loss: 4.28968763\n",
      "It: 70000 Batch: 22000 Epoch 1 Train Loss: 3.22601533 lr: 0.900000 Val Loss: 2.98803948\n",
      "It: 80000 Batch: 32000 Epoch 1 Train Loss: 3.21861120 lr: 0.900000 Val Loss: 4.32539879\n",
      "It: 90000 Batch: 42000 Epoch 1 Train Loss: 3.22750974 lr: 0.900000 Val Loss: 2.90155024\n",
      "It: 100000 Batch: 4000 Epoch 2 Train Loss: 3.21630811 lr: 0.900000 Val Loss: 4.20964222\n",
      "It: 110000 Batch: 14000 Epoch 2 Train Loss: 3.13444883 lr: 0.810000 Val Loss: 2.86949898\n",
      "It: 120000 Batch: 24000 Epoch 2 Train Loss: 3.13432590 lr: 0.810000 Val Loss: 3.57900771\n",
      "It: 130000 Batch: 34000 Epoch 2 Train Loss: 3.12994808 lr: 0.810000 Val Loss: 3.36760666\n",
      "It: 140000 Batch: 44000 Epoch 2 Train Loss: 3.13373229 lr: 0.810000 Val Loss: 3.73593591\n",
      "It: 150000 Batch: 6000 Epoch 3 Train Loss: 3.14549355 lr: 0.810000 Val Loss: 2.87286917\n",
      "It: 160000 Batch: 16000 Epoch 3 Train Loss: 3.13799804 lr: 0.810000 Val Loss: 3.44856369\n",
      "It: 170000 Batch: 26000 Epoch 3 Train Loss: 3.13386371 lr: 0.810000 Val Loss: 2.95881140\n",
      "It: 180000 Batch: 36000 Epoch 3 Train Loss: 3.13327242 lr: 0.810000 Val Loss: 2.87247199\n",
      "It: 190000 Batch: 46000 Epoch 3 Train Loss: 3.12606075 lr: 0.810000 Val Loss: 3.40967371\n",
      "It: 200000 Batch: 8000 Epoch 4 Train Loss: 3.10054370 lr: 0.810000 Val Loss: 4.18255840\n",
      "It: 210000 Batch: 18000 Epoch 4 Train Loss: 3.04148006 lr: 0.729000 Val Loss: 2.76555072\n",
      "It: 220000 Batch: 28000 Epoch 4 Train Loss: 3.01709580 lr: 0.729000 Val Loss: 2.96349208\n",
      "It: 230000 Batch: 38000 Epoch 4 Train Loss: 3.00846064 lr: 0.729000 Val Loss: 2.69765678\n",
      "It: 240000 Batch: 48000 Epoch 4 Train Loss: 3.00695743 lr: 0.729000 Val Loss: 4.06459278\n",
      "It: 250000 Batch: 10000 Epoch 5 Train Loss: 2.99970445 lr: 0.729000 Val Loss: 2.91707754\n",
      "It: 260000 Batch: 20000 Epoch 5 Train Loss: 3.00098385 lr: 0.729000 Val Loss: 2.78040211\n",
      "It: 270000 Batch: 30000 Epoch 5 Train Loss: 2.99937451 lr: 0.729000 Val Loss: 2.76670649\n",
      "It: 280000 Batch: 40000 Epoch 5 Train Loss: 2.99270600 lr: 0.729000 Val Loss: 2.75795310\n",
      "It: 290000 Batch: 2000 Epoch 6 Train Loss: 2.94004979 lr: 0.729000 Val Loss: 2.48466228\n",
      "It: 300000 Batch: 12000 Epoch 6 Train Loss: 2.84695863 lr: 0.729000 Val Loss: 2.42132162\n",
      "It: 310000 Batch: 22000 Epoch 6 Train Loss: 2.35608759 lr: 0.656100 Val Loss: 2.03880795\n",
      "It: 320000 Batch: 32000 Epoch 6 Train Loss: 2.21861183 lr: 0.656100 Val Loss: 2.01807398\n",
      "It: 330000 Batch: 42000 Epoch 6 Train Loss: 2.19875068 lr: 0.656100 Val Loss: 2.07068284\n",
      "It: 340000 Batch: 4000 Epoch 7 Train Loss: 2.17846741 lr: 0.656100 Val Loss: 2.61190169\n",
      "It: 350000 Batch: 14000 Epoch 7 Train Loss: 2.21285513 lr: 0.656100 Val Loss: 2.55882245\n",
      "It: 360000 Batch: 24000 Epoch 7 Train Loss: 2.19833393 lr: 0.656100 Val Loss: 1.99155566\n",
      "It: 370000 Batch: 34000 Epoch 7 Train Loss: 2.26443144 lr: 0.656100 Val Loss: 2.26276337\n",
      "It: 380000 Batch: 44000 Epoch 7 Train Loss: 2.17693492 lr: 0.656100 Val Loss: 2.08577209\n",
      "It: 390000 Batch: 6000 Epoch 8 Train Loss: 2.14107085 lr: 0.656100 Val Loss: 2.41777502\n",
      "It: 400000 Batch: 16000 Epoch 8 Train Loss: 2.16445493 lr: 0.656100 Val Loss: 2.52727415\n",
      "It: 410000 Batch: 26000 Epoch 8 Train Loss: 2.11965361 lr: 0.590490 Val Loss: 2.48184368\n",
      "It: 420000 Batch: 36000 Epoch 8 Train Loss: 2.05174378 lr: 0.590490 Val Loss: 2.14672843\n",
      "It: 430000 Batch: 46000 Epoch 8 Train Loss: 2.10864696 lr: 0.590490 Val Loss: 1.95642130\n",
      "It: 440000 Batch: 8000 Epoch 9 Train Loss: 2.06389521 lr: 0.590490 Val Loss: 2.35908016\n",
      "It: 450000 Batch: 18000 Epoch 9 Train Loss: 2.06852057 lr: 0.590490 Val Loss: 2.42118300\n",
      "It: 460000 Batch: 28000 Epoch 9 Train Loss: 2.09906349 lr: 0.590490 Val Loss: 1.93007367\n",
      "It: 470000 Batch: 38000 Epoch 9 Train Loss: 2.10578537 lr: 0.590490 Val Loss: 1.86318735\n",
      "It: 480000 Batch: 48000 Epoch 9 Train Loss: 2.05669167 lr: 0.590490 Val Loss: 2.34690776\n",
      "It: 490000 Batch: 10000 Epoch 10 Train Loss: 2.05157323 lr: 0.590490 Val Loss: 1.86854490\n",
      "It: 500000 Batch: 20000 Epoch 10 Train Loss: 2.05755546 lr: 0.590490 Val Loss: 2.04353435\n",
      "It: 510000 Batch: 30000 Epoch 10 Train Loss: 2.04453049 lr: 0.531441 Val Loss: 2.00170334\n",
      "It: 520000 Batch: 40000 Epoch 10 Train Loss: 2.05029747 lr: 0.531441 Val Loss: 1.91916964\n",
      "It: 530000 Batch: 2000 Epoch 11 Train Loss: 2.00947654 lr: 0.531441 Val Loss: 2.25262743\n",
      "It: 540000 Batch: 12000 Epoch 11 Train Loss: 1.97723496 lr: 0.531441 Val Loss: 1.89499719\n",
      "It: 550000 Batch: 22000 Epoch 11 Train Loss: 2.00673695 lr: 0.531441 Val Loss: 1.94364517\n",
      "It: 560000 Batch: 32000 Epoch 11 Train Loss: 1.99560124 lr: 0.531441 Val Loss: 1.91856585\n",
      "It: 570000 Batch: 42000 Epoch 11 Train Loss: 1.98802715 lr: 0.531441 Val Loss: 1.84524591\n",
      "It: 580000 Batch: 4000 Epoch 12 Train Loss: 1.97799211 lr: 0.531441 Val Loss: 2.00381351\n",
      "It: 590000 Batch: 14000 Epoch 12 Train Loss: 1.98159553 lr: 0.531441 Val Loss: 1.82603542\n",
      "It: 600000 Batch: 24000 Epoch 12 Train Loss: 2.00792570 lr: 0.531441 Val Loss: 2.21666240\n",
      "It: 610000 Batch: 34000 Epoch 12 Train Loss: 1.94300698 lr: 0.478297 Val Loss: 1.75642453\n",
      "It: 620000 Batch: 44000 Epoch 12 Train Loss: 1.96594167 lr: 0.478297 Val Loss: 1.99154527\n",
      "It: 630000 Batch: 6000 Epoch 13 Train Loss: 1.93333275 lr: 0.478297 Val Loss: 2.03720112\n",
      "It: 640000 Batch: 16000 Epoch 13 Train Loss: 1.94376987 lr: 0.478297 Val Loss: 1.87708301\n",
      "It: 650000 Batch: 26000 Epoch 13 Train Loss: 1.93106401 lr: 0.478297 Val Loss: 1.79154123\n",
      "It: 660000 Batch: 36000 Epoch 13 Train Loss: 1.89467750 lr: 0.478297 Val Loss: 1.73567603\n",
      "It: 670000 Batch: 46000 Epoch 13 Train Loss: 1.89874493 lr: 0.478297 Val Loss: 2.43842879\n",
      "It: 680000 Batch: 8000 Epoch 14 Train Loss: 1.90630399 lr: 0.478297 Val Loss: 1.72034537\n",
      "It: 690000 Batch: 18000 Epoch 14 Train Loss: 1.91335069 lr: 0.478297 Val Loss: 1.82203311\n",
      "It: 700000 Batch: 28000 Epoch 14 Train Loss: 1.91664757 lr: 0.478297 Val Loss: 1.87627239\n",
      "It: 710000 Batch: 38000 Epoch 14 Train Loss: 1.85902354 lr: 0.430467 Val Loss: 1.88566505\n",
      "It: 720000 Batch: 48000 Epoch 14 Train Loss: 1.83035147 lr: 0.430467 Val Loss: 1.69227834\n",
      "It: 730000 Batch: 10000 Epoch 15 Train Loss: 1.83857668 lr: 0.430467 Val Loss: 1.87234784\n",
      "It: 740000 Batch: 20000 Epoch 15 Train Loss: 1.83405977 lr: 0.430467 Val Loss: 1.80202978\n",
      "It: 750000 Batch: 30000 Epoch 15 Train Loss: 1.85121534 lr: 0.430467 Val Loss: 1.68989240\n",
      "It: 760000 Batch: 40000 Epoch 15 Train Loss: 1.83351060 lr: 0.430467 Val Loss: 2.02407405\n",
      "It: 770000 Batch: 2000 Epoch 16 Train Loss: 1.88303229 lr: 0.430467 Val Loss: 1.77363955\n",
      "It: 780000 Batch: 12000 Epoch 16 Train Loss: 1.82252138 lr: 0.430467 Val Loss: 1.81086713\n",
      "It: 790000 Batch: 22000 Epoch 16 Train Loss: 1.83147231 lr: 0.430467 Val Loss: 1.79917299\n",
      "It: 800000 Batch: 32000 Epoch 16 Train Loss: 1.87588925 lr: 0.430467 Val Loss: 2.10021354\n",
      "It: 810000 Batch: 42000 Epoch 16 Train Loss: 1.81726210 lr: 0.387420 Val Loss: 1.77308584\n",
      "It: 820000 Batch: 4000 Epoch 17 Train Loss: 1.81895738 lr: 0.387420 Val Loss: 1.69419289\n",
      "It: 830000 Batch: 14000 Epoch 17 Train Loss: 1.76718805 lr: 0.387420 Val Loss: 1.69219888\n",
      "It: 840000 Batch: 24000 Epoch 17 Train Loss: 1.81702906 lr: 0.387420 Val Loss: 1.68971998\n",
      "It: 850000 Batch: 34000 Epoch 17 Train Loss: 1.81413622 lr: 0.387420 Val Loss: 1.80931553\n",
      "It: 860000 Batch: 44000 Epoch 17 Train Loss: 1.82547671 lr: 0.387420 Val Loss: 2.16930122\n",
      "It: 870000 Batch: 6000 Epoch 18 Train Loss: 1.83086006 lr: 0.387420 Val Loss: 1.66235392\n",
      "It: 880000 Batch: 16000 Epoch 18 Train Loss: 1.77891246 lr: 0.387420 Val Loss: 1.79378954\n",
      "It: 890000 Batch: 26000 Epoch 18 Train Loss: 1.82629632 lr: 0.387420 Val Loss: 1.91918759\n",
      "It: 900000 Batch: 36000 Epoch 18 Train Loss: 1.81633450 lr: 0.387420 Val Loss: 1.72904140\n",
      "It: 910000 Batch: 46000 Epoch 18 Train Loss: 1.78338784 lr: 0.348678 Val Loss: 1.72026763\n",
      "It: 920000 Batch: 8000 Epoch 19 Train Loss: 1.77446968 lr: 0.348678 Val Loss: 1.81778020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 930000 Batch: 18000 Epoch 19 Train Loss: 1.77083869 lr: 0.348678 Val Loss: 1.69397969\n",
      "It: 940000 Batch: 28000 Epoch 19 Train Loss: 1.74447353 lr: 0.348678 Val Loss: 1.66076428\n",
      "It: 950000 Batch: 38000 Epoch 19 Train Loss: 1.76743259 lr: 0.348678 Val Loss: 1.73094647\n",
      "It: 960000 Batch: 48000 Epoch 19 Train Loss: 1.77321208 lr: 0.348678 Val Loss: 1.66826071\n",
      "It: 970000 Batch: 10000 Epoch 20 Train Loss: 1.73889178 lr: 0.348678 Val Loss: 1.76681763\n",
      "It: 980000 Batch: 20000 Epoch 20 Train Loss: 1.79543158 lr: 0.348678 Val Loss: 1.65476881\n",
      "It: 990000 Batch: 30000 Epoch 20 Train Loss: 1.74007315 lr: 0.348678 Val Loss: 1.74744515\n",
      "It: 1000000 Batch: 40000 Epoch 20 Train Loss: 1.75468573 lr: 0.348678 Val Loss: 1.63396254\n",
      "It: 1010000 Batch: 2000 Epoch 21 Train Loss: 1.73992684 lr: 0.313811 Val Loss: 1.64142493\n",
      "It: 1020000 Batch: 12000 Epoch 21 Train Loss: 1.72141557 lr: 0.313811 Val Loss: 1.60015546\n",
      "It: 1030000 Batch: 22000 Epoch 21 Train Loss: 1.70340441 lr: 0.313811 Val Loss: 1.69522551\n",
      "It: 1040000 Batch: 32000 Epoch 21 Train Loss: 1.73835270 lr: 0.313811 Val Loss: 1.67679600\n",
      "It: 1050000 Batch: 42000 Epoch 21 Train Loss: 1.73980940 lr: 0.313811 Val Loss: 1.76730830\n",
      "It: 1060000 Batch: 4000 Epoch 22 Train Loss: 1.72127431 lr: 0.313811 Val Loss: 1.75553236\n",
      "It: 1070000 Batch: 14000 Epoch 22 Train Loss: 1.73071699 lr: 0.313811 Val Loss: 1.65400235\n",
      "It: 1080000 Batch: 24000 Epoch 22 Train Loss: 1.74598404 lr: 0.313811 Val Loss: 1.77424386\n",
      "It: 1090000 Batch: 34000 Epoch 22 Train Loss: 1.72749714 lr: 0.313811 Val Loss: 1.64443384\n",
      "It: 1100000 Batch: 44000 Epoch 22 Train Loss: 1.71397819 lr: 0.313811 Val Loss: 1.73614972\n",
      "It: 1110000 Batch: 6000 Epoch 23 Train Loss: 1.71788250 lr: 0.282430 Val Loss: 1.95852965\n",
      "It: 1120000 Batch: 16000 Epoch 23 Train Loss: 1.71040509 lr: 0.282430 Val Loss: 1.66555057\n",
      "It: 1130000 Batch: 26000 Epoch 23 Train Loss: 1.67369597 lr: 0.282430 Val Loss: 2.03827720\n",
      "It: 1140000 Batch: 36000 Epoch 23 Train Loss: 1.69352774 lr: 0.282430 Val Loss: 1.64615265\n",
      "It: 1150000 Batch: 46000 Epoch 23 Train Loss: 1.68912145 lr: 0.282430 Val Loss: 1.65526634\n",
      "It: 1160000 Batch: 8000 Epoch 24 Train Loss: 1.73327283 lr: 0.282430 Val Loss: 1.61955861\n",
      "It: 1170000 Batch: 18000 Epoch 24 Train Loss: 1.71055938 lr: 0.282430 Val Loss: 1.66363408\n",
      "It: 1180000 Batch: 28000 Epoch 24 Train Loss: 1.67997242 lr: 0.282430 Val Loss: 1.91140773\n",
      "It: 1190000 Batch: 38000 Epoch 24 Train Loss: 1.71333154 lr: 0.282430 Val Loss: 1.64151995\n",
      "It: 1200000 Batch: 48000 Epoch 24 Train Loss: 1.69061445 lr: 0.282430 Val Loss: 1.65275179\n",
      "It: 1210000 Batch: 10000 Epoch 25 Train Loss: 1.66113174 lr: 0.254187 Val Loss: 1.83705993\n",
      "It: 1220000 Batch: 20000 Epoch 25 Train Loss: 1.67869507 lr: 0.254187 Val Loss: 1.64219454\n",
      "It: 1230000 Batch: 30000 Epoch 25 Train Loss: 1.68480108 lr: 0.254187 Val Loss: 1.66271445\n",
      "It: 1240000 Batch: 40000 Epoch 25 Train Loss: 1.67204296 lr: 0.254187 Val Loss: 1.64606117\n",
      "It: 1250000 Batch: 2000 Epoch 26 Train Loss: 1.67510032 lr: 0.254187 Val Loss: 1.66544300\n",
      "It: 1260000 Batch: 12000 Epoch 26 Train Loss: 1.67253024 lr: 0.254187 Val Loss: 1.60353173\n",
      "It: 1270000 Batch: 22000 Epoch 26 Train Loss: 1.66854170 lr: 0.254187 Val Loss: 1.62568159\n",
      "It: 1280000 Batch: 32000 Epoch 26 Train Loss: 1.67176039 lr: 0.254187 Val Loss: 1.65117220\n",
      "It: 1290000 Batch: 42000 Epoch 26 Train Loss: 1.69077918 lr: 0.254187 Val Loss: 1.76054871\n",
      "It: 1300000 Batch: 4000 Epoch 27 Train Loss: 1.65493505 lr: 0.254187 Val Loss: 1.95574600\n",
      "It: 1310000 Batch: 14000 Epoch 27 Train Loss: 1.65502328 lr: 0.228768 Val Loss: 1.66401125\n",
      "It: 1320000 Batch: 24000 Epoch 27 Train Loss: 1.62828735 lr: 0.228768 Val Loss: 1.73235142\n",
      "It: 1330000 Batch: 34000 Epoch 27 Train Loss: 1.65999712 lr: 0.228768 Val Loss: 1.57975599\n",
      "It: 1340000 Batch: 44000 Epoch 27 Train Loss: 1.63850877 lr: 0.228768 Val Loss: 1.56132059\n",
      "It: 1350000 Batch: 6000 Epoch 28 Train Loss: 1.67605599 lr: 0.228768 Val Loss: 1.59880749\n",
      "It: 1360000 Batch: 16000 Epoch 28 Train Loss: 1.65163340 lr: 0.228768 Val Loss: 1.58923047\n",
      "It: 1370000 Batch: 26000 Epoch 28 Train Loss: 1.60764266 lr: 0.228768 Val Loss: 1.59777854\n",
      "It: 1380000 Batch: 36000 Epoch 28 Train Loss: 1.60601576 lr: 0.228768 Val Loss: 1.84803148\n",
      "It: 1390000 Batch: 46000 Epoch 28 Train Loss: 1.67699487 lr: 0.228768 Val Loss: 1.70754846\n",
      "It: 1400000 Batch: 8000 Epoch 29 Train Loss: 1.64807403 lr: 0.228768 Val Loss: 1.64076241\n",
      "It: 1410000 Batch: 18000 Epoch 29 Train Loss: 1.59811971 lr: 0.205891 Val Loss: 1.63989797\n",
      "It: 1420000 Batch: 28000 Epoch 29 Train Loss: 1.63003615 lr: 0.205891 Val Loss: 1.81326056\n",
      "It: 1430000 Batch: 38000 Epoch 29 Train Loss: 1.62153979 lr: 0.205891 Val Loss: 1.59962211\n",
      "It: 1440000 Batch: 48000 Epoch 29 Train Loss: 1.61966336 lr: 0.205891 Val Loss: 1.68053413\n",
      "It: 1450000 Batch: 10000 Epoch 30 Train Loss: 1.62636127 lr: 0.205891 Val Loss: 1.73282806\n",
      "It: 1460000 Batch: 20000 Epoch 30 Train Loss: 1.61485199 lr: 0.205891 Val Loss: 1.59184357\n",
      "It: 1470000 Batch: 30000 Epoch 30 Train Loss: 1.62567717 lr: 0.205891 Val Loss: 1.62143532\n",
      "It: 1480000 Batch: 40000 Epoch 30 Train Loss: 1.62322917 lr: 0.205891 Val Loss: 1.61756991\n",
      "It: 1490000 Batch: 2000 Epoch 31 Train Loss: 1.61686776 lr: 0.205891 Val Loss: 1.73630547\n",
      "It: 1500000 Batch: 12000 Epoch 31 Train Loss: 1.59954704 lr: 0.205891 Val Loss: 1.60793737\n",
      "It: 1510000 Batch: 22000 Epoch 31 Train Loss: 1.60722961 lr: 0.185302 Val Loss: 1.67345180\n",
      "It: 1520000 Batch: 32000 Epoch 31 Train Loss: 1.60411760 lr: 0.185302 Val Loss: 1.75352040\n",
      "It: 1530000 Batch: 42000 Epoch 31 Train Loss: 1.61761726 lr: 0.185302 Val Loss: 1.69038329\n",
      "It: 1540000 Batch: 4000 Epoch 32 Train Loss: 1.58908768 lr: 0.185302 Val Loss: 1.58757130\n",
      "It: 1550000 Batch: 14000 Epoch 32 Train Loss: 1.58150912 lr: 0.185302 Val Loss: 1.60691314\n",
      "It: 1560000 Batch: 24000 Epoch 32 Train Loss: 1.62468209 lr: 0.185302 Val Loss: 1.57202274\n",
      "It: 1570000 Batch: 34000 Epoch 32 Train Loss: 1.59129885 lr: 0.185302 Val Loss: 1.60295896\n",
      "It: 1580000 Batch: 44000 Epoch 32 Train Loss: 1.59664555 lr: 0.185302 Val Loss: 1.57923747\n",
      "It: 1590000 Batch: 6000 Epoch 33 Train Loss: 1.60596015 lr: 0.185302 Val Loss: 1.61264279\n",
      "It: 1600000 Batch: 16000 Epoch 33 Train Loss: 1.59271937 lr: 0.185302 Val Loss: 1.75332746\n",
      "It: 1610000 Batch: 26000 Epoch 33 Train Loss: 1.56534305 lr: 0.166772 Val Loss: 1.54113961\n",
      "It: 1620000 Batch: 36000 Epoch 33 Train Loss: 1.57858679 lr: 0.166772 Val Loss: 1.55579357\n",
      "It: 1630000 Batch: 46000 Epoch 33 Train Loss: 1.58429053 lr: 0.166772 Val Loss: 1.54659090\n",
      "It: 1640000 Batch: 8000 Epoch 34 Train Loss: 1.60477780 lr: 0.166772 Val Loss: 1.67158508\n",
      "It: 1650000 Batch: 18000 Epoch 34 Train Loss: 1.59397133 lr: 0.166772 Val Loss: 1.60271879\n",
      "It: 1660000 Batch: 28000 Epoch 34 Train Loss: 1.56361383 lr: 0.166772 Val Loss: 1.57690933\n",
      "It: 1670000 Batch: 38000 Epoch 34 Train Loss: 1.59334188 lr: 0.166772 Val Loss: 1.55192585\n",
      "It: 1680000 Batch: 48000 Epoch 34 Train Loss: 1.59810993 lr: 0.166772 Val Loss: 1.65002882\n",
      "It: 1690000 Batch: 10000 Epoch 35 Train Loss: 1.60043356 lr: 0.166772 Val Loss: 1.64120912\n",
      "It: 1700000 Batch: 20000 Epoch 35 Train Loss: 1.58646964 lr: 0.166772 Val Loss: 1.57511866\n",
      "It: 1710000 Batch: 30000 Epoch 35 Train Loss: 1.56296726 lr: 0.150095 Val Loss: 1.60233940\n",
      "It: 1720000 Batch: 40000 Epoch 35 Train Loss: 1.55284571 lr: 0.150095 Val Loss: 1.48709050\n",
      "It: 1730000 Batch: 2000 Epoch 36 Train Loss: 1.44654966 lr: 0.150095 Val Loss: 1.47377793\n",
      "It: 1740000 Batch: 12000 Epoch 36 Train Loss: 1.43123386 lr: 0.150095 Val Loss: 1.50424835\n",
      "It: 1750000 Batch: 22000 Epoch 36 Train Loss: 1.41591459 lr: 0.150095 Val Loss: 1.40487428\n",
      "It: 1760000 Batch: 32000 Epoch 36 Train Loss: 1.36308986 lr: 0.150095 Val Loss: 1.35206255\n",
      "It: 1770000 Batch: 42000 Epoch 36 Train Loss: 1.34678169 lr: 0.150095 Val Loss: 1.30491933\n",
      "It: 1780000 Batch: 4000 Epoch 37 Train Loss: 1.36392083 lr: 0.150095 Val Loss: 1.47445436\n",
      "It: 1790000 Batch: 14000 Epoch 37 Train Loss: 1.33370916 lr: 0.150095 Val Loss: 1.45489246\n",
      "It: 1800000 Batch: 24000 Epoch 37 Train Loss: 1.33394409 lr: 0.150095 Val Loss: 1.43312760\n",
      "It: 1810000 Batch: 34000 Epoch 37 Train Loss: 1.33708196 lr: 0.135085 Val Loss: 1.35491150\n",
      "It: 1820000 Batch: 44000 Epoch 37 Train Loss: 1.31779148 lr: 0.135085 Val Loss: 1.38065628\n",
      "It: 1830000 Batch: 6000 Epoch 38 Train Loss: 1.32900636 lr: 0.135085 Val Loss: 1.34909059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 1840000 Batch: 16000 Epoch 38 Train Loss: 1.30646080 lr: 0.135085 Val Loss: 1.41753268\n",
      "It: 1850000 Batch: 26000 Epoch 38 Train Loss: 1.31939462 lr: 0.135085 Val Loss: 1.33737915\n",
      "It: 1860000 Batch: 36000 Epoch 38 Train Loss: 1.31166263 lr: 0.135085 Val Loss: 1.44198727\n",
      "It: 1870000 Batch: 46000 Epoch 38 Train Loss: 1.34407145 lr: 0.135085 Val Loss: 1.28926647\n",
      "It: 1880000 Batch: 8000 Epoch 39 Train Loss: 1.29806511 lr: 0.135085 Val Loss: 1.33164038\n",
      "It: 1890000 Batch: 18000 Epoch 39 Train Loss: 1.29390731 lr: 0.135085 Val Loss: 1.33651642\n",
      "It: 1900000 Batch: 28000 Epoch 39 Train Loss: 1.28353097 lr: 0.135085 Val Loss: 1.35520923\n",
      "It: 1910000 Batch: 38000 Epoch 39 Train Loss: 1.30670201 lr: 0.121577 Val Loss: 1.29707106\n",
      "It: 1920000 Batch: 48000 Epoch 39 Train Loss: 1.31249946 lr: 0.121577 Val Loss: 1.42524622\n",
      "It: 1930000 Batch: 10000 Epoch 40 Train Loss: 1.31145236 lr: 0.121577 Val Loss: 1.28355521\n",
      "It: 1940000 Batch: 20000 Epoch 40 Train Loss: 1.29349967 lr: 0.121577 Val Loss: 1.35293057\n",
      "It: 1950000 Batch: 30000 Epoch 40 Train Loss: 1.29458085 lr: 0.121577 Val Loss: 1.34823071\n",
      "It: 1960000 Batch: 40000 Epoch 40 Train Loss: 1.28161948 lr: 0.121577 Val Loss: 1.33179250\n",
      "It: 1970000 Batch: 2000 Epoch 41 Train Loss: 1.27525223 lr: 0.121577 Val Loss: 1.32135273\n",
      "It: 1980000 Batch: 12000 Epoch 41 Train Loss: 1.29783374 lr: 0.121577 Val Loss: 1.35396535\n",
      "It: 1990000 Batch: 22000 Epoch 41 Train Loss: 1.27491918 lr: 0.121577 Val Loss: 1.28964336\n",
      "It: 2000000 Batch: 32000 Epoch 41 Train Loss: 1.31119395 lr: 0.121577 Val Loss: 1.28687953\n",
      "It: 2010000 Batch: 42000 Epoch 41 Train Loss: 1.28291407 lr: 0.109419 Val Loss: 1.35886794\n",
      "It: 2020000 Batch: 4000 Epoch 42 Train Loss: 1.25074453 lr: 0.109419 Val Loss: 1.32203738\n",
      "It: 2030000 Batch: 14000 Epoch 42 Train Loss: 1.29259159 lr: 0.109419 Val Loss: 1.31933374\n",
      "It: 2040000 Batch: 24000 Epoch 42 Train Loss: 1.26975134 lr: 0.109419 Val Loss: 1.37488869\n",
      "It: 2050000 Batch: 34000 Epoch 42 Train Loss: 1.29863577 lr: 0.109419 Val Loss: 1.26064429\n",
      "It: 2060000 Batch: 44000 Epoch 42 Train Loss: 1.28113936 lr: 0.109419 Val Loss: 1.31584213\n",
      "It: 2070000 Batch: 6000 Epoch 43 Train Loss: 1.26468365 lr: 0.109419 Val Loss: 1.28142591\n",
      "It: 2080000 Batch: 16000 Epoch 43 Train Loss: 1.25288029 lr: 0.109419 Val Loss: 1.18334569\n",
      "It: 2090000 Batch: 26000 Epoch 43 Train Loss: 1.14206241 lr: 0.109419 Val Loss: 1.23298247\n",
      "It: 2100000 Batch: 36000 Epoch 43 Train Loss: 1.13292469 lr: 0.109419 Val Loss: 1.13957923\n",
      "It: 2110000 Batch: 46000 Epoch 43 Train Loss: 1.10497445 lr: 0.098477 Val Loss: 1.20011609\n",
      "It: 2120000 Batch: 8000 Epoch 44 Train Loss: 1.11325544 lr: 0.098477 Val Loss: 1.19121738\n",
      "It: 2130000 Batch: 18000 Epoch 44 Train Loss: 1.08893363 lr: 0.098477 Val Loss: 1.18032194\n",
      "It: 2140000 Batch: 28000 Epoch 44 Train Loss: 1.12744117 lr: 0.098477 Val Loss: 1.14759467\n",
      "It: 2150000 Batch: 38000 Epoch 44 Train Loss: 1.11157552 lr: 0.098477 Val Loss: 1.23194263\n",
      "It: 2160000 Batch: 48000 Epoch 44 Train Loss: 1.12567534 lr: 0.098477 Val Loss: 1.11852246\n",
      "It: 2170000 Batch: 10000 Epoch 45 Train Loss: 1.13204194 lr: 0.098477 Val Loss: 1.10587512\n",
      "It: 2180000 Batch: 20000 Epoch 45 Train Loss: 1.06142497 lr: 0.098477 Val Loss: 1.14318509\n",
      "It: 2190000 Batch: 30000 Epoch 45 Train Loss: 1.07088221 lr: 0.098477 Val Loss: 1.11106042\n",
      "It: 2200000 Batch: 40000 Epoch 45 Train Loss: 1.08535837 lr: 0.098477 Val Loss: 1.18558288\n",
      "It: 2210000 Batch: 2000 Epoch 46 Train Loss: 1.09713316 lr: 0.088629 Val Loss: 1.12164919\n",
      "It: 2220000 Batch: 12000 Epoch 46 Train Loss: 1.09987566 lr: 0.088629 Val Loss: 1.07951084\n",
      "It: 2230000 Batch: 22000 Epoch 46 Train Loss: 1.08048488 lr: 0.088629 Val Loss: 1.15723022\n",
      "It: 2240000 Batch: 32000 Epoch 46 Train Loss: 1.08760930 lr: 0.088629 Val Loss: 1.11781231\n",
      "It: 2250000 Batch: 42000 Epoch 46 Train Loss: 1.07397651 lr: 0.088629 Val Loss: 1.09371124\n",
      "It: 2260000 Batch: 4000 Epoch 47 Train Loss: 1.11386577 lr: 0.088629 Val Loss: 1.12970972\n",
      "It: 2270000 Batch: 14000 Epoch 47 Train Loss: 1.06705417 lr: 0.088629 Val Loss: 1.17088381\n",
      "It: 2280000 Batch: 24000 Epoch 47 Train Loss: 1.05947777 lr: 0.088629 Val Loss: 1.07385257\n",
      "It: 2290000 Batch: 34000 Epoch 47 Train Loss: 1.05955647 lr: 0.088629 Val Loss: 1.12474719\n",
      "It: 2300000 Batch: 44000 Epoch 47 Train Loss: 1.07471098 lr: 0.088629 Val Loss: 1.13337742\n",
      "It: 2310000 Batch: 6000 Epoch 48 Train Loss: 1.07850173 lr: 0.079766 Val Loss: 1.09147104\n",
      "It: 2320000 Batch: 16000 Epoch 48 Train Loss: 1.04344857 lr: 0.079766 Val Loss: 1.09461614\n",
      "It: 2330000 Batch: 26000 Epoch 48 Train Loss: 1.05461719 lr: 0.079766 Val Loss: 1.11966018\n",
      "It: 2340000 Batch: 36000 Epoch 48 Train Loss: 1.06021677 lr: 0.079766 Val Loss: 1.06999024\n",
      "It: 2350000 Batch: 46000 Epoch 48 Train Loss: 1.05963168 lr: 0.079766 Val Loss: 1.08407982\n",
      "It: 2360000 Batch: 8000 Epoch 49 Train Loss: 1.02597808 lr: 0.079766 Val Loss: 1.07437908\n",
      "It: 2370000 Batch: 18000 Epoch 49 Train Loss: 1.01097363 lr: 0.079766 Val Loss: 1.05850759\n",
      "It: 2380000 Batch: 28000 Epoch 49 Train Loss: 1.04074572 lr: 0.079766 Val Loss: 1.04706731\n",
      "It: 2390000 Batch: 38000 Epoch 49 Train Loss: 1.00210951 lr: 0.079766 Val Loss: 1.07665222\n",
      "It: 2400000 Batch: 48000 Epoch 49 Train Loss: 1.00939956 lr: 0.079766 Val Loss: 1.03751354\n",
      "It: 2410000 Batch: 10000 Epoch 50 Train Loss: 0.95327170 lr: 0.071790 Val Loss: 1.05060038\n",
      "It: 2420000 Batch: 20000 Epoch 50 Train Loss: 0.97168024 lr: 0.071790 Val Loss: 1.03661101\n",
      "It: 2430000 Batch: 30000 Epoch 50 Train Loss: 0.98364812 lr: 0.071790 Val Loss: 1.08150617\n",
      "It: 2440000 Batch: 40000 Epoch 50 Train Loss: 0.96690455 lr: 0.071790 Val Loss: 1.02810113\n",
      "It: 2450000 Batch: 2000 Epoch 51 Train Loss: 0.99280673 lr: 0.071790 Val Loss: 1.03055085\n",
      "It: 2460000 Batch: 12000 Epoch 51 Train Loss: 0.97202525 lr: 0.071790 Val Loss: 0.99895429\n",
      "It: 2470000 Batch: 22000 Epoch 51 Train Loss: 0.96966289 lr: 0.071790 Val Loss: 1.04002755\n",
      "It: 2480000 Batch: 32000 Epoch 51 Train Loss: 0.99642718 lr: 0.071790 Val Loss: 1.05742200\n",
      "It: 2490000 Batch: 42000 Epoch 51 Train Loss: 0.96556205 lr: 0.071790 Val Loss: 1.00092979\n",
      "It: 2500000 Batch: 4000 Epoch 52 Train Loss: 0.96638743 lr: 0.071790 Val Loss: 0.98519642\n",
      "It: 2510000 Batch: 14000 Epoch 52 Train Loss: 0.93358726 lr: 0.064611 Val Loss: 0.98963539\n",
      "It: 2520000 Batch: 24000 Epoch 52 Train Loss: 0.98439698 lr: 0.064611 Val Loss: 0.98920266\n",
      "It: 2530000 Batch: 34000 Epoch 52 Train Loss: 0.96948618 lr: 0.064611 Val Loss: 0.99858618\n",
      "It: 2540000 Batch: 44000 Epoch 52 Train Loss: 0.96559377 lr: 0.064611 Val Loss: 0.99813747\n",
      "It: 2550000 Batch: 6000 Epoch 53 Train Loss: 0.91848833 lr: 0.064611 Val Loss: 0.96860716\n",
      "It: 2560000 Batch: 16000 Epoch 53 Train Loss: 0.94916177 lr: 0.064611 Val Loss: 1.02476100\n",
      "It: 2570000 Batch: 26000 Epoch 53 Train Loss: 0.95758545 lr: 0.064611 Val Loss: 0.98736165\n",
      "It: 2580000 Batch: 36000 Epoch 53 Train Loss: 0.93593626 lr: 0.064611 Val Loss: 1.01799045\n",
      "It: 2590000 Batch: 46000 Epoch 53 Train Loss: 0.95798673 lr: 0.064611 Val Loss: 0.98287876\n",
      "It: 2600000 Batch: 8000 Epoch 54 Train Loss: 0.91380706 lr: 0.064611 Val Loss: 0.96278223\n",
      "It: 2610000 Batch: 18000 Epoch 54 Train Loss: 0.94231369 lr: 0.058150 Val Loss: 0.99288197\n",
      "It: 2620000 Batch: 28000 Epoch 54 Train Loss: 0.92651758 lr: 0.058150 Val Loss: 0.95873106\n",
      "It: 2630000 Batch: 38000 Epoch 54 Train Loss: 0.90927152 lr: 0.058150 Val Loss: 0.97750632\n",
      "It: 2640000 Batch: 48000 Epoch 54 Train Loss: 0.93603471 lr: 0.058150 Val Loss: 1.00365656\n",
      "It: 2650000 Batch: 10000 Epoch 55 Train Loss: 0.91387336 lr: 0.058150 Val Loss: 1.05025225\n",
      "It: 2660000 Batch: 20000 Epoch 55 Train Loss: 0.91152212 lr: 0.058150 Val Loss: 0.97333585\n",
      "It: 2670000 Batch: 30000 Epoch 55 Train Loss: 0.90422805 lr: 0.058150 Val Loss: 0.98475619\n",
      "It: 2680000 Batch: 40000 Epoch 55 Train Loss: 0.95112141 lr: 0.058150 Val Loss: 0.99617078\n",
      "It: 2690000 Batch: 2000 Epoch 56 Train Loss: 0.92547543 lr: 0.058150 Val Loss: 0.95801557\n",
      "It: 2700000 Batch: 12000 Epoch 56 Train Loss: 0.89696540 lr: 0.058150 Val Loss: 0.93123491\n",
      "It: 2710000 Batch: 22000 Epoch 56 Train Loss: 0.87348918 lr: 0.052335 Val Loss: 0.97489592\n",
      "It: 2720000 Batch: 32000 Epoch 56 Train Loss: 0.92324490 lr: 0.052335 Val Loss: 0.97053872\n",
      "It: 2730000 Batch: 42000 Epoch 56 Train Loss: 0.92555156 lr: 0.052335 Val Loss: 0.95320778\n",
      "It: 2740000 Batch: 4000 Epoch 57 Train Loss: 0.92148090 lr: 0.052335 Val Loss: 0.95747751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 2750000 Batch: 14000 Epoch 57 Train Loss: 0.92092622 lr: 0.052335 Val Loss: 0.96263935\n",
      "It: 2760000 Batch: 24000 Epoch 57 Train Loss: 0.89973862 lr: 0.052335 Val Loss: 0.95167296\n",
      "It: 2770000 Batch: 34000 Epoch 57 Train Loss: 0.90876601 lr: 0.052335 Val Loss: 0.92964534\n",
      "It: 2780000 Batch: 44000 Epoch 57 Train Loss: 0.89796069 lr: 0.052335 Val Loss: 0.94844042\n",
      "It: 2790000 Batch: 6000 Epoch 58 Train Loss: 0.89236014 lr: 0.052335 Val Loss: 0.95875794\n",
      "It: 2800000 Batch: 16000 Epoch 58 Train Loss: 0.91237532 lr: 0.052335 Val Loss: 0.93242812\n",
      "It: 2810000 Batch: 26000 Epoch 58 Train Loss: 0.87530856 lr: 0.047101 Val Loss: 0.95356145\n",
      "It: 2820000 Batch: 36000 Epoch 58 Train Loss: 0.90652631 lr: 0.047101 Val Loss: 0.95687287\n",
      "It: 2830000 Batch: 46000 Epoch 58 Train Loss: 0.88769975 lr: 0.047101 Val Loss: 0.95548759\n",
      "It: 2840000 Batch: 8000 Epoch 59 Train Loss: 0.88304227 lr: 0.047101 Val Loss: 0.96618418\n",
      "It: 2850000 Batch: 18000 Epoch 59 Train Loss: 0.91145984 lr: 0.047101 Val Loss: 0.95389653\n",
      "It: 2860000 Batch: 28000 Epoch 59 Train Loss: 0.88256629 lr: 0.047101 Val Loss: 0.98811559\n",
      "It: 2870000 Batch: 38000 Epoch 59 Train Loss: 0.88923995 lr: 0.047101 Val Loss: 0.94167736\n",
      "It: 2880000 Batch: 48000 Epoch 59 Train Loss: 0.92051354 lr: 0.047101 Val Loss: 0.96009375\n",
      "It: 2890000 Batch: 10000 Epoch 60 Train Loss: 0.90931501 lr: 0.047101 Val Loss: 0.97268757\n",
      "It: 2900000 Batch: 20000 Epoch 60 Train Loss: 0.89511232 lr: 0.047101 Val Loss: 0.94038192\n",
      "It: 2910000 Batch: 30000 Epoch 60 Train Loss: 0.86734287 lr: 0.042391 Val Loss: 0.96213405\n",
      "It: 2920000 Batch: 40000 Epoch 60 Train Loss: 0.90574015 lr: 0.042391 Val Loss: 0.93897153\n",
      "It: 2930000 Batch: 2000 Epoch 61 Train Loss: 0.88624541 lr: 0.042391 Val Loss: 0.98001433\n",
      "It: 2940000 Batch: 12000 Epoch 61 Train Loss: 0.87870578 lr: 0.042391 Val Loss: 0.96143144\n",
      "It: 2950000 Batch: 22000 Epoch 61 Train Loss: 0.88110417 lr: 0.042391 Val Loss: 0.95909468\n",
      "It: 2960000 Batch: 32000 Epoch 61 Train Loss: 0.88423361 lr: 0.042391 Val Loss: 0.97405025\n",
      "It: 2970000 Batch: 42000 Epoch 61 Train Loss: 0.87759866 lr: 0.042391 Val Loss: 0.96202331\n",
      "It: 2980000 Batch: 4000 Epoch 62 Train Loss: 0.90230873 lr: 0.042391 Val Loss: 0.93077658\n",
      "It: 2990000 Batch: 14000 Epoch 62 Train Loss: 0.88337218 lr: 0.042391 Val Loss: 0.98181682\n",
      "It: 3000000 Batch: 24000 Epoch 62 Train Loss: 0.89677391 lr: 0.042391 Val Loss: 0.93913287\n",
      "It: 3010000 Batch: 34000 Epoch 62 Train Loss: 0.87220176 lr: 0.038152 Val Loss: 0.94703837\n",
      "It: 3020000 Batch: 44000 Epoch 62 Train Loss: 0.85733888 lr: 0.038152 Val Loss: 0.94632013\n",
      "It: 3030000 Batch: 6000 Epoch 63 Train Loss: 0.89407511 lr: 0.038152 Val Loss: 0.98925162\n",
      "It: 3040000 Batch: 16000 Epoch 63 Train Loss: 0.91294138 lr: 0.038152 Val Loss: 0.95562312\n",
      "It: 3050000 Batch: 26000 Epoch 63 Train Loss: 0.91685841 lr: 0.038152 Val Loss: 0.95768520\n",
      "It: 3060000 Batch: 36000 Epoch 63 Train Loss: 0.91285298 lr: 0.038152 Val Loss: 0.97085165\n",
      "It: 3070000 Batch: 46000 Epoch 63 Train Loss: 0.88576996 lr: 0.038152 Val Loss: 0.95805677\n",
      "It: 3080000 Batch: 8000 Epoch 64 Train Loss: 0.86318586 lr: 0.038152 Val Loss: 1.00986500\n",
      "It: 3090000 Batch: 18000 Epoch 64 Train Loss: 0.89991945 lr: 0.038152 Val Loss: 0.95529652\n",
      "It: 3100000 Batch: 28000 Epoch 64 Train Loss: 0.90418378 lr: 0.038152 Val Loss: 0.96328079\n",
      "It: 3110000 Batch: 38000 Epoch 64 Train Loss: 0.87945151 lr: 0.034337 Val Loss: 0.96978279\n",
      "It: 3120000 Batch: 48000 Epoch 64 Train Loss: 0.93430451 lr: 0.034337 Val Loss: 0.96905956\n",
      "It: 3130000 Batch: 10000 Epoch 65 Train Loss: 0.89834555 lr: 0.034337 Val Loss: 0.98292782\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bf28707ebb8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m           \u001b[0mdecay_iteractions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m           \u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m           print_interval=print_interval)\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mY_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/local/LogisticRegression-ANN/NN/network.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, lr, max_iter, lr_optimizer, epsilon, power_t, t, print_interval, decay_iteractions, decay_rate, X_val, Y_val)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;31m# print(X_.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/local/LogisticRegression-ANN/NN/network.py\u001b[0m in \u001b[0;36mfeed_forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0minput_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0minput_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/local/LogisticRegression-ANN/NN/network.py\u001b[0m in \u001b[0;36mfeed_forward\u001b[0;34m(self, input_values)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mdprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'net'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/local/LogisticRegression-ANN/NN/activation_functions.py\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(h)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0msig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import timeit\n",
    "from utils import dataset_helper\n",
    "from utils import custom_scores\n",
    "reload(custom_scores)\n",
    "reload(dataset_helper)\n",
    "reload(loss_functions)\n",
    "reload(activation_functions)\n",
    "reload(network)\n",
    "reload(dataset_helper)\n",
    "\n",
    "nclasses = 5\n",
    "nsamples = 60000\n",
    "nfeatures = 60\n",
    "eps = np.finfo(np.float32).eps\n",
    "X,  X_val, Y, Y_val = dataset_helper.get_toy_data_multiclass(nclasses, nsamples, nfeatures)\n",
    "\n",
    "Y = dataset_helper.one_hot_encode(Y, nclasses)\n",
    "Y_val = dataset_helper.one_hot_encode(Y_val, nclasses)\n",
    "\n",
    "eps = 0.9\n",
    "lr = 1.\n",
    "max_iter = 50 * X.shape[0]\n",
    "print_interval = 10 * 1000\n",
    "\n",
    "network.DEBUG = False\n",
    "\n",
    "eps = np.finfo(np.float32).eps\n",
    "\n",
    "h1 = network.Layer(nfeatures, 32, 'sigmoid', label=\"H1\")\n",
    "h2 = network.Layer(32, 32, 'sigmoid', label=\"H2\")\n",
    "h3 = network.Layer(32, 24, 'sigmoid', label=\"H3\")\n",
    "o = network.Layer(24, nclasses, 'sigmoid', label=\"Output\")\n",
    "\n",
    "model = network.NN(loss='cross_entropy')\n",
    "model.add_layer(h1)\n",
    "model.add_layer(h2)\n",
    "model.add_layer(h3)\n",
    "model.add_layer(o)\n",
    "model.show_weights()\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "start = time.process_time()\n",
    "model.fit(X, Y, max_iter=max_iter, \n",
    "          lr=lr, epsilon=eps,\n",
    "          decay_iteractions=100 * 1000, decay_rate = 0.9,\n",
    "          X_val=X_val, Y_val=Y_val,\n",
    "          print_interval=print_interval)\n",
    "\n",
    "Y_ = np.array(model.predict(X))\n",
    "Y_ = Y_.argmax(axis=-1).flatten()\n",
    "Y = Y.argmax(axis=-1)\n",
    "mae = np.absolute(Y - Y_).mean()\n",
    "\n",
    "print(\"Time Spent \", time.process_time() - start)\n",
    "\n",
    "\n",
    "Y_val_ = np.array(model.predict(X_val))\n",
    "iteraction_log = network.get_iteration_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import timeit\n",
    "from utils import dataset_helper\n",
    "from utils import custom_scores\n",
    "custom_scores.evalute_multiclass(Y_val.argmax(axis=-1), Y_val_.argmax(axis=-1))\n",
    "iteraction_log.index = iteraction_log.it\n",
    "iteraction_log.error_train.plot()\n",
    "iteraction_log.error_val.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
