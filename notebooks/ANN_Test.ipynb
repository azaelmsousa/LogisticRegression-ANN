{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from NN import network, activation_functions, loss_functions\n",
    "from NN import ANN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Run Checks\n",
    "To identify if there is no code broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3775 0.3925]]\n",
      "[[0.59326999 0.59688438]]\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(network)\n",
    "I = np.array([[0.05, 0.10]])\n",
    "W = np.array([[0.15, 0.2], [0.25, 0.3]]).T\n",
    "h1 = network.Layer(2, 2, 'sigmoid', weights=W, bias=0.35, label=\"H1\")\n",
    "netH, outH =  h1.feed_forward(I)\n",
    "print(netH)\n",
    "print(outH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.10590597 1.2249214 ]]\n",
      "[[0.75136507 0.77292847]]\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(network)\n",
    "I = np.array([[0.59326999, 0.59688438]])\n",
    "W = np.array([[0.4, 0.45], [0.50, 0.55]]).T\n",
    "o1 = network.Layer(2, 2, 'sigmoid', weights=W, bias=0.6, label=\"H1\")\n",
    "netO, outO =  o1.feed_forward(I)\n",
    "print(netO)\n",
    "print(outO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergence Checkings\n",
    "Based on the sample given by the class teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Weights\n",
      "-------------------------------\n",
      "H1      (input=2, neurons=2, activation=sigmoid)\n",
      "[[0.15 0.25]\n",
      " [0.2  0.3 ]]\n",
      "H1      (input=2, neurons=2, activation=sigmoid)\n",
      "[[0.4  0.5 ]\n",
      " [0.45 0.55]]\n",
      "-------------------------------\n",
      "Y\n",
      " [1.10590597, 1.2249214]\n",
      "\n",
      "a(Y)\n",
      " [0.75136507, 0.77292847]\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(network)\n",
    "model = network.NN(loss='smd')\n",
    "X = np.array([[0.05, 0.10]])\n",
    "\n",
    "Wh = np.array([[0.15, 0.2], [0.25, 0.3]]).T\n",
    "h = network.Layer(2, 2, 'sigmoid', weights=Wh, bias=0.35, label=\"H1\")\n",
    "\n",
    "Wo = np.array([[0.4, 0.45], [0.50, 0.55]]).T\n",
    "o = network.Layer(2, 2, 'sigmoid', weights=Wo, bias=0.6, label=\"H1\")\n",
    "model.add_layer(h)\n",
    "model.add_layer(o)\n",
    "model.show_weights()\n",
    "\n",
    "Y, aY = model.feed_forward(X)\n",
    "Y_, aY_ = ([1.10590597, 1.2249214 ], [0.75136507, 0.77292847])\n",
    "print(\"Y\\n\",Y_)\n",
    "print(\"\\na(Y)\\n\",aY_)\n",
    "\n",
    "assert((Y - Y_).sum() < np.finfo(np.float32).eps)\n",
    "assert((aY - aY_).sum() < np.finfo(np.float32).eps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on the back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Weights\n",
      "-------------------------------\n",
      "H1      (input=2, neurons=2, activation=sigmoid)\n",
      "[[0.15 0.25]\n",
      " [0.2  0.3 ]]\n",
      "H1      (input=2, neurons=2, activation=sigmoid)\n",
      "[[0.4  0.5 ]\n",
      " [0.45 0.55]]\n",
      "-------------------------------\n",
      "0.2983711087600027\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(network)\n",
    "model.show_weights()\n",
    "Y = np.array([0.01, 0.99])\n",
    "Etotal = loss_functions.smd(aY, Y)\n",
    "print(Etotal)\n",
    "assert((Etotal - 0.2983711) < np.finfo(np.float32).eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emulating the weights update for the layer O "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.74136507 -0.21707153]]\n",
      "dout [[0.1868156  0.17551005]]\n",
      "dnet [[0.59326999 0.59688438]]\n",
      "delta [[ 0.13849856 -0.03809824]]\n",
      "dw [[ 0.08216704 -0.02274024]]\n",
      "update [[0.35891648 0.51137012]\n",
      " [0.40891648 0.56137012]]\n"
     ]
    }
   ],
   "source": [
    "reload(loss_functions)\n",
    "reload(activation_functions)\n",
    "lr = 0.5\n",
    "#Done - Partial\n",
    "dEo_dw = loss_functions.smd_derivative_chain(outO, Y)\n",
    "print(dEo_dw)\n",
    "\n",
    "# Done\n",
    "dOuto_Dneto = activation_functions.sigmoid_derivative_chain(outO)\n",
    "print('dout',dOuto_Dneto)\n",
    "\n",
    "# Done - Self.input\n",
    "dNeto  = outH\n",
    "print('dnet',dNeto) \n",
    "# Done\n",
    "deltaO  = dEo_dw * dOuto_Dneto\n",
    "print('delta', deltaO)\n",
    "\n",
    "dWO = deltaO * outH\n",
    "print('dw', dWO)\n",
    "# Done\n",
    "updateO = Wo - lr * dWO\n",
    "print('update', updateO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.05539942 -0.01904912]\n",
      " [ 0.06232435 -0.02095403]]\n",
      "dETotal_dOh [0.03635031 0.04137032]\n",
      "dOuth_Dneth [[0.24130071 0.24061342]]\n",
      "deltaH [[0.00877135 0.00995425]]\n",
      "dWh [[0.00043857 0.00099543]]\n",
      "update [[0.14978072 0.24950229]\n",
      " [0.19978072 0.29950229]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Done - Partial\n",
    "dEo_dOh = deltaO * Wo\n",
    "dETotal_dOh = dEo_dOh.sum(axis=1)\n",
    "print (dEo_dOh)\n",
    "print('dETotal_dOh', dETotal_dOh)\n",
    "\n",
    "# Done\n",
    "dOuth_Dneth = activation_functions.sigmoid_derivative_chain(outH)\n",
    "print('dOuth_Dneth', dOuth_Dneth)\n",
    "\n",
    "\n",
    "# Done\n",
    "deltaH = dETotal_dOh * dOuth_Dneth \n",
    "print('deltaH', deltaH)\n",
    "\n",
    "# done\n",
    "# self.input\n",
    "dNeth_dw = X\n",
    "dWh = deltaH * dNeth_dw\n",
    "print('dWh', dWh)\n",
    "\n",
    "# Done\n",
    "updateH = Wh - lr * dWh\n",
    "print('update', updateH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the implemented Grad calculation for Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 error 0.96071132939975\n",
      "1000 error 0.0001601216083159306\n",
      "2000 error 0.00017951623928426981\n",
      "3000 error 0.0001862230989718681\n",
      "4000 error 0.0001896236984334605\n",
      "5000 error 0.00019167917494260456\n",
      "6000 error 0.0001930557158084457\n",
      "7000 error 0.00019404194961147484\n",
      "8000 error 0.00019478322200835535\n",
      "9000 error 0.00019536068831676047\n",
      "[[1.05748042e-04 9.99895786e-01]\n",
      " [1.05748042e-04 9.99895786e-01]]\n"
     ]
    }
   ],
   "source": [
    "reload(loss_functions)\n",
    "reload(activation_functions)\n",
    "reload(network)\n",
    "network.DEBUG = False\n",
    "\n",
    "X = np.array([[0.05, 0.10], [0.05, 0.10]], np.float64)\n",
    "\n",
    "# Wh = np.array([[0.15, 0.2], [0.25, 0.3]], np.float64).T\n",
    "h = network.Layer(2, 10, 'sigmoid',  bias=0.35, label=\"H1\")\n",
    "\n",
    "# Wo = np.array([[0.4, 0.45], [0.50, 0.55]], np.float64).T\n",
    "o = network.Layer(10, 2, 'sigmoid', bias=0.6, label=\"O\")\n",
    "Y = np.array([[0.01, 0.99], [0.01, 0.99]], np.float64)\n",
    "\n",
    "for i in range(10000):\n",
    "    netH, outH = h.feed_forward(X)\n",
    "    netO, outO = o.feed_forward(outH)\n",
    "\n",
    "    \n",
    "    Etotal = loss_functions.smd(outO, Y)\n",
    "    if (i % 1000) == 0:\n",
    "        print(i, 'error', Etotal)\n",
    "    \n",
    "    dEo_dw = loss_functions.cross_entropy_derivative_chain(outO, Y)\n",
    "    \n",
    "    network.dprint(dEo_dw)\n",
    "    network.dprint (\"\")\n",
    "    network.dprint (\"==========================================\")\n",
    "    network.dprint (\"Back Propagate Layer O\")\n",
    "    network.dprint (\"==========================================\")\n",
    "    o.backpropagate(dETotal_dOut=dEo_dw)\n",
    "    network.dprint (\"==========================================\")\n",
    "    network.dprint (\"\")\n",
    "\n",
    "    network.dprint (\"==========================================\")\n",
    "    network.dprint (\"Back Propagate Layer H\")\n",
    "    network.dprint (\"==========================================\")\n",
    "    h.backpropagate(output_layer=o)\n",
    "    network.dprint (\"==========================================\")\n",
    "\n",
    "print (outO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Fit Methods with the full network calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMD as the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Weights\n",
      "-------------------------------\n",
      "H1      (input=2, neurons=3, activation=sigmoid)\n",
      "[[0.93882413 0.19500006 0.16673673]\n",
      " [0.29650804 0.38296736 0.74113667]]\n",
      "Output  (input=3, neurons=2, activation=sigmoid)\n",
      "[[0.66385273 0.71361663]\n",
      " [0.82387165 0.9979182 ]\n",
      " [0.48388118 0.42768328]]\n",
      "-------------------------------\n",
      "Shuffled\n",
      "It: 2000 Batch: 1 Epoch 1999 Error: 0.00553536 lr: 0.500000 \n",
      "It: 4000 Batch: 1 Epoch 3999 Error: 0.00014716 lr: 0.500000 \n",
      "It: 6000 Batch: 1 Epoch 5999 Error: 0.00006185 lr: 0.500000 \n",
      "It: 8000 Batch: 1 Epoch 7999 Error: 0.00003328 lr: 0.500000 \n",
      "It: 10000 Batch: 1 Epoch 9999 Error: 0.00002011 lr: 0.500000 \n",
      "It: 12000 Batch: 1 Epoch 11999 Error: 0.00001300 lr: 0.500000 \n",
      "It: 14000 Batch: 1 Epoch 13999 Error: 0.00000879 lr: 0.500000 \n",
      "Finished \n",
      " It: 14001 Batch: 1 Epoch 14000 Train Loss: 0.00000879 lr: 0.500000 \n",
      "[[0.01270133 0.98730822]] 0.0026965590326849956\n"
     ]
    }
   ],
   "source": [
    "from utils import dataset_helper\n",
    "reload(loss_functions)\n",
    "reload(activation_functions)\n",
    "reload(network)\n",
    "reload(dataset_helper)\n",
    "network.DEBUG = False\n",
    "eps = 0.00001\n",
    "\n",
    "X = np.array([[0.05, 0.10]], np.float64)\n",
    "Y = np.array([[0.01, 0.99]], np.float64)\n",
    "\n",
    "# Wh = np.array([[0.15, 0.2], [0.25, 0.3]], np.float64).T\n",
    "h = network.Layer(2, 3, 'sigmoid', bias=0.35, label=\"H1\")\n",
    "\n",
    "# Wo = np.array([[0.4, 0.45], [0.50, 0.55]], np.float64).T\n",
    "o = network.Layer(3, 2, 'sigmoid', bias=0.6, label=\"Output\")\n",
    "\n",
    "model = network.NN(loss='smd')\n",
    "model.add_layer(h)\n",
    "model.add_layer(o)\n",
    "model.show_weights()\n",
    "\n",
    "model.fit(X, Y, max_iter=50000, \n",
    "          lr=0.5, epsilon=eps, b_sz=2, \n",
    "          print_interval=2000)\n",
    "\n",
    "_, Y_ = model.feed_forward(X)\n",
    "\n",
    "mae = np.absolute(Y - Y_).mean()\n",
    "print(Y_, mae)\n",
    "assert(mae < 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Weights\n",
      "-------------------------------\n",
      "H1      (input=2, neurons=2, activation=sigmoid)\n",
      "[[0.15 0.25]\n",
      " [0.2  0.3 ]]\n",
      "Output  (input=2, neurons=2, activation=sigmoid)\n",
      "[[0.4  0.5 ]\n",
      " [0.45 0.55]]\n",
      "-------------------------------\n",
      "Shuffled\n",
      "It: 2000 Batch: 1 Epoch 1999 Error: 0.76574643 lr: 0.100000 \n",
      "It: 4000 Batch: 1 Epoch 3999 Error: 0.66248688 lr: 0.100000 \n",
      "It: 6000 Batch: 1 Epoch 5999 Error: 0.65524647 lr: 0.100000 \n",
      "It: 8000 Batch: 1 Epoch 7999 Error: 0.65246400 lr: 0.100000 \n",
      "It: 10000 Batch: 1 Epoch 9999 Error: 0.65099797 lr: 0.100000 \n",
      "Finished \n",
      " It: 10000 Batch: 1 Epoch 9999 Train Loss: 0.65099797 lr: 0.100000 \n",
      "[[0.0038555  0.99623122]] 0.006187855935083348\n"
     ]
    }
   ],
   "source": [
    "from utils import dataset_helper\n",
    "reload(loss_functions)\n",
    "reload(activation_functions)\n",
    "reload(network)\n",
    "reload(dataset_helper)\n",
    "network.DEBUG = False\n",
    "eps = 0.0001\n",
    "X = np.array([[0.05, 0.10]], np.float64)\n",
    "Y = np.array([[0.01, 0.99]], np.float64)\n",
    "\n",
    "Wh = np.array([[0.15, 0.2], [0.25, 0.3]], np.float64).T\n",
    "h = network.Layer(2, 2, 'sigmoid', weights=Wh, bias=0.35, label=\"H1\")\n",
    "\n",
    "Wo = np.array([[0.4, 0.45], [0.50, 0.55]], np.float64).T\n",
    "o = network.Layer(2, 2, 'sigmoid', weights=Wo, bias=0.6, label=\"Output\")\n",
    "\n",
    "model = network.NN(loss='cross_entropy')\n",
    "model.add_layer(h)\n",
    "model.add_layer(o)\n",
    "model.show_weights()\n",
    "\n",
    "model.fit(X, Y, max_iter=10000,\n",
    "          lr=0.1, epsilon=eps, \n",
    "          print_interval=2000)\n",
    "\n",
    "_, Y_ = model.feed_forward(X)\n",
    "\n",
    "mae = np.absolute(Y - Y_).mean()\n",
    "print(Y_, mae)\n",
    "assert(mae < 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Weights\n",
      "-------------------------------\n",
      "H1      (input=2, neurons=10, activation=sigmoid)\n",
      "[[0.7278321  0.52009655 0.39918015 0.23696724 0.7736045  0.94884612\n",
      "  0.42310302 0.57990131 0.84848208 0.94855152]\n",
      " [0.34801577 0.05642662 0.73882445 0.00230771 0.74707891 0.02962981\n",
      "  0.72329036 0.57095065 0.97767136 0.42079226]]\n",
      "Output  (input=10, neurons=2, activation=sigmoid)\n",
      "[[0.95997424 0.86727522]\n",
      " [0.26026361 0.87896529]\n",
      " [0.395428   0.01318163]\n",
      " [0.39379508 0.36371827]\n",
      " [0.08316574 0.19400122]\n",
      " [0.14407298 0.5694082 ]\n",
      " [0.0966904  0.10178734]\n",
      " [0.82290464 0.91400647]\n",
      " [0.20374662 0.60130484]\n",
      " [0.43791923 0.91621748]]\n",
      "-------------------------------\n",
      "Shuffled\n",
      "It: 2000 Batch: 2 Epoch 666 Error: 1.28613137 lr: 0.900000 \n",
      "It: 4000 Batch: 1 Epoch 1333 Error: 1.28161702 lr: 0.900000 \n",
      "It: 6000 Batch: 3 Epoch 1999 Error: 1.26640426 lr: 0.900000 \n",
      "It: 8000 Batch: 2 Epoch 2666 Error: 1.03661381 lr: 0.900000 \n",
      "It: 10000 Batch: 1 Epoch 3333 Error: 0.70017559 lr: 0.900000 \n",
      "It: 12000 Batch: 3 Epoch 3999 Error: 0.65072086 lr: 0.900000 \n",
      "It: 14000 Batch: 2 Epoch 4666 Error: 0.63981592 lr: 0.900000 \n",
      "It: 16000 Batch: 1 Epoch 5333 Error: 0.63539855 lr: 0.900000 \n",
      "It: 18000 Batch: 3 Epoch 5999 Error: 0.63306952 lr: 0.900000 \n",
      "It: 20000 Batch: 2 Epoch 6666 Error: 0.63165446 lr: 0.900000 \n",
      "It: 22000 Batch: 1 Epoch 7333 Error: 0.63071137 lr: 0.900000 \n",
      "It: 24000 Batch: 3 Epoch 7999 Error: 0.63004451 lr: 0.900000 \n",
      "It: 26000 Batch: 2 Epoch 8666 Error: 0.62955068 lr: 0.900000 \n",
      "It: 28000 Batch: 1 Epoch 9333 Error: 0.62916987 lr: 0.900000 \n",
      "It: 30000 Batch: 3 Epoch 9999 Error: 0.62886883 lr: 0.900000 \n",
      "It: 32000 Batch: 2 Epoch 10666 Error: 0.62862464 lr: 0.900000 \n",
      "It: 34000 Batch: 1 Epoch 11333 Error: 0.62842497 lr: 0.900000 \n",
      "It: 36000 Batch: 3 Epoch 11999 Error: 0.62825652 lr: 0.900000 \n",
      "It: 38000 Batch: 2 Epoch 12666 Error: 0.62811551 lr: 0.900000 \n",
      "It: 40000 Batch: 1 Epoch 13333 Error: 0.62799224 lr: 0.900000 \n",
      "It: 42000 Batch: 3 Epoch 13999 Error: 0.62788706 lr: 0.900000 \n",
      "It: 44000 Batch: 2 Epoch 14666 Error: 0.62779460 lr: 0.900000 \n",
      "It: 46000 Batch: 1 Epoch 15333 Error: 0.62771270 lr: 0.900000 \n",
      "It: 48000 Batch: 3 Epoch 15999 Error: 0.62764092 lr: 0.900000 \n",
      "It: 50000 Batch: 2 Epoch 16666 Error: 0.62757643 lr: 0.900000 \n",
      "Finished \n",
      " It: 50000 Batch: 2 Epoch 16666 Train Loss: 0.62757643 lr: 0.900000 \n",
      "0.0\n",
      "[1 1 0] [1 1 0]\n"
     ]
    }
   ],
   "source": [
    "from utils import dataset_helper\n",
    "reload(loss_functions)\n",
    "reload(activation_functions)\n",
    "reload(network)\n",
    "reload(dataset_helper)\n",
    "network.DEBUG = False\n",
    "\n",
    "eps = np.finfo(np.float32).eps\n",
    "\n",
    "X = np.array([[0.05, 0.10], [0.05, 0.10], [0.10, 0.05]], np.float64)\n",
    "Y = np.array([[0.0, 1], [0.0, 1], [1., 0.]], np.float64)\n",
    "\n",
    "h1 = network.Layer(2, 10, 'sigmoid',  label=\"H1\")\n",
    "o = network.Layer(10, 2, 'sigmoid',  label=\"Output\")\n",
    "\n",
    "model = network.NN(loss='cross_entropy')\n",
    "model.add_layer(h1)\n",
    "model.add_layer(o)\n",
    "model.show_weights()\n",
    "\n",
    "model.fit(X, Y, max_iter=50000, \n",
    "          lr=0.9, epsilon=eps,\n",
    "          print_interval=2000)\n",
    "\n",
    "Y_ = np.array(model.predict(X))\n",
    "Y_ = Y_.argmax(axis=-1).flatten()\n",
    "Y = Y.argmax(axis=-1)\n",
    "mae = np.absolute(Y - Y_).mean()\n",
    "print(mae)\n",
    "print(Y, Y_)\n",
    "assert(mae < 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi class Classification Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclasses = 5\n",
    "nsamples = 60000\n",
    "nfeatures = 60\n",
    "\n",
    "X,  X_val, Y, Y_val = dataset_helper.get_toy_data_multiclass(nclasses, nsamples, nfeatures)\n",
    "Y = dataset_helper.one_hot_encode(Y, nclasses)\n",
    "Y_val = dataset_helper.one_hot_encode(Y_val, nclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary\n",
      "-------------------------------\n",
      "H1      (input=60, neurons=32, activation=sigmoid)\n",
      "H2      (input=32, neurons=64, activation=sigmoid)\n",
      "H3      (input=64, neurons=24, activation=sigmoid)\n",
      "Output  (input=24, neurons=5, activation=sigmoid)\n",
      "-------------------------------\n",
      "\n",
      "Shuffled\n",
      "It: 10000 Batch: 10000 Epoch 0 Train Loss: 1.36657578 lr: 0.900000 Val Loss: 1.06702362\n",
      "It: 20000 Batch: 20000 Epoch 0 Train Loss: 1.09489642 lr: 0.900000 Val Loss: 1.25005179\n",
      "It: 30000 Batch: 30000 Epoch 0 Train Loss: 0.91366700 lr: 0.900000 Val Loss: 0.81649533\n",
      "It: 40000 Batch: 40000 Epoch 0 Train Loss: 0.62271033 lr: 0.900000 Val Loss: 0.44435940\n",
      "It: 50000 Batch: 2000 Epoch 1 Train Loss: 0.49507521 lr: 0.900000 Val Loss: 0.43634736\n",
      "It: 60000 Batch: 12000 Epoch 1 Train Loss: 0.49657263 lr: 0.900000 Val Loss: 0.43590114\n",
      "It: 70000 Batch: 22000 Epoch 1 Train Loss: 0.49351742 lr: 0.900000 Val Loss: 0.66531414\n",
      "It: 80000 Batch: 32000 Epoch 1 Train Loss: 0.49131257 lr: 0.900000 Val Loss: 0.43663375\n",
      "It: 90000 Batch: 42000 Epoch 1 Train Loss: 0.49360646 lr: 0.900000 Val Loss: 0.49834737\n",
      "It: 100000 Batch: 4000 Epoch 2 Train Loss: 0.49276572 lr: 0.900000 Val Loss: 0.64342610\n",
      "It: 110000 Batch: 14000 Epoch 2 Train Loss: 0.49107077 lr: 0.900000 Val Loss: 0.47131551\n",
      "It: 120000 Batch: 24000 Epoch 2 Train Loss: 0.49457668 lr: 0.900000 Val Loss: 0.43024169\n",
      "It: 130000 Batch: 34000 Epoch 2 Train Loss: 0.48419607 lr: 0.810000 Val Loss: 0.69019214\n",
      "It: 140000 Batch: 44000 Epoch 2 Train Loss: 0.48326923 lr: 0.810000 Val Loss: 0.43407711\n",
      "It: 150000 Batch: 6000 Epoch 3 Train Loss: 0.48663477 lr: 0.810000 Val Loss: 0.43016140\n",
      "It: 160000 Batch: 16000 Epoch 3 Train Loss: 0.48371776 lr: 0.810000 Val Loss: 0.57115714\n",
      "It: 170000 Batch: 26000 Epoch 3 Train Loss: 0.48474723 lr: 0.810000 Val Loss: 0.67901663\n",
      "It: 180000 Batch: 36000 Epoch 3 Train Loss: 0.48459550 lr: 0.810000 Val Loss: 0.43147441\n",
      "It: 190000 Batch: 46000 Epoch 3 Train Loss: 0.48407044 lr: 0.810000 Val Loss: 0.42764571\n",
      "It: 200000 Batch: 8000 Epoch 4 Train Loss: 0.48350077 lr: 0.810000 Val Loss: 0.42909941\n",
      "It: 210000 Batch: 18000 Epoch 4 Train Loss: 0.48371456 lr: 0.810000 Val Loss: 0.43344479\n",
      "It: 220000 Batch: 28000 Epoch 4 Train Loss: 0.48350222 lr: 0.810000 Val Loss: 0.55248498\n",
      "It: 230000 Batch: 38000 Epoch 4 Train Loss: 0.48614414 lr: 0.810000 Val Loss: 0.43394820\n",
      "It: 240000 Batch: 48000 Epoch 4 Train Loss: 0.48541289 lr: 0.810000 Val Loss: 0.65687475\n",
      "It: 250000 Batch: 10000 Epoch 5 Train Loss: 0.47769025 lr: 0.729000 Val Loss: 0.68736334\n",
      "It: 260000 Batch: 20000 Epoch 5 Train Loss: 0.47688997 lr: 0.729000 Val Loss: 0.43675653\n",
      "It: 270000 Batch: 30000 Epoch 5 Train Loss: 0.47749864 lr: 0.729000 Val Loss: 0.42755724\n",
      "It: 280000 Batch: 40000 Epoch 5 Train Loss: 0.47806867 lr: 0.729000 Val Loss: 0.42931194\n",
      "It: 290000 Batch: 2000 Epoch 6 Train Loss: 0.47767588 lr: 0.729000 Val Loss: 0.45680891\n",
      "It: 300000 Batch: 12000 Epoch 6 Train Loss: 0.47656291 lr: 0.729000 Val Loss: 0.43471977\n",
      "It: 310000 Batch: 22000 Epoch 6 Train Loss: 0.47752272 lr: 0.729000 Val Loss: 0.48993332\n",
      "It: 320000 Batch: 32000 Epoch 6 Train Loss: 0.47676582 lr: 0.729000 Val Loss: 0.42862064\n",
      "It: 330000 Batch: 42000 Epoch 6 Train Loss: 0.47681891 lr: 0.729000 Val Loss: 0.42589360\n",
      "It: 340000 Batch: 4000 Epoch 7 Train Loss: 0.47742233 lr: 0.729000 Val Loss: 0.43080365\n",
      "It: 350000 Batch: 14000 Epoch 7 Train Loss: 0.47734648 lr: 0.729000 Val Loss: 0.42521761\n",
      "It: 360000 Batch: 24000 Epoch 7 Train Loss: 0.47763965 lr: 0.729000 Val Loss: 0.44512785\n",
      "It: 370000 Batch: 34000 Epoch 7 Train Loss: 0.47047628 lr: 0.656100 Val Loss: 0.50569974\n",
      "It: 380000 Batch: 44000 Epoch 7 Train Loss: 0.47109183 lr: 0.656100 Val Loss: 0.42407093\n",
      "It: 390000 Batch: 6000 Epoch 8 Train Loss: 0.46847993 lr: 0.656100 Val Loss: 0.66348578\n",
      "It: 400000 Batch: 16000 Epoch 8 Train Loss: 0.46992810 lr: 0.656100 Val Loss: 0.44701077\n",
      "It: 410000 Batch: 26000 Epoch 8 Train Loss: 0.46854407 lr: 0.656100 Val Loss: 0.55170954\n",
      "It: 420000 Batch: 36000 Epoch 8 Train Loss: 0.46966797 lr: 0.656100 Val Loss: 0.65911279\n",
      "It: 430000 Batch: 46000 Epoch 8 Train Loss: 0.46985794 lr: 0.656100 Val Loss: 0.65635647\n",
      "It: 440000 Batch: 8000 Epoch 9 Train Loss: 0.46953654 lr: 0.656100 Val Loss: 0.42999510\n",
      "It: 450000 Batch: 18000 Epoch 9 Train Loss: 0.46996105 lr: 0.656100 Val Loss: 0.44418529\n",
      "It: 460000 Batch: 28000 Epoch 9 Train Loss: 0.46966601 lr: 0.656100 Val Loss: 0.65639658\n",
      "It: 470000 Batch: 38000 Epoch 9 Train Loss: 0.46990887 lr: 0.656100 Val Loss: 0.42446863\n",
      "It: 480000 Batch: 48000 Epoch 9 Train Loss: 0.47017527 lr: 0.656100 Val Loss: 0.42833886\n",
      "It: 490000 Batch: 10000 Epoch 10 Train Loss: 0.46266008 lr: 0.590490 Val Loss: 0.64601525\n",
      "It: 500000 Batch: 20000 Epoch 10 Train Loss: 0.46301068 lr: 0.590490 Val Loss: 0.42381355\n",
      "It: 510000 Batch: 30000 Epoch 10 Train Loss: 0.46400860 lr: 0.590490 Val Loss: 0.42241812\n",
      "It: 520000 Batch: 40000 Epoch 10 Train Loss: 0.46198583 lr: 0.590490 Val Loss: 0.42833892\n",
      "It: 530000 Batch: 2000 Epoch 11 Train Loss: 0.46342020 lr: 0.590490 Val Loss: 0.42070092\n",
      "It: 540000 Batch: 12000 Epoch 11 Train Loss: 0.46319658 lr: 0.590490 Val Loss: 0.42695852\n",
      "It: 550000 Batch: 22000 Epoch 11 Train Loss: 0.46324554 lr: 0.590490 Val Loss: 0.59956579\n",
      "It: 560000 Batch: 32000 Epoch 11 Train Loss: 0.46307630 lr: 0.590490 Val Loss: 0.43244539\n",
      "It: 570000 Batch: 42000 Epoch 11 Train Loss: 0.46180666 lr: 0.590490 Val Loss: 0.64067583\n",
      "It: 580000 Batch: 4000 Epoch 12 Train Loss: 0.46436527 lr: 0.590490 Val Loss: 0.42202340\n",
      "It: 590000 Batch: 14000 Epoch 12 Train Loss: 0.46334754 lr: 0.590490 Val Loss: 0.42276814\n",
      "It: 600000 Batch: 24000 Epoch 12 Train Loss: 0.46332534 lr: 0.590490 Val Loss: 0.44060102\n",
      "It: 610000 Batch: 34000 Epoch 12 Train Loss: 0.45766762 lr: 0.531441 Val Loss: 0.59928229\n",
      "It: 620000 Batch: 44000 Epoch 12 Train Loss: 0.45832499 lr: 0.531441 Val Loss: 0.62387016\n",
      "It: 630000 Batch: 6000 Epoch 13 Train Loss: 0.45685817 lr: 0.531441 Val Loss: 0.42295491\n",
      "It: 640000 Batch: 16000 Epoch 13 Train Loss: 0.45755687 lr: 0.531441 Val Loss: 0.44482225\n",
      "It: 650000 Batch: 26000 Epoch 13 Train Loss: 0.45723550 lr: 0.531441 Val Loss: 0.42235169\n",
      "It: 660000 Batch: 36000 Epoch 13 Train Loss: 0.45768900 lr: 0.531441 Val Loss: 0.42335835\n",
      "It: 670000 Batch: 46000 Epoch 13 Train Loss: 0.45741842 lr: 0.531441 Val Loss: 0.43038349\n",
      "It: 680000 Batch: 8000 Epoch 14 Train Loss: 0.45802109 lr: 0.531441 Val Loss: 0.42302117\n",
      "It: 690000 Batch: 18000 Epoch 14 Train Loss: 0.45689466 lr: 0.531441 Val Loss: 0.42118833\n",
      "It: 700000 Batch: 28000 Epoch 14 Train Loss: 0.45671113 lr: 0.531441 Val Loss: 0.63382493\n",
      "It: 710000 Batch: 38000 Epoch 14 Train Loss: 0.45649808 lr: 0.531441 Val Loss: 0.44220233\n",
      "It: 720000 Batch: 48000 Epoch 14 Train Loss: 0.45864557 lr: 0.531441 Val Loss: 0.47650610\n",
      "It: 730000 Batch: 10000 Epoch 15 Train Loss: 0.45217069 lr: 0.478297 Val Loss: 0.42152547\n",
      "It: 740000 Batch: 20000 Epoch 15 Train Loss: 0.45228819 lr: 0.478297 Val Loss: 0.41977913\n",
      "It: 750000 Batch: 30000 Epoch 15 Train Loss: 0.45228629 lr: 0.478297 Val Loss: 0.41758534\n",
      "It: 760000 Batch: 40000 Epoch 15 Train Loss: 0.45180952 lr: 0.478297 Val Loss: 0.41616415\n",
      "It: 770000 Batch: 2000 Epoch 16 Train Loss: 0.45235470 lr: 0.478297 Val Loss: 0.42348268\n",
      "It: 780000 Batch: 12000 Epoch 16 Train Loss: 0.45148370 lr: 0.478297 Val Loss: 0.43631195\n",
      "It: 790000 Batch: 22000 Epoch 16 Train Loss: 0.45248016 lr: 0.478297 Val Loss: 0.47353325\n",
      "It: 800000 Batch: 32000 Epoch 16 Train Loss: 0.45157870 lr: 0.478297 Val Loss: 0.41742019\n",
      "It: 810000 Batch: 42000 Epoch 16 Train Loss: 0.45243868 lr: 0.478297 Val Loss: 0.42306979\n",
      "It: 820000 Batch: 4000 Epoch 17 Train Loss: 0.45182670 lr: 0.478297 Val Loss: 0.52205317\n",
      "It: 830000 Batch: 14000 Epoch 17 Train Loss: 0.45253708 lr: 0.478297 Val Loss: 0.42462245\n",
      "It: 840000 Batch: 24000 Epoch 17 Train Loss: 0.45302980 lr: 0.478297 Val Loss: 0.56676292\n",
      "It: 850000 Batch: 34000 Epoch 17 Train Loss: 0.44695769 lr: 0.430467 Val Loss: 0.42368067\n",
      "It: 860000 Batch: 44000 Epoch 17 Train Loss: 0.44692118 lr: 0.430467 Val Loss: 0.45798742\n",
      "It: 870000 Batch: 6000 Epoch 18 Train Loss: 0.44799940 lr: 0.430467 Val Loss: 0.59623993\n",
      "It: 880000 Batch: 16000 Epoch 18 Train Loss: 0.44689310 lr: 0.430467 Val Loss: 0.42606939\n",
      "It: 890000 Batch: 26000 Epoch 18 Train Loss: 0.44722251 lr: 0.430467 Val Loss: 0.41667069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 900000 Batch: 36000 Epoch 18 Train Loss: 0.44677339 lr: 0.430467 Val Loss: 0.42023392\n",
      "It: 910000 Batch: 46000 Epoch 18 Train Loss: 0.44606588 lr: 0.430467 Val Loss: 0.42532126\n",
      "It: 920000 Batch: 8000 Epoch 19 Train Loss: 0.44708259 lr: 0.430467 Val Loss: 0.41652225\n",
      "It: 930000 Batch: 18000 Epoch 19 Train Loss: 0.44659447 lr: 0.430467 Val Loss: 0.60582586\n",
      "It: 940000 Batch: 28000 Epoch 19 Train Loss: 0.44740048 lr: 0.430467 Val Loss: 0.42975697\n",
      "It: 950000 Batch: 38000 Epoch 19 Train Loss: 0.44724726 lr: 0.430467 Val Loss: 0.41703145\n",
      "It: 960000 Batch: 48000 Epoch 19 Train Loss: 0.44641090 lr: 0.430467 Val Loss: 0.42368046\n",
      "It: 970000 Batch: 10000 Epoch 20 Train Loss: 0.44236296 lr: 0.387420 Val Loss: 0.45257934\n",
      "It: 980000 Batch: 20000 Epoch 20 Train Loss: 0.44264561 lr: 0.387420 Val Loss: 0.48260209\n",
      "It: 990000 Batch: 30000 Epoch 20 Train Loss: 0.44184217 lr: 0.387420 Val Loss: 0.43023297\n",
      "It: 1000000 Batch: 40000 Epoch 20 Train Loss: 0.44263120 lr: 0.387420 Val Loss: 0.53543482\n",
      "It: 1010000 Batch: 2000 Epoch 21 Train Loss: 0.44202278 lr: 0.387420 Val Loss: 0.43086517\n",
      "It: 1020000 Batch: 12000 Epoch 21 Train Loss: 0.44256585 lr: 0.387420 Val Loss: 0.41936330\n",
      "It: 1030000 Batch: 22000 Epoch 21 Train Loss: 0.44236324 lr: 0.387420 Val Loss: 0.42287033\n",
      "It: 1040000 Batch: 32000 Epoch 21 Train Loss: 0.44247787 lr: 0.387420 Val Loss: 0.56119889\n",
      "It: 1050000 Batch: 42000 Epoch 21 Train Loss: 0.44196550 lr: 0.387420 Val Loss: 0.42245937\n",
      "It: 1060000 Batch: 4000 Epoch 22 Train Loss: 0.44264596 lr: 0.387420 Val Loss: 0.42814346\n",
      "It: 1070000 Batch: 14000 Epoch 22 Train Loss: 0.44274951 lr: 0.387420 Val Loss: 0.44187322\n",
      "It: 1080000 Batch: 24000 Epoch 22 Train Loss: 0.44182468 lr: 0.387420 Val Loss: 0.42684445\n",
      "It: 1090000 Batch: 34000 Epoch 22 Train Loss: 0.43800013 lr: 0.348678 Val Loss: 0.46401672\n",
      "It: 1100000 Batch: 44000 Epoch 22 Train Loss: 0.43920697 lr: 0.348678 Val Loss: 0.42420014\n",
      "It: 1110000 Batch: 6000 Epoch 23 Train Loss: 0.43826242 lr: 0.348678 Val Loss: 0.50261571\n",
      "It: 1120000 Batch: 16000 Epoch 23 Train Loss: 0.43889176 lr: 0.348678 Val Loss: 0.41868598\n",
      "It: 1130000 Batch: 26000 Epoch 23 Train Loss: 0.43822078 lr: 0.348678 Val Loss: 0.41898405\n",
      "It: 1140000 Batch: 36000 Epoch 23 Train Loss: 0.43903148 lr: 0.348678 Val Loss: 0.41641779\n",
      "It: 1150000 Batch: 46000 Epoch 23 Train Loss: 0.43856359 lr: 0.348678 Val Loss: 0.46948390\n",
      "It: 1160000 Batch: 8000 Epoch 24 Train Loss: 0.43945883 lr: 0.348678 Val Loss: 0.45955838\n",
      "It: 1170000 Batch: 18000 Epoch 24 Train Loss: 0.43880589 lr: 0.348678 Val Loss: 0.42074722\n",
      "It: 1180000 Batch: 28000 Epoch 24 Train Loss: 0.43857773 lr: 0.348678 Val Loss: 0.41325523\n",
      "It: 1190000 Batch: 38000 Epoch 24 Train Loss: 0.43828229 lr: 0.348678 Val Loss: 0.41686638\n",
      "It: 1200000 Batch: 48000 Epoch 24 Train Loss: 0.43919422 lr: 0.348678 Val Loss: 0.42435325\n",
      "It: 1210000 Batch: 10000 Epoch 25 Train Loss: 0.43532544 lr: 0.313811 Val Loss: 0.41189362\n",
      "It: 1220000 Batch: 20000 Epoch 25 Train Loss: 0.43460603 lr: 0.313811 Val Loss: 0.41431391\n",
      "It: 1230000 Batch: 30000 Epoch 25 Train Loss: 0.43471131 lr: 0.313811 Val Loss: 0.41520339\n",
      "It: 1240000 Batch: 40000 Epoch 25 Train Loss: 0.43396494 lr: 0.313811 Val Loss: 0.43001392\n",
      "It: 1250000 Batch: 2000 Epoch 26 Train Loss: 0.43493722 lr: 0.313811 Val Loss: 0.44348532\n",
      "It: 1260000 Batch: 12000 Epoch 26 Train Loss: 0.43502912 lr: 0.313811 Val Loss: 0.49398436\n",
      "It: 1270000 Batch: 22000 Epoch 26 Train Loss: 0.43479056 lr: 0.313811 Val Loss: 0.41198921\n",
      "It: 1280000 Batch: 32000 Epoch 26 Train Loss: 0.43506005 lr: 0.313811 Val Loss: 0.44004970\n",
      "It: 1290000 Batch: 42000 Epoch 26 Train Loss: 0.43468313 lr: 0.313811 Val Loss: 0.42376769\n",
      "It: 1300000 Batch: 4000 Epoch 27 Train Loss: 0.43492386 lr: 0.313811 Val Loss: 0.56677428\n",
      "It: 1310000 Batch: 14000 Epoch 27 Train Loss: 0.43399165 lr: 0.313811 Val Loss: 0.41918463\n",
      "It: 1320000 Batch: 24000 Epoch 27 Train Loss: 0.43538102 lr: 0.313811 Val Loss: 0.43383943\n",
      "It: 1330000 Batch: 34000 Epoch 27 Train Loss: 0.43124381 lr: 0.282430 Val Loss: 0.42327128\n",
      "It: 1340000 Batch: 44000 Epoch 27 Train Loss: 0.43148159 lr: 0.282430 Val Loss: 0.42583608\n",
      "It: 1350000 Batch: 6000 Epoch 28 Train Loss: 0.43132800 lr: 0.282430 Val Loss: 0.42588738\n",
      "It: 1360000 Batch: 16000 Epoch 28 Train Loss: 0.43165709 lr: 0.282430 Val Loss: 0.43572917\n",
      "It: 1370000 Batch: 26000 Epoch 28 Train Loss: 0.43173516 lr: 0.282430 Val Loss: 0.44938763\n",
      "It: 1380000 Batch: 36000 Epoch 28 Train Loss: 0.43073815 lr: 0.282430 Val Loss: 0.48732144\n",
      "It: 1390000 Batch: 46000 Epoch 28 Train Loss: 0.43220476 lr: 0.282430 Val Loss: 0.42180323\n",
      "It: 1400000 Batch: 8000 Epoch 29 Train Loss: 0.43085792 lr: 0.282430 Val Loss: 0.42583487\n",
      "It: 1410000 Batch: 18000 Epoch 29 Train Loss: 0.43117643 lr: 0.282430 Val Loss: 0.41149656\n",
      "It: 1420000 Batch: 28000 Epoch 29 Train Loss: 0.43152863 lr: 0.282430 Val Loss: 0.41360446\n",
      "It: 1430000 Batch: 38000 Epoch 29 Train Loss: 0.43175116 lr: 0.282430 Val Loss: 0.41040820\n",
      "It: 1440000 Batch: 48000 Epoch 29 Train Loss: 0.43065582 lr: 0.282430 Val Loss: 0.44166116\n",
      "It: 1450000 Batch: 10000 Epoch 30 Train Loss: 0.42800929 lr: 0.254187 Val Loss: 0.43224070\n",
      "It: 1460000 Batch: 20000 Epoch 30 Train Loss: 0.42794712 lr: 0.254187 Val Loss: 0.49871596\n",
      "It: 1470000 Batch: 30000 Epoch 30 Train Loss: 0.42853115 lr: 0.254187 Val Loss: 0.42883243\n",
      "It: 1480000 Batch: 40000 Epoch 30 Train Loss: 0.42771957 lr: 0.254187 Val Loss: 0.44382234\n",
      "It: 1490000 Batch: 2000 Epoch 31 Train Loss: 0.42840900 lr: 0.254187 Val Loss: 0.42249264\n",
      "It: 1500000 Batch: 12000 Epoch 31 Train Loss: 0.42917165 lr: 0.254187 Val Loss: 0.49873391\n",
      "It: 1510000 Batch: 22000 Epoch 31 Train Loss: 0.42865165 lr: 0.254187 Val Loss: 0.41047674\n",
      "It: 1520000 Batch: 32000 Epoch 31 Train Loss: 0.42795976 lr: 0.254187 Val Loss: 0.42884713\n",
      "It: 1530000 Batch: 42000 Epoch 31 Train Loss: 0.42789153 lr: 0.254187 Val Loss: 0.50188058\n",
      "It: 1540000 Batch: 4000 Epoch 32 Train Loss: 0.42838056 lr: 0.254187 Val Loss: 0.42173062\n",
      "It: 1550000 Batch: 14000 Epoch 32 Train Loss: 0.42809959 lr: 0.254187 Val Loss: 0.41241798\n",
      "It: 1560000 Batch: 24000 Epoch 32 Train Loss: 0.42816680 lr: 0.254187 Val Loss: 0.41037392\n",
      "It: 1570000 Batch: 34000 Epoch 32 Train Loss: 0.42602641 lr: 0.228768 Val Loss: 0.52100871\n",
      "It: 1580000 Batch: 44000 Epoch 32 Train Loss: 0.42503494 lr: 0.228768 Val Loss: 0.41164952\n",
      "It: 1590000 Batch: 6000 Epoch 33 Train Loss: 0.42570493 lr: 0.228768 Val Loss: 0.41306317\n",
      "It: 1600000 Batch: 16000 Epoch 33 Train Loss: 0.42611202 lr: 0.228768 Val Loss: 0.46889570\n",
      "It: 1610000 Batch: 26000 Epoch 33 Train Loss: 0.42563246 lr: 0.228768 Val Loss: 0.43416242\n",
      "It: 1620000 Batch: 36000 Epoch 33 Train Loss: 0.42519338 lr: 0.228768 Val Loss: 0.41018945\n",
      "It: 1630000 Batch: 46000 Epoch 33 Train Loss: 0.42621994 lr: 0.228768 Val Loss: 0.40992848\n",
      "It: 1640000 Batch: 8000 Epoch 34 Train Loss: 0.42626794 lr: 0.228768 Val Loss: 0.41069778\n",
      "It: 1650000 Batch: 18000 Epoch 34 Train Loss: 0.42584281 lr: 0.228768 Val Loss: 0.41135310\n",
      "It: 1660000 Batch: 28000 Epoch 34 Train Loss: 0.42618725 lr: 0.228768 Val Loss: 0.54275722\n",
      "It: 1670000 Batch: 38000 Epoch 34 Train Loss: 0.42589642 lr: 0.228768 Val Loss: 0.42619630\n",
      "It: 1680000 Batch: 48000 Epoch 34 Train Loss: 0.42499975 lr: 0.228768 Val Loss: 0.41964104\n",
      "It: 1690000 Batch: 10000 Epoch 35 Train Loss: 0.42315499 lr: 0.205891 Val Loss: 0.44956279\n",
      "It: 1700000 Batch: 20000 Epoch 35 Train Loss: 0.42352824 lr: 0.205891 Val Loss: 0.41454669\n",
      "It: 1710000 Batch: 30000 Epoch 35 Train Loss: 0.42342037 lr: 0.205891 Val Loss: 0.42344366\n",
      "It: 1720000 Batch: 40000 Epoch 35 Train Loss: 0.42329774 lr: 0.205891 Val Loss: 0.47666602\n",
      "It: 1730000 Batch: 2000 Epoch 36 Train Loss: 0.42253993 lr: 0.205891 Val Loss: 0.41077661\n",
      "It: 1740000 Batch: 12000 Epoch 36 Train Loss: 0.42315117 lr: 0.205891 Val Loss: 0.41036977\n",
      "It: 1750000 Batch: 22000 Epoch 36 Train Loss: 0.42297073 lr: 0.205891 Val Loss: 0.42517936\n",
      "It: 1760000 Batch: 32000 Epoch 36 Train Loss: 0.42280452 lr: 0.205891 Val Loss: 0.40915040\n",
      "It: 1770000 Batch: 42000 Epoch 36 Train Loss: 0.42281741 lr: 0.205891 Val Loss: 0.44819212\n",
      "It: 1780000 Batch: 4000 Epoch 37 Train Loss: 0.42336891 lr: 0.205891 Val Loss: 0.40921841\n",
      "It: 1790000 Batch: 14000 Epoch 37 Train Loss: 0.42355675 lr: 0.205891 Val Loss: 0.41354942\n",
      "It: 1800000 Batch: 24000 Epoch 37 Train Loss: 0.42254516 lr: 0.205891 Val Loss: 0.45830937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 1810000 Batch: 34000 Epoch 37 Train Loss: 0.42148348 lr: 0.185302 Val Loss: 0.43601040\n",
      "It: 1820000 Batch: 44000 Epoch 37 Train Loss: 0.42099883 lr: 0.185302 Val Loss: 0.41533174\n",
      "It: 1830000 Batch: 6000 Epoch 38 Train Loss: 0.42057018 lr: 0.185302 Val Loss: 0.41452096\n",
      "It: 1840000 Batch: 16000 Epoch 38 Train Loss: 0.42048414 lr: 0.185302 Val Loss: 0.40772191\n",
      "It: 1850000 Batch: 26000 Epoch 38 Train Loss: 0.42090071 lr: 0.185302 Val Loss: 0.41679459\n",
      "It: 1860000 Batch: 36000 Epoch 38 Train Loss: 0.42097870 lr: 0.185302 Val Loss: 0.41129569\n",
      "It: 1870000 Batch: 46000 Epoch 38 Train Loss: 0.42085604 lr: 0.185302 Val Loss: 0.40724518\n",
      "It: 1880000 Batch: 8000 Epoch 39 Train Loss: 0.42089725 lr: 0.185302 Val Loss: 0.41358648\n",
      "It: 1890000 Batch: 18000 Epoch 39 Train Loss: 0.42104421 lr: 0.185302 Val Loss: 0.41388406\n",
      "It: 1900000 Batch: 28000 Epoch 39 Train Loss: 0.42172330 lr: 0.185302 Val Loss: 0.41224354\n",
      "It: 1910000 Batch: 38000 Epoch 39 Train Loss: 0.42074846 lr: 0.185302 Val Loss: 0.41968439\n",
      "It: 1920000 Batch: 48000 Epoch 39 Train Loss: 0.42096165 lr: 0.185302 Val Loss: 0.43940355\n",
      "It: 1930000 Batch: 10000 Epoch 40 Train Loss: 0.41837079 lr: 0.166772 Val Loss: 0.41331456\n",
      "It: 1940000 Batch: 20000 Epoch 40 Train Loss: 0.41836252 lr: 0.166772 Val Loss: 0.41712409\n",
      "It: 1950000 Batch: 30000 Epoch 40 Train Loss: 0.41959730 lr: 0.166772 Val Loss: 0.41265485\n",
      "It: 1960000 Batch: 40000 Epoch 40 Train Loss: 0.41858876 lr: 0.166772 Val Loss: 0.41263357\n",
      "It: 1970000 Batch: 2000 Epoch 41 Train Loss: 0.41890042 lr: 0.166772 Val Loss: 0.40837907\n",
      "It: 1980000 Batch: 12000 Epoch 41 Train Loss: 0.41871842 lr: 0.166772 Val Loss: 0.46207505\n",
      "It: 1990000 Batch: 22000 Epoch 41 Train Loss: 0.41903058 lr: 0.166772 Val Loss: 0.42613485\n",
      "It: 2000000 Batch: 32000 Epoch 41 Train Loss: 0.41875116 lr: 0.166772 Val Loss: 0.40749891\n",
      "It: 2010000 Batch: 42000 Epoch 41 Train Loss: 0.41915529 lr: 0.166772 Val Loss: 0.41643819\n",
      "It: 2020000 Batch: 4000 Epoch 42 Train Loss: 0.41934571 lr: 0.166772 Val Loss: 0.40553274\n",
      "It: 2030000 Batch: 14000 Epoch 42 Train Loss: 0.41917127 lr: 0.166772 Val Loss: 0.40996707\n",
      "It: 2040000 Batch: 24000 Epoch 42 Train Loss: 0.41886662 lr: 0.166772 Val Loss: 0.40593560\n",
      "It: 2050000 Batch: 34000 Epoch 42 Train Loss: 0.41730001 lr: 0.150095 Val Loss: 0.43275173\n",
      "It: 2060000 Batch: 44000 Epoch 42 Train Loss: 0.41696457 lr: 0.150095 Val Loss: 0.41909081\n",
      "It: 2070000 Batch: 6000 Epoch 43 Train Loss: 0.41736111 lr: 0.150095 Val Loss: 0.40452038\n",
      "It: 2080000 Batch: 16000 Epoch 43 Train Loss: 0.41740297 lr: 0.150095 Val Loss: 0.42132615\n",
      "It: 2090000 Batch: 26000 Epoch 43 Train Loss: 0.41762460 lr: 0.150095 Val Loss: 0.40485590\n",
      "It: 2100000 Batch: 36000 Epoch 43 Train Loss: 0.41708758 lr: 0.150095 Val Loss: 0.40706238\n",
      "It: 2110000 Batch: 46000 Epoch 43 Train Loss: 0.41656137 lr: 0.150095 Val Loss: 0.40862181\n",
      "It: 2120000 Batch: 8000 Epoch 44 Train Loss: 0.41756496 lr: 0.150095 Val Loss: 0.41114827\n",
      "It: 2130000 Batch: 18000 Epoch 44 Train Loss: 0.41676073 lr: 0.150095 Val Loss: 0.43897028\n",
      "It: 2140000 Batch: 28000 Epoch 44 Train Loss: 0.41697892 lr: 0.150095 Val Loss: 0.42520116\n",
      "It: 2150000 Batch: 38000 Epoch 44 Train Loss: 0.41755937 lr: 0.150095 Val Loss: 0.41180342\n",
      "It: 2160000 Batch: 48000 Epoch 44 Train Loss: 0.41690775 lr: 0.150095 Val Loss: 0.40460843\n",
      "It: 2170000 Batch: 10000 Epoch 45 Train Loss: 0.41577680 lr: 0.135085 Val Loss: 0.40769617\n",
      "It: 2180000 Batch: 20000 Epoch 45 Train Loss: 0.41569634 lr: 0.135085 Val Loss: 0.42071049\n",
      "It: 2190000 Batch: 30000 Epoch 45 Train Loss: 0.41532460 lr: 0.135085 Val Loss: 0.40370037\n",
      "It: 2200000 Batch: 40000 Epoch 45 Train Loss: 0.41542711 lr: 0.135085 Val Loss: 0.41622164\n",
      "It: 2210000 Batch: 2000 Epoch 46 Train Loss: 0.41502966 lr: 0.135085 Val Loss: 0.41759072\n",
      "It: 2220000 Batch: 12000 Epoch 46 Train Loss: 0.41554772 lr: 0.135085 Val Loss: 0.40712357\n",
      "It: 2230000 Batch: 22000 Epoch 46 Train Loss: 0.41546539 lr: 0.135085 Val Loss: 0.41943756\n",
      "It: 2240000 Batch: 32000 Epoch 46 Train Loss: 0.41613547 lr: 0.135085 Val Loss: 0.40728458\n",
      "It: 2250000 Batch: 42000 Epoch 46 Train Loss: 0.41472636 lr: 0.135085 Val Loss: 0.42513111\n",
      "It: 2260000 Batch: 4000 Epoch 47 Train Loss: 0.41495796 lr: 0.135085 Val Loss: 0.40500747\n",
      "It: 2270000 Batch: 14000 Epoch 47 Train Loss: 0.41452287 lr: 0.135085 Val Loss: 0.43646546\n",
      "It: 2280000 Batch: 24000 Epoch 47 Train Loss: 0.41533192 lr: 0.135085 Val Loss: 0.40716335\n",
      "It: 2290000 Batch: 34000 Epoch 47 Train Loss: 0.41363282 lr: 0.121577 Val Loss: 0.42214747\n",
      "It: 2300000 Batch: 44000 Epoch 47 Train Loss: 0.41446550 lr: 0.121577 Val Loss: 0.41105615\n",
      "It: 2310000 Batch: 6000 Epoch 48 Train Loss: 0.41375665 lr: 0.121577 Val Loss: 0.41044693\n",
      "It: 2320000 Batch: 16000 Epoch 48 Train Loss: 0.41421997 lr: 0.121577 Val Loss: 0.40491510\n",
      "It: 2330000 Batch: 26000 Epoch 48 Train Loss: 0.41364537 lr: 0.121577 Val Loss: 0.42240203\n",
      "It: 2340000 Batch: 36000 Epoch 48 Train Loss: 0.41317267 lr: 0.121577 Val Loss: 0.40741908\n",
      "It: 2350000 Batch: 46000 Epoch 48 Train Loss: 0.41368619 lr: 0.121577 Val Loss: 0.43815481\n",
      "It: 2360000 Batch: 8000 Epoch 49 Train Loss: 0.41378025 lr: 0.121577 Val Loss: 0.41981322\n",
      "It: 2370000 Batch: 18000 Epoch 49 Train Loss: 0.41418609 lr: 0.121577 Val Loss: 0.42043247\n",
      "It: 2380000 Batch: 28000 Epoch 49 Train Loss: 0.41403859 lr: 0.121577 Val Loss: 0.40693331\n",
      "It: 2390000 Batch: 38000 Epoch 49 Train Loss: 0.41351050 lr: 0.121577 Val Loss: 0.40784653\n",
      "It: 2400000 Batch: 48000 Epoch 49 Train Loss: 0.41390529 lr: 0.121577 Val Loss: 0.41255728\n",
      "It: 2410000 Batch: 10000 Epoch 50 Train Loss: 0.41250684 lr: 0.109419 Val Loss: 0.40880819\n",
      "It: 2420000 Batch: 20000 Epoch 50 Train Loss: 0.41273060 lr: 0.109419 Val Loss: 0.41475518\n",
      "It: 2430000 Batch: 30000 Epoch 50 Train Loss: 0.41280891 lr: 0.109419 Val Loss: 0.40718404\n",
      "It: 2440000 Batch: 40000 Epoch 50 Train Loss: 0.41287734 lr: 0.109419 Val Loss: 0.41168321\n",
      "It: 2450000 Batch: 2000 Epoch 51 Train Loss: 0.41238834 lr: 0.109419 Val Loss: 0.40606940\n",
      "It: 2460000 Batch: 12000 Epoch 51 Train Loss: 0.41290566 lr: 0.109419 Val Loss: 0.42172242\n",
      "It: 2470000 Batch: 22000 Epoch 51 Train Loss: 0.41270559 lr: 0.109419 Val Loss: 0.41117229\n",
      "It: 2480000 Batch: 32000 Epoch 51 Train Loss: 0.41236597 lr: 0.109419 Val Loss: 0.41166564\n",
      "It: 2490000 Batch: 42000 Epoch 51 Train Loss: 0.41264985 lr: 0.109419 Val Loss: 0.41512835\n",
      "It: 2500000 Batch: 4000 Epoch 52 Train Loss: 0.41257354 lr: 0.109419 Val Loss: 0.41379854\n",
      "It: 2510000 Batch: 14000 Epoch 52 Train Loss: 0.41258609 lr: 0.109419 Val Loss: 0.41671554\n",
      "It: 2520000 Batch: 24000 Epoch 52 Train Loss: 0.41243361 lr: 0.109419 Val Loss: 0.40953226\n",
      "It: 2530000 Batch: 34000 Epoch 52 Train Loss: 0.41149834 lr: 0.098477 Val Loss: 0.41947886\n",
      "It: 2540000 Batch: 44000 Epoch 52 Train Loss: 0.41145050 lr: 0.098477 Val Loss: 0.42855787\n",
      "It: 2550000 Batch: 6000 Epoch 53 Train Loss: 0.41134704 lr: 0.098477 Val Loss: 0.40552655\n",
      "It: 2560000 Batch: 16000 Epoch 53 Train Loss: 0.41103452 lr: 0.098477 Val Loss: 0.40413137\n",
      "It: 2570000 Batch: 26000 Epoch 53 Train Loss: 0.41156849 lr: 0.098477 Val Loss: 0.40724668\n",
      "It: 2580000 Batch: 36000 Epoch 53 Train Loss: 0.41079373 lr: 0.098477 Val Loss: 0.40448086\n",
      "It: 2590000 Batch: 46000 Epoch 53 Train Loss: 0.41181943 lr: 0.098477 Val Loss: 0.41597362\n",
      "It: 2600000 Batch: 8000 Epoch 54 Train Loss: 0.41143846 lr: 0.098477 Val Loss: 0.40832233\n",
      "It: 2610000 Batch: 18000 Epoch 54 Train Loss: 0.41111649 lr: 0.098477 Val Loss: 0.40780299\n",
      "It: 2620000 Batch: 28000 Epoch 54 Train Loss: 0.41167795 lr: 0.098477 Val Loss: 0.41288792\n",
      "It: 2630000 Batch: 38000 Epoch 54 Train Loss: 0.41148230 lr: 0.098477 Val Loss: 0.40830625\n",
      "It: 2640000 Batch: 48000 Epoch 54 Train Loss: 0.41161309 lr: 0.098477 Val Loss: 0.42111562\n",
      "It: 2650000 Batch: 10000 Epoch 55 Train Loss: 0.41031172 lr: 0.088629 Val Loss: 0.41369852\n",
      "It: 2660000 Batch: 20000 Epoch 55 Train Loss: 0.41034793 lr: 0.088629 Val Loss: 0.40249988\n",
      "It: 2670000 Batch: 30000 Epoch 55 Train Loss: 0.41015157 lr: 0.088629 Val Loss: 0.40835798\n",
      "It: 2680000 Batch: 40000 Epoch 55 Train Loss: 0.41062204 lr: 0.088629 Val Loss: 0.40329699\n",
      "It: 2690000 Batch: 2000 Epoch 56 Train Loss: 0.41058376 lr: 0.088629 Val Loss: 0.40764835\n",
      "It: 2700000 Batch: 12000 Epoch 56 Train Loss: 0.41034946 lr: 0.088629 Val Loss: 0.40809865\n",
      "It: 2710000 Batch: 22000 Epoch 56 Train Loss: 0.41057935 lr: 0.088629 Val Loss: 0.41479648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 2720000 Batch: 32000 Epoch 56 Train Loss: 0.41019048 lr: 0.088629 Val Loss: 0.42289414\n",
      "It: 2730000 Batch: 42000 Epoch 56 Train Loss: 0.41061096 lr: 0.088629 Val Loss: 0.40517369\n",
      "It: 2740000 Batch: 4000 Epoch 57 Train Loss: 0.41018725 lr: 0.088629 Val Loss: 0.41160635\n",
      "It: 2750000 Batch: 14000 Epoch 57 Train Loss: 0.40999942 lr: 0.088629 Val Loss: 0.42407892\n",
      "It: 2760000 Batch: 24000 Epoch 57 Train Loss: 0.41068301 lr: 0.088629 Val Loss: 0.41633677\n",
      "It: 2770000 Batch: 34000 Epoch 57 Train Loss: 0.40916820 lr: 0.079766 Val Loss: 0.40868129\n",
      "It: 2780000 Batch: 44000 Epoch 57 Train Loss: 0.40911096 lr: 0.079766 Val Loss: 0.40284140\n",
      "It: 2790000 Batch: 6000 Epoch 58 Train Loss: 0.40927515 lr: 0.079766 Val Loss: 0.40329655\n",
      "It: 2800000 Batch: 16000 Epoch 58 Train Loss: 0.40932267 lr: 0.079766 Val Loss: 0.40154871\n",
      "It: 2810000 Batch: 26000 Epoch 58 Train Loss: 0.40894806 lr: 0.079766 Val Loss: 0.40724473\n",
      "It: 2820000 Batch: 36000 Epoch 58 Train Loss: 0.40915517 lr: 0.079766 Val Loss: 0.40369735\n",
      "It: 2830000 Batch: 46000 Epoch 58 Train Loss: 0.40946836 lr: 0.079766 Val Loss: 0.40995131\n",
      "It: 2840000 Batch: 8000 Epoch 59 Train Loss: 0.40925577 lr: 0.079766 Val Loss: 0.40740614\n",
      "It: 2850000 Batch: 18000 Epoch 59 Train Loss: 0.40946461 lr: 0.079766 Val Loss: 0.40492378\n",
      "It: 2860000 Batch: 28000 Epoch 59 Train Loss: 0.40906458 lr: 0.079766 Val Loss: 0.41792333\n",
      "It: 2870000 Batch: 38000 Epoch 59 Train Loss: 0.40905494 lr: 0.079766 Val Loss: 0.40427600\n",
      "It: 2880000 Batch: 48000 Epoch 59 Train Loss: 0.40948068 lr: 0.079766 Val Loss: 0.40459506\n",
      "Finished \n",
      " It: 2880000 Batch: 48000 Epoch 59 Train Loss: 0.40948068 lr: 0.079766 Val Loss: 0.40459506\n",
      "Time Spent  422.454886796\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import timeit\n",
    "from utils import dataset_helper\n",
    "from utils import custom_scores\n",
    "reload(custom_scores)\n",
    "reload(dataset_helper)\n",
    "reload(loss_functions)\n",
    "reload(activation_functions)\n",
    "reload(network)\n",
    "reload(dataset_helper)\n",
    "\n",
    "eps = np.finfo(np.float32).eps\n",
    "\n",
    "\n",
    "lr = .9\n",
    "max_iter = 60 * X.shape[0]\n",
    "print_interval = 10*1000\n",
    "\n",
    "network.DEBUG = False\n",
    "\n",
    "eps = np.finfo(np.float32).eps\n",
    "\n",
    "h1 = network.Layer(nfeatures, 32, 'sigmoid', label=\"H1\")\n",
    "h2 = network.Layer(32, 64, 'sigmoid', label=\"H2\")\n",
    "h3 = network.Layer(64, 24, 'sigmoid', label=\"H3\")\n",
    "o = network.Layer(24, nclasses, 'sigmoid', label=\"Output\")\n",
    "\n",
    "model = network.NN(loss='smd')\n",
    "model.add_layer(h1)\n",
    "model.add_layer(h2)\n",
    "model.add_layer(h3)\n",
    "model.add_layer(o)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "start = time.process_time()\n",
    "model.fit(X, Y, max_iter=max_iter, \n",
    "          lr=lr, epsilon=eps, b_sz = 1,\n",
    "          decay_iteractions= 2.5 * X.shape[0], decay_rate = 0.9,\n",
    "          X_val=X_val, Y_val=Y_val,\n",
    "          print_interval=print_interval)\n",
    "\n",
    "Y_ = np.array(model.predict(X))\n",
    "Y_ = Y_.argmax(axis=-1).flatten()\n",
    "mae = np.absolute(Y.argmax(axis=-1) - Y_).mean()\n",
    "\n",
    "print(\"Time Spent \", time.process_time() - start)\n",
    "\n",
    "\n",
    "Y_val_ = np.array(model.predict(X_val))\n",
    "iteraction_log = network.get_iteration_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Stats...\n",
      "Accuracy: 0.199\n",
      "Precision: nan\n",
      "Recall: 0.200\n",
      "F1 Score: 0.332407\n",
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n",
      "Confusion matrix, without normalization\n",
      "Normalized confusion matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../utils/custom_scores.py:29: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision += TP / (TP + FP)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa4784aa6a0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEKCAYAAAD5MJl4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJztnXeYHMWZ/z81YaNW0kq7EsoBBEIIGZAIIplswBgwxiQHDAbuR7LvfD4Dh20w9p3N2WBMsDCYZIzJ2SabjBBIQjnnLO1KK22enVS/P6p7uiftzO7OaNXy+3meeaanu6a7akJ96w1VrbTWCIIgCIIbX29XQBAEQdjzEHEQBEEQ0hBxEARBENIQcRAEQRDSEHEQBEEQ0hBxEARBENIQcRAEQRDSEHEQBEEQ0hBxEARBENII5CqglHoYOAuo01pP7KTc4cCnwEVa6+dynbempkaPHj26C1UVBEEQZs+evV1rXVvs6+QUB+BR4F7gL9kKKKX8wO3AW/leePTo0cyaNSvf4oIgCAKglFq3O66T062ktf4QaMhR7HrgeaCuEJUSBEEQepcexxyUUsOArwPT8ih7lVJqllJqVn19fU8vLQiCIBSJQgSk7wJu0FrHcxXUWj+gtZ6itZ5SW1t0l5kgCILQTfKJOeRiCvCUUgqgBjhTKRXVWr9UgHMLgiAIvUCPxUFrPcbeVko9CvxdhEEQBMHb5JPK+iRwAlCjlNoI3AIEAbTW9xe1doIgCEKvkFMctNYX53syrfX3elQbQRAEYY/AczOkl21t5o63lrGjpaO3qyIIgrDX4jlxWFnXwj3vrmR7S7i3qyIIgrDX4jlxCPgVAJFYzsxZQRAEoZt4ThyCljhE47qXayIIgrD34jlxCPhMlaNiOQiCIBQND4qDWA6CIAjFxnvi4LctBxEHQRCEYuFBcbAC0nFxKwmCIBQLz4lD0CeWgyAIQrHxnDjYloMEpAVBEIqH58QhmHArieUgCIJQLDwnDpLKKgiCUHy8Jw4Jt5JYDoIgCMXCc+IQtFNZxa0kCIJQNDwnDs4kOHErCYIgFAsPioOpckTcSoIgCEXDe+IgqayCIAhFx7viIDEHQRCEouE5cQgm3EpiOQiCIBQLz4mDz6fwKUllFQRBKCaeEwcwK7PKwnuCIAjFw5PiEPQpYmI5CIIgFA1PikPA75OAtCAIQhHxpDgE/UoC0oIgCEXEk+JQqTo4afODEO3o7aoIgiDslXhSHCazlJPrH4NNX/R2VQRBEPZKPCkOFb6I2dCx3q2IIAjCXoonxaHMZ4lCXMRBEAShGHhTHJRYDoIgCMXEo+IQNRsyEU4QBKEoeFIcSm1xEMtBEAShKHhSHBzLQcRBEAShGOQUB6XUw0qpOqXUwizHv6WUmq+UWqCUmq6U+lLhq5lMKVbMIR4t9qUEQRD+JcnHcngUOL2T42uAL2utDwZ+CTxQgHp1SokEpAVBEIpKIFcBrfWHSqnRnRyf7no5Axje82p1TiniVhIEQSgmhY45fB94PdtBpdRVSqlZSqlZ9fX13b5Iie1W0pKtJAiCUAwKJg5KqRMx4nBDtjJa6we01lO01lNqa2u7fS0n5iCWgyAIQjHI6VbKB6XUJODPwBla6x2FOGdnBJGYgyAIQjHpseWglBoJvAB8R2u9vOdVyk2JxBwEQRCKSk7LQSn1JHACUKOU2gjcAgQBtNb3Az8HBgJ/VEoBRLXWU4pVYRDLQRAEodjkk610cY7jVwBXFKxGeRCUmIMgCEJR8eQM6aAOmw3JVhIEQSgKHhUHsRwEQRCKiSfFIaAl5iAIglBMvC0OYjkIgiAUBW+KQ9zEHOIiDoIgCEXBm+JgBaTjMVmVVRAEoRh4Uhz8llspHhPLQRAEoRh4UxziYjkIgiAUE2+Lg8QcBEEQioL3xEFrsRwEQRCKjPfEIR5Foc2miIMgCEJR8J44RDsSm1rcSoIgCEXB4+IgloMgCEIx8J44xBxxkFRWQRCE4uA9cRC3kiAIQtHxnjjEwolNEQdBEITi4D1xiIYSm5KtJAiCUBw8KA5iOQiCIBQb74mDBKQFQRCKjvfEQVJZBUEQio6nxUHWVhIEQSgO3hOHmKSyCoIgFBvvicOwKTSf+Uc2xGvlNqGCIAhFwnvi0H8ETLqARirFchAEQSgS3hMHoCTgI4ZPxEEQBKFIeFMc/D7iIg6CIAhFw5PioJQijl9iDoIgCEXCk+IAoJVPxEEQBKFIeFgc/KBFHARBEIqBh8XBBzJDWhAEoSh4Vhzw+UHHe7sWgiAIeyWeFQet/ChxKwmCIBSFnOKglHpYKVWnlFqY5bhSSt2tlFqplJqvlDqs8NXMdGGfxBwEQRCKRD6Ww6PA6Z0cPwMYZz2uAqb1vFp54POjxK0kCIJQFHKKg9b6Q6ChkyLnAH/RhhlAf6XUkEJVMCviVhIEQSgahYg5DAM2uF5vtPaloZS6Sik1Syk1q76+vmdXFctBEAShaOzWgLTW+gGt9RSt9ZTa2tqenUwsB0EQhKJRCHHYBIxwvR5u7SsqSiwHQRCEolEIcXgF+K6VtXQU0Ki13lKA83aOz49PxEEQBKEoBHIVUEo9CZwA1CilNgK3AEEArfX9wGvAmcBKoA24rFiVTaqXz49C3EqCIAjFIKc4aK0vznFcA9cWrEZ54vP58SGWgyAIQjHw7Axp5Re3kiAIQrHwrjiogFgOgiAIRcK74uD34ydONCYCIQiCUGg8Kw52zKEjKuIgCIJQaDwrDrblEBZxEARBKDieFQef38QcwuJWEgRBKDieFgc/cToiIg6CIAiFxtPiEFBxwlG5VaggCEKh8a44+PwAdEREHARBEAqNZ8XBHzCTu8ORSC/XRBAEYe/Du+Lgs8VBLAdBEIRC41lx8AWMWykiloMgCELB8a44WJZDVALSgiAIBce74uA3lkMsJuIgCIJQaDwsDsZyiMfkng6CIAiFxsPiYFsOEnMQBEEoNN4VByvmEIuK5SAIglBovCsOtlspLjEHQRCEQuNZcfBLQFoQBKFoeFYclLV8hgSkBUEQCo9nxcFePkPEQRAEofB4VhwSAWlxKwmCIBQcz4qDP2C7lUQcBEEQCo13xUEmwQmCIBQNz4qD7VaSVFZBEITC41lxwMpW0nGxHARBEAqNd8VBScxBEAShWHhXHGzLQcRBEASh4HheHCQgLQiCUHi8Kw5KYg6CIAjFwrvikAhIi1tJEASh0HhXHMRyEARBKBp5iYNS6nSl1DKl1Eql1I0Zjo9USr2nlJqjlJqvlDqz8FVNwWeqHhdxEARBKDg5xUEp5QfuA84AJgAXK6UmpBT7KfCM1vpQ4CLgj4WuaBrWJDgtAWlBEISCk4/lcASwUmu9WmsdBp4Czkkpo4G+1nY/YHPhqpgFy62ExBwEQRAKTj7iMAzY4Hq90drn5lbg20qpjcBrwPWZTqSUukopNUspNau+vr4b1XVhWQ4iDoIgCIWnUAHpi4FHtdbDgTOBx5VSaefWWj+gtZ6itZ5SW1vbsyvabiWJOQiCIBScfMRhEzDC9Xq4tc/N94FnALTWnwJlQE0hKpgVKyAtloMgCELhyUccZgLjlFJjlFIlmIDzKyll1gMnAyilDsSIQw/9RjlIuJXiRb2MIAjCvyI5xUFrHQWuA94ElmCykhYppW5TSp1tFftP4Eql1DzgSeB7WmtdrEoDTkBai+UgCIJQaAL5FNJav4YJNLv3/dy1vRg4prBVy0HCcpCYgyAIQqHx7gxpn6SyCoIgFAvvi4MWy0EQBKHQeFgcjFtJiVtJEASh4HhXHCQgLQiCUDS8Kw5iOQiCIBQND4uDsRyUxBwEQRAKjnfFwVqdQ2mZBCcIglBoPCwOijh+iTkIgiAUAe+KAxBXfrEcBEEQioDnxcEnk+AEQRAKjqfFQSsfPglIC4IgFBxPi0NcBVDEiceLu8afIAjCvxqeFgetfASIERVxEARBKCgeFwc/PuJE5Z4OgiAIBcXz4hAgTiQmloMgCEIh8bw4+FWcmLiVBEEQCoq3xcEXwE+MaCyHW2nWw/Dc5V07+bbF0Lyt+5UTBEHwMJ4WB5SPAPHcAekNM2HNh10797Sp8MAJ3a6aIAiCl/G0OGhfwASkc8Uc4hGIRZzXzdvgnV9AtkB2S71VbnNhKioIguAxPC0OKL+VyprDrRSLJN9OdMWb8PGdsHNN5vJ1i81zWf/C1FMQBMFjeFscfHYqay7LIZpsOYRbzbN7n5v6peZ54H49r6MgCIIH8bY4WKmsCbfSi1fD8rfSy8UixrVk09Fi7Q9nPm/dEvNcMaB79Vr2OrRu7957BUEQ9gA8LQ6JmIPtVpr/NKzNEHiOR0DHwb5rXLjF2Z8JWxyyWRadEWmHJy+GuU+Y10v+DvOedq4tCILgATwtDvgCBIiZSXDxGOhY5g7d3mc/53Ir7Vhhnruz4mu0A9BGJACe/R68eBW89VPz+pO74fHzun5eQRCE3YinxUH5fM4kuIQAZHAV2Z28bSnYlkM2cUiIRxa3U2fY17LfW9rHPNvWyLZFsHVB188rCIKwG/G0OOAL4CduJsHZnbH9vGMV7FqfvC+WIg7Z3Eqp5btCaj3CbcnnioWdY+s/g8fO7t51BEEQiojHxcHvrMqaGLFbzy9fC2/cZO2zOl+7TGdupXjMxCcgu3h0hn3OaNhsxzqs/S7RsMtsnAlrPoD2XdnP98ZNsOKdrtdDEAShB3haHJQ7IJ06Ym/fCaFGs50QjjxiDm5XUqwbMQe3hWBbKO7zujOnUuuciVmPwMq3u16Ple+IRSIIQrfxtDgYy8FKZU3taCPtVnAYl+VgPXd04lZKEofuxBxcwe8OtzjY+zvMeXWOOEnifR1dr0fDavjrN0xKrSAIQjfwtDg4loO7o7XdOiHzgOQOGzoPSLv3dcut5BIp20LJVId4NHdsw3ZxpYpD81aoW5q9DrYodTR3re6CIAgWgd6uQE9QVswhEounj8IjIZfl0MtupbL+6ZZNLOJsdzUwfscB5vnWxhx16Oha3QVBECw8bTn4AkF8xOmIuEbXdnwh2u50jvEUt5ItDhndSta+QFn33EpuC8YWh/LqdPFyB6azXSfakX5c53HvCrt8tBv1FwRBIE9xUEqdrpRappRaqZS6MUuZC5RSi5VSi5RSfytsNTPj9wcJEKMtHE3uaOMx85wac4hFzSPanrzfjb0vWNFzt5Lt3qkYkN5huy2HbG6lVFcZQFMeK8UmMqTEchAEoXvkdCsppfzAfcCpwEZgplLqFa31YleZccBNwDFa651KqUHFqrAbf8DcCa49kpKtZMcaEjEH1yS4iCsO0JlbqaQS2hq6Xil3JpJtoZRXw46Vyed3z3fIZjkkBMXVyW+anbsO7nTaYrHoJfAHYfxXi3cNQRB6jXwshyOAlVrr1VrrMPAUcE5KmSuB+7TWOwG01nWFrWZm/P4gfuK0R2LJAV976Yo0yyGSOUjsxu6Qu205uK9lBYTLB6RbAfFI5+KwfoZJx009npc42OctouUw/R749I/FO78gCL1KPuIwDNjger3R2udmf2B/pdQnSqkZSqnTM51IKXWVUmqWUmpWfX1992rsPp+VytoeTsn8SROHLBlEmTplu/MuqXRSTruCOxjsthw6C0inWjChRnjkDPjiL+nH7eXEUdnrlsni6A6xKLx+AzRuynCsQ9xWgrAXU6iAdAAYB5wAXAw8qJRKu1OO1voBrfUUrfWU2tranl/VFyCgLMvBHXOw3Un2QnzaWhE1Hk1O78yUjeR2K0HXV1NNizkoKOtnrq01STOmswWkw60mhbVte/rxhLjp7HWLZhGdrrJzLXx2P6z6Z+Zr9FR8BEHYY8lHHDYBI1yvh1v73GwEXtFaR7TWa4DlGLEoLj4/fmK0hWOZLQdImaXcRbdStjKd4Z6NHW41IhMocfa53UvZ3Ep2pxtqSj/u7pCzjdwL5VaKtKVf031tEQdB2GvJRxxmAuOUUmOUUiXARcArKWVewlgNKKVqMG6m1QWsZ2Z8AfxoQtksB0iepRyPdMGtVJH8Ol+SXFjNUNIH/CXOvnzcSnana1s57uNRl/DlDGT3MCCd6p5LqmNY3EqCsBeTUxy01lHgOuBNYAnwjNZ6kVLqNqXU2VaxN4EdSqnFwHvAf2mtdxSr0gksy6E9nCIOWS2HaPrrVBKWg+VW6rI4pAS/SyqziEMnbiVb3BLikMVyyNb5F2oSnC1EbrFNXEMsB0HYm8lrhrTW+jXgtZR9P3dta+BH1mP3oTK4leLRZHFItRzcHW1nbqWSbrqV3B1+R4u5n4M/aPalurSyuZXs1x22W8ltObg66qxuJWt/TzvvXJZDV4P1giB4Bk/PkMZaW8m4ldzzHLJZDmHn/grZZkC7J8G5X+dLamaU260UTpljkdWtZFsOGWIOkVDuuuWz2ms22nfCmzebzt8Wh0witCdbDlvmmVRbQRC6jcfFwY8PTXtHJHnEHnGNrlMD0naQtaxfjmylPsmv88U93yI15hBJiXdkdSulxhxS3EqlfZPLpbUh0vnxzlj9AXx6L2xbmN1y0NYquF1xW835qzNvo9jMfwbe/rlYNoLQAzwvDgAdkUiy+8edrprkVoqaUbnyZZ/kluZW6uLie0kL79kxhwxupc5mSNudsX3ToVS3UlnfzO9LbUN3LAf37PJEtlJKzMHtwssn1bdps7n50qIXu16f7hANWavZyv0sBKG7eHpVVpQRh2gknNwR2u4YSJnXYKW5BsrNaD4p/hCHN25wOuZuu5UyxBx8ljh0pATDs81HyNoZx81ovTSHONjn7Y7lYAtCpM0lFCnnSQqKdzhCmg1bFG2XXrFJWDztThqxIAhdwtvi4DPV74hEkztYtziEUwLSkXYIlpnRvNut1LYdPn8AKq3JebZbqcsBaVeH3b6zk5hDHpaDu6x7Al3elkN3xCHkPGdzKyW5uUK5xSEhOO2dlysU9nUiIeM+FAShy3jcrWTEIRwJo91pnSHXfQ5SU1ejVkDXF0ju+O2O276fc3fnObjdULEOSxxst1JKcDxbQDqtU7dmQ9sj+Zwxhx7MkHa7krKJQ9JEvDxcV7bgRHeTOER38/XypXEjvHTtnhvIFwQXHhcH41by6TixJHFwu5VSOuRIm8lUSnUr2Z1ivEDZSjbueQ5Jqawpy4y7yTYj2e5kc1oOPQhI2x1rpN0lDqluro708m60NrcqtelNy2FPYs2HMPevsGNVb9dEEHKyV4hDgDjRiKvDcscZUucWREIQLE93K7n94b6A06F3d56DTWl33EqZJp2F0y2HrOKQ4SZB+ZLoWNtdk+BSLYdw5m2ble/APZPNSBlcgtMLMYc9CXc8RxD2cLwtDlZA2k+MWCQlIG25nBLLZgOJG/0Ey9PdSu40U3+Ja1ZzD8Uhm1sp2tHJPIcs8y/sTtr2o2edId2TgLSrY+2u5dC0yWQLtWyzzml3irtpJB/dQy0H+/N0/w4EYQ/F2+JgCYCfODG35RBqMvdthvQZ0pH2zG4lt+XgD4LfEpcuxxwyiUMGyyEaAqw8/LwtB6tzyWk55Lj9aCZad8CC51yjfFcqa2oMxC1Kma5hf+b2c8Ia2V2WQy/EHHatd+JV2UiIg1gOwp6Px8XBshxUjHg0xXKwR9dpk+DaTTwh1a3k7rj8JU76aU+ylSD7PIfOFgDMGHNwLZFdWmXtyxGQ7orlMP9peP77jiso0ubqZFOzlXJYDonU1RRxyFS2GPSG5fCXc+D933Rexv6Nub97QdhD8bg4uCyH1IC0LQ6ZLIdEKqvbcnCVK6RbqbQqs1spdSmNpHNkEoeI07naAelsbqXE/ImO/GcJ2xleLdZN/JKylVI62dR5DqnYrjy7je44xu6gN2IOLXWOGy0bic9BxEHY8/G4ONgxh3hyKmu42ckSCqdMgkuksgZTUlkL4FYKNebIVsomDnm6lSL5BqTdbp88628H8VstcegsIJ1tlVgbu232Od0ZULsD91yN3YHWps25LIKE5SBuJWHPx9vioBxxiKeOogNl4C91LAflc9ZWCmSYBJcakO6qW6lpC/x2P1iZcte0bDGHbrmVUlNZc7iVOiuTii2ibdZK6/laDpnOb3/mCbdSjlTWLfPMmk6FIrobLIe1HzsWWqQd0LnFISxuJcE7eFscLLdSgBjNra1on2vCd7AMAqVOBxWszJDKmiUg7Qs6rqB8R94711jn00aIbJJiDq5rpMZC3GQUB1e2UsJyyLYqq3stpjyD0h0pGTSRNteEuCxLimerq922hFspRyrre7+G1/4rv3rmIhZxJiL2xHJY+zG8f7vzunmbWR8q0g4718GjX4Ul1j2vUmMs2RC3kuAhPC4OjuVQ39hKc7w0cWjW1iit8YDTkQXLU1JZU9xKkVS3UhdjDq31zrZ9oyBIsRys0bm/tJuWg9W52NlWWWdI53Er0VTcc0PAylZyLcLnjl3kjDmkZitlWcDPpn0nhHJk+nTGzIfMqq+QbJ30xHKY/wx8+Fun3Ws+MNfYttiJy9jPqWKYDXErCR7C4+JgLIX/OHlfxg0sJeIrTxxauKuEujanQwupUmIdLSb/PpNbyfXHjqqgM08i1a20+GV4/OvpdXGLQ4klDv4Ss/BbqluppDJZjPJOZbU6YttlllfMIU/LIXXU63YroZNFMpZDHBJupdSYQ5ZOMbQreVZ7V/n8Qfji8eRrQeeWw7rpnV+zbYd1W1mrLfZy4x2NzrYtaAnLIZc4tOdXThD2ALwtDpb75qT9BzCwDAZW908cuvSUKQwZ6Lze0qpYsX4zAH/+bAtPfbGVUEeISx6cwQ+fmsOcVZsTZWdtbOHu99aYF6mWw+oPYNW76X/w1u3Oti0O9uJ9iQl5rc5+u9MJlGXIVnJ16G4Lxu74AqXpbrGk90cc6yVvt1Kq5dBmRt527MXd6UazxDTadyV/Ngm3Uo5JcO27zLW6u+ZQ8xYn2yrJcshyvbYG4xb64rHs52xrSH625zCEmlziYF0zb3GwPwcRB2HPZ69YlZW4dQ/pEsedoyprKSuvAOt/HPaV0le3goaIr4zqqgoCDVFCkRhfrN/JN12jyL6VFdz94Tp+UApLNzfw4YeriMQ0Q/uXcezW9dQCsxcvp7l8GPXNHYRjcY5au4Z97ROkioNSppO1O6uSSicrqKSyc8uhpA+0NyRnKwWseEpnC++V9jGdUMtWGLhvwgWXlVRxCLea81TWGqsoW5DbXdcZ0+DD/4MSax5G6iS4WIf5rlLrEnJ1vH1qO69nKpF28357LawkEcsiDg1rjAXZvDX7ee3AfHsDVI9y6tjRlL5IY9gVgNfafN/Z6gqZRaSl3qRfyxLjwh7CXiIOUeMCKHcsBSprTCdqccCIfaB+KbTC1SdPNAHkD2K8cPXR5s/82O/BMhb2HzqQUf4q4s2KtxZs4M45SxPneb5kLbU+uO3pD5mn90vsvze4hn3tPs8Wh9I+Tn38JaaOymeC5W4rIlNA2l9qOtNSWxwsy8EXMGm2/mD2eEg0DH36A9vgsa/B1/4Ak7+XXm7LPNi5Fiack+5WskfH5dVGHLJZDm6B2rrAdLod9ojaFoeUDtsl4kRCzrlDjV0Xh+Ytznsh2XWVzVLZaX3RtlWQCVscEpaDbS00OUvCp7qVtLVybtBxbyaRbYZ0PA73HQHH/gcc84PsdRKE3YjHxcHqjbV1D2l3p1NZY0bXNoEyx8dsZyuB6WADJUl/2ECwhNf//cvwv0G+dehQvnf6aZT4fazd0crov4agBX5z+hBaR02ltqqUsqCf9gd+C1ZfOGdrhEOBVY2aXzz8OQ2tHTwZVVQBHTrAiq3tTIybjisWrMTfviO5XdEOM4psrXNG4VHrns0Bq+OxxSMTtuVgU788c7mP7jBZORPOSbcc7E6xvNq5fuL8HSaNODUoXrco+RypqaxgOkj39+QORNsd/NYF8Nmf4Kt3JH+HmbBH/5FWK1XZLURZAtINtjhsz3w8HjeCDI4otLssh2xuJXt73XTTzgPPSj5vNrdS23ZzPVu0BGEPwOPikOJWCrpuOlNRA1VDnXL+EqcztbOVwAo4l5g/tZ3B5C+hJOADf5CB5T4oM2XHD66CkOlQDqzqgNEDEpfTZW0JcWiMGddAQ6SEuqYQw/qXE98VhBjEfUFKSsvA6rfWNsGwWIhfv7yQyaMHsKGhje+2thD1lVMN7IqV0B94cfYa9o1u5UBfCUEwnWYmy8G+v7Od7grZZ+42rDYjZNuF5MYOxNtrVCVZDh1O3CNxv+sWY4W4sd1KSUFil1AsftnMD7GxhWLG/WZp60EHwtRrM9fdptn9/qZkQchpOezIfDy0y7lFqy2SoQwxh1S3kr390R2mXmnikMWtZAtctvoUmnjMXLPfsN1zPcGTeFsc7PkE8ahlObhGy5U10H+k2dbamfEMTrYSOB1spNX415s3O0Fgt+smFjEdhd35pIw6lStb6YSJo2HedA7ffwRvXHS82XlnJTTtpLysjP2HDABrSf9NbX4G+zp4auYGHvt0HQAXlrazUfeh2gfz66Ic74clG3cQjdZT44eX31/FhSGNv7mFL5bWEfArWkJR/D5FS1uI89C0qTJsqQzt2oLjYMP5TBrWmu2GlBFrSZWTaVSeQRxiYSsTy2W91C8lDbfloPzGwnN32G/fYu4vbWO7a2xr4eO7YMr3jRsuG6niYp/fF8zDcsjSGbvdTakWRD6WQ8s2MxciFnF+Z/G4U59Ut5It3q2diENLHWydD/udkr1Mvix4Fl75AfxoCVQO7Pn5hL0Sb4tDWkDaZTkEy13iEHMsBbAW3kuZxxBug75DLXGwyrozgt64CeY+4ZzDnZ0UizqdCKQHpO1zgXEHuepyxAEjKF2zjPk/O42FmxoZN7iKqt9D9dARsGYVB40ZCusXcNNpY4ms38m25WXc/sZSppRECDfUc9mjM5M+knJCnFcGb61s5VzL67Z18zp2rDP1qywNEI1pysIN7GcJwNa1i9jHdY72QF/KbXGwb5uaGmcIlJq22JbDthSXUvmA5ElwFQNM7CKx0mvErGSqY857Eus72Z1lnRGdoYeQleZUcbDOXzEgt+WQrTN2i0ZatlInqaz2dkudadfOdVBjxaWSrKcUy8Fub2eWw4xp8PGRFT0uAAAbbElEQVTv4SerTdt6wvYVRtS3L4fKqT07l7DX4nFxsHo/+65qqf7p6lHOtt+VBRIsS5/HEGmDPoOSy7onyq37JNkl4hYH+0/dbwQ0bnCyVUoziYNr9jVQVtEXYmFK/T4mj7L+9NEQfmvEPrB6AKwHFYtQosMMr+nPJ5ecRP9n7iUS7uCFs48mGtP0KQ0QjcfpE2+Bh2HS2OFgDBH6x3ZywrRPkz6aw9RyXrA+rsdefZcbghDTCr/SrGwJcrBllC2MDGUisKF+J4H+7dT2KSUaaieggsRVkNamZhq3tzJy2yJ8wUpjse1aB1X7QN1iM2KOtJvX7sB244ZkYQBHHJq3Grdg23bjqupUHFwZR6FG5/zl1Zkth0i7EZRghbGObKFz4+6k21PcSm7LIdJmRNMtDq3bHQtox0pHHGyXUkkfUz7aAe/+Eo7+YXa30q715n21B5htNKz/FMZ/NfvnkQ+2GDWsglEiDkJmPC4Ormwl29XhxrYcINmtZC/ZDdZyC3HzR69MEQd/wFgFkRDUL3O9vzJ50pu9PWCs6fQi7VDaz3RwieuXOM/uetpWRjxq6hSPm7bYq8rax61sJRUsY1j/cqiogM3TOWzV/XDiTc75WswIf+zQQY44qFYe/c4kCJTS2hHD74Pa1evhC3P8grFh2ADh8lrKQ3WU9a2FlrUA/OyTCC+Wwi0vzObduEYpuDuwngkqRgdRNu7YwlW/e5/nKz5hv/774guWUcU6mgID6QvMWb2JL0Xa6Qj2oxxYvrGOuvbNjG5cwHBScFsOI4+CpX9Pj2Ok0rzVxEVCu8z77U64vDrz/RVWvWeex3wZlr9uLIO+Q5LL2J10RY0ZZc/5qyujqsm8x3aThRqTYw7uoPKOlc52wqIZaMRz7ccw/R7zm3FbDu5U2NdvMO2/5lNnKfV10wsnDnK7UqETvC0OfQabjnruE2aE73YdgROQhuRj9vITYHW6VodSaXXmiVG+dUOg+iXJo9xBByaLw5Z55nnooWaZhUgbXP469HN1f7aQVe2TWRxi4WQ3lr24XqDMdESxsHOjIjAuHYAPfmPSVO0OLvH+fkkfxQnDNPQf5Oyo3wkoQDNGmZFr+YDhsLmOfUcOh8Uzifgr+NWFR8FLcO1xwzllwMFsbWzn4BXl9Av1QfvLGFxeye2HTmTsa+t5ve4w+qgQZ/nh7Q3wDT9c8+f3+LQszEcbY5zmhzv+PpsbAz/GpyLm8hbhQBUrV29gdsVaLmnayty+JzHe34+VC+YyR62hNOhne3MH5xwyjJED7TkNYZMhNfQQWP0+zHvK+S7KByS7nGzmPmEGAZO+aYnDjmRxeO0n8PmfzHbNODNSf9kVFG/faQShepTpuEONxhKwxcJ97+wdK5xtWxwqa404bJptlVnlWA622Nhxnu0rjMUQj5u764GxYPNl+VvGKhr75eT99vUauikOjZtg0yyT5VYMNs6CoYeBz9tzdL2Otz/98v5wyq1mVi4kuWvMa5f2uY/ZtwkFa4kE1x8XnM47UGZEYMv85PPW7J/sAljzgRllDp9iXofbYPBByR103WLzPPEbyXWx4xKJG/TYK6/2c+rit7Kpti4wHVZqezbMcLbtGED/kXDV+3ChFSex1wGyqV9mxKu0r9NJ9DVi6rN82sHqERw0wnwmk4dWcMmRI/nRaQcwul+A6qo+DOjXl4GlmgsnlFNNE5OPOIZJ4w8AYMrE8QDcfc4IAMaPNS6+W/dfwxjfNoaqBuKWOjTrcjaHK1i2fhN3vDwDv47w99VxVsdqaNqykltfXcxNLyzgjreX89W7P+L6J+fw42fn8dzTD0P7Tv7UagX9l7+REITPt8YJN27lzw/cxYYdltuneRt6+RvoSReagQUQa3FEPh7XjjC4Po8E5QOMZYg2I34wFku41fnt2OIQrEgemSfEwRqAbJxlnnesSs4ms39X8bgRkVgHNG00gXt/qfktpqYdZ0JrePWH8NbN6cfs38KO1enHwAT5O5tZP/0eeOa7sH1l9jLZeP92ePay7Me3LoQ/nwwLnun6uYWC4m3LAeDwK4xLZscKM5IZcST0H5FervYAZztY7ozAl7zqdNw142DwRGMZAEw4F977lfkzllTBGbfD+unGNdBaDyvegRl/hFX/hIPOc6yATMsj2J3/QeearBMb+z3RDtj0hdMhlfYzHUx5tRGHNR8aH/m408xxd4ey/jOoPRA+vRcOucTs8weNJWPjLt++C5a/CZMuMO4NWxyqrBG0LUz9hjn++NTF/Pyllmi1JILR4yYeCZtLYAWMGrUfLIHDBxixGznMxECGrH05cRpdcwB652oqKwdQVjaQoX0qOPaY/eFxuOGbJ1C2ohW9eQ5fXHEqbeEosbjm168tZd6GXURicU5tf54dvn68EjqUK/DhJ54495pmxRHxEFdsvoXLfh9ipv9QbtAPcbGKc/L7YxgyczlPATc8/j7Ty+I0d0QJdYRYXOoniLESWzYswBU1os4/mEHaxCC2+oewDzBr6Wr23bWTqopaAi1bnUyokVNh8xzHTWS7u2xX40YrkWDHSsuNaLnG2hrMjPbmLc5vZv1nxqo44CyT/rvhc9jvZDpl+wqTXNFanxxXicec2fkNq9NndEc74L4j4fDLzcArE/bvd+4TcMotndcjleWvm8ULY9HkwVvquZe/AV+6qGvnzsaGz6F2vGONC3nhfXHw+WDqNc5re2Rtc96DZtQ1+TKTu77uUyMMY78Mo46F9/7HpMSefAvsfzoccIbz3qnXwuxHYMtc+NIlcOi3zGP+M+aP+8Q3nLJjjnfWM8p034LTf2P++OXVzpLSZf0dcXj9J+aPf5C1qF9pH7jyXRPk/vhO4z7xlxhfOThuiT6DjeWy6p8m+2TtR2a/7XayRshJ4jD/GeNKm3KZ6UQSloMlDvYot+8wR0RDTeYP/cHtRqjGHG9cKwueM38+MNZSn8FGbMefaUat9mqp7gybKZfD/Gfw144DHUUFyvBV9IdIM7WYOEFZ9VCoHo1a8goDynwMKC+Fhc9z/wVnmLvrtTWgfzcHjriSf5x+EtzqCAPAhUftB9PN9u8qnyAUeJl92paxaNA5nLPvseys3wwr4HfqLh6prWHdoJMYFd9IcG6MO/UlvBOeyFHbl/DzoDOBcFZjFWdaORDTFgX4RRAee3ce3w/U0UwfjvEpfFbM4Z5N+3F96J9ce+/zzGmpZipzuQN4anE7F0Ei0B1vWAMoGmsOoTo0k1Xr1qIXz6KqaQXWN8emuW8zDKgfeSY1S/6OWjc9tzisse6PEY8Y8R52mHndut38D2rHm0yw5i3JFlLdEjPDfe7f4MSfpnfgWpvRPcC8J+Gkn+ZemgXMby5QZiZkxjqMMNWMSxemuiVme9V72QWkK2yeAw+dCsf/BE7KYEV1ldbt5vcXKHXqag8m9zK8Lw65mHSBs334FeYBplO+9BXTsdWMc8x9NyUV8L1/mBHdEFfGzMHfNB312o9g/Fkw6yE48GvOvIvJGczmo652tocfYXzk5z/qZMMsftm4Jha9aFxC+5/udKh2Bz36WCcDauRU0wEc+m0z6Ur5jMtq4fPmuO0aq6wFFMx62AjG6GNMSuSwycaysP3bgXIYcwIMecVc+7P74eDzjSuldrxp44bPku9hcOh3Yfaj5nz9R5rPsLIGznvAlDno6yanHqB6tNP+U28zllafweY6cSsVeONscz4wxwaMMcd2rjHX/edtcMwPzfsXvYiKR9JHl+UDTLaQPQIdfRwD134EAw+FcRcz6ZRbmdSnFmL7wi9NkcvCf4OvXQdLVsJcuPLSy5mqx1ISUCz0/5Kad3/MPquf49Sjj4DPjBCe/c3L4KXHuPm4vvRb6mONbyAdu8oo1+00+/qxq2YybIT9o8so2/csJjYGYRNsqz0aNpnvaG58LIf4jGvn2S2DuCoAf3n9I24IPEWFciy10MoPwQeXvNzIHSWjYfrr3Lb8ZMqCfkKRGG3hGHGtzfxHrdne0sE96lkmq3IqdDvL5nzE7A0D2RWKMEGt4wRgcdUxTKhfyoz3XmVZ7VdoDUcpDfg5eNv7HAHQso3IyneJL3+HjvIaIkf9kOqKEqIN6yjpaISxJ8Lq94xLd9ypZuDz2o/h1F8aizPUZDrRFW+ZAc7rPzGiZFvVL11trJprPzOW/NpP4PFzzYAEzH/uqYvhpJ/BkElkZc4T8PkDcPkb6cuWaA1v/cxsr/80/b2pbJgJn9wFZ9+TOV24aTPcd5QZIJ76S3j8PNOeaz5LT2rIREud+T9mW3trD0PpfO8xXGCmTJmiZ82a1SvX3uOoX2YeI46Ed2+DqdfDoPHO8dXvm8Dkfqc4o7yOZuMeKu1j7j439FDjB1/6d2MdnXiT+XOC8RF//qDxYYMJzl/xjgnkvng1zPsbnDvNcUmBda9t68+2/E342wVGgI74N/hsmumEf7Ia/nQ8bFsI33kpPfC5bRH85Vw47Ltw4n+bP+jgiclrYNm8ebNxi9n892bzZ/rjUUa865YC2sRITv0FfPIHI4BXTzd/tlstV9iNG4w7bv7T8PI1cON6E0TuPyr9T7nsdVjzEcy4Dy74i/mcZz1sru1e4iMaho2fm87hhSuNEP5wvvGNN202caIDzjTfQ8tWGDQB/u0j+M0IOPQ7cOb/mZHzC1fC9V9Ayzb0S1ezaeK1DP/I3ORo4cWfM/HJI2jufyBVu8yIVCsf4bIaStuNG+iF02YwdN4fmLztOZ7uexmzAl/ix02381L15cyvOo4R4VXsE9lA31LFN9f/ireqzuX05hdMU+PD+VHkGkaoOu4vuYvzO37OgyV3slwPp5Qw/xm5mlV6GLcFHuHr/o+J4mdufF+O9S2kkUqO6Pgjfr+fU9RMpgXu5FuxW7jHdycz9EH8mP/ghqo3uLT1EV6puoj9w4sY37GAd6rO5YTmV2n29aU6vjPjT/+TA25iy/7f5uB5t3HA+qcB2DH0ywSbN1HZso6Ng77Mp5N/T4AoYwb1Y2zLLKLaR+uQo/H7FPs8fQbBbXPZcdwviYw4mj7LnkP5g5St+DvR8edQ+unvzUCjo9n8NmxLZOafjYAd9yMT3wk3w7PfM2J30Hlw/sPJvxet4alLYNlr0G8kfOV/4JnvmGPjvgKXPN15p79zLdx7OHzlf+GIK7OXywOl1Gyt9ZQenSSf6+QjDkqp04E/AH7gz1rr32Qp9w3gOeBwrXWnPb+Iw25Ga+OKWvSisYImfdPsb95q3FFjju/8vbMfgX0mmaD7gueMSI062rilmjbB2BN6Vr9Y1AR7N88xpvuRV5n9M+6HN24w5598GTx7qdlf1h/OvtvJmLHF4dbG5HrnGqW1NcDv9nfmswQr4OYMWU5grJpXfwgTz4fzHzIi/Mjp5tjhV8LSfxg//36nwrefg0e+ajqF/U42Aei6RWZWsi3woSbj3z/5Z/Cli+FXg43LpXKQlUU1zIh/3WITD/rPpeY7fO5yc157JrsvaCzT6fcA1v+5/0j4f58QfuZySla/Tay0Gn+H00HXXf4ZfT/5X8qWmRhQ7ICzCA+ZTMmMe2ioGMOW0jFM2vJcovzrUx5iXbSak9f9nv12fsxvD/snJ276E4fVPc+9E57kwqU/YJ/YFqL4CRBjo38Yw2ObMn6MdbqaQWonIR2kjVJejU3lFP8XDFMmGP/byAXcFzuXmwN/5VL/m1wauZE7gtNo1JXspzYTxc9r8SMZwg6O9i8mrP3ErOv6ieNTmhZdRh8VYj2D+WvJRfx3+A/8oN8f2Fg6joGqiXu3fZdSwtxe9gNOjnzIhNhSKgixQQ1lhN7MfaPuYmdwCDuD+1Cm25i68xXO2jaN9eXjGdm+lO0lwwjoCO/3/wbn1k/j04NvY+2I5Pu8BCON7Lv2adYNOplRG1/l0HUPsatqHM8d/jQTh/fnqLHdm52+x4iDUsoPLAdOBTYCM4GLtdaLU8pVAf8ASoDrRByEgqC18e3Wjjcd/ayHTPB91NHJHf+m2WY+yuhjun6NOX81KaRbF5hMtON+lLlcSx28+G9w9r3OukTznzX3hTjqGuMCa1hlxKP/CCMm/7R8V9Wjjd//9Nuzp2i+foOJ50y53Pjc/QFTn+n3Gvfm8MlOWTvl9ss3mHuMbJhhxPvcaSZGNvxwk4TRvtOZx7PqXePeiYbg5m3GBfnyNVA9Jnl+xuFXwqQL4aFTTPJFR7OxIkONJmX36OuN9bZrPUw7xlhwbdthn4PNZzjoIPj6NGNVjjjK7NvnYNi+DO0LGvfoklfZdvYTDPzibgIbpqN0nNahR1O5eTorT34QDjiT6pYVDPzLCQBEKwcTD4doLR1ERcc2SiNNxPHhI87nh9/F0E1v0FI+lCWjLyUaj1PWvo0T5/yA54f8iFW+0dy2+iJmVhzH4MhG+sQaGRBvYEfJUAaGNxNH0VAyjP6Rbdw64iF+tP56AAbQyMe+KRwen0cpET70Hc7v/FfySsQMXH6mruO9khO5K/RTJrCa/xf5d1bFh3Ky/wuuC7xMve7HQb51xLUiTIAofvqoEPdEz6X/QafynYu/3fXfKnuWOEwFbtVaf8V6fROA1vrXKeXuAt4G/gv4sYiDIPSAWNQSP2WeUy2gWMRkmo053mQgzX/KZLJV7ZPxdAkiIdOR9xtu3GVLXoFRx5i5HBPOMZPspl5rOvMHTzJiG2o0ls8h34IJZydPLl34glmn6ch/MzGqaUfDWXfB5EuNhTnkEGNZVtYYV1+4DY64ygSkD7CsrjlPwKf3wXdeMOJ40HnJ7p9oh5MN6A+abKe2HSapYtvi5EmgbmzLUWv424Ww4k0YsK853/ApZin7NR9A3+EweIKxogeMMbeHffdXxk3bsBpGHweHf9/E4oLl8Mylptwpt5qPtHEL6qFTCTRtSFw6XjYAX6iB5hN+ib99O2WLn6Xl5F/T57Xr8YWb0FOvQ33lf/L+ObjZk8ThfOB0rfUV1uvvAEdqra9zlTkMuFlr/Q2l1PtkEQel1FXAVQAjR46cvG7duoI1RBCEXsLtvtu13gSg98Sga6jJuA19flPnbBZcJGRidwd+zWRmDZmUPocqleatJmbVut3EoU76qQm4u5fwAbNSsY4Zl2A3J/ntLnHocbaSUsoH3Al8L1dZrfUDwANgLIeeXlsQhD0AtxC4rYo9Dfc8h87EK1hmrCBIduV1RtU+6ZlzJaPSy7nXW9vDyUe6NgHuWWXDrX02VcBE4H2l1FrgKOAVpVTRlU0QBEEoDvmIw0xgnFJqjFKqBLgIeMU+qLVu1FrXaK1Ha61HAzOAs3PFHARBEIQ9l5zioLWOAtcBbwJLgGe01ouUUrcppc4udgUFQRCE3U9eMQet9WvAayn7fp6l7Ak9r5YgCILQm3h7VVZBEAShKIg4CIIgCGmIOAiCIAhpiDgIgiAIafTaqqxKqXoSdznuEjXA9gJXp7eRNnkDaZM32BvbBE67Rmmta4t9sV4Th+6ilJq1O6aO706kTd5A2uQN9sY2we5vl7iVBEEQhDREHARBEIQ0vCgOD/R2BYqAtMkbSJu8wd7YJtjN7fJczEEQBEEoPl60HARBEIQi4ylxUEqdrpRappRaqZS6sbfrA6CUWquUWqCUmquUmmXtG6CUelsptcJ6rrb2K6XU3Vb951s3SbLPc6lVfoVS6lLX/snW+Vda71WdXaMH7XhYKVWnlFro2tdr7ejsGj1s061KqU3W9zVXKXWm69hN1vWWKaW+4tqf8XdnrVT8mbX/aWvVYpRSpdbrldbx0bmukWd7Riil3lNKLVZKLVJK/bC7n+Ge8j110iYvf09lSqnPlVLzrDb9otD1KGRbs6K19sQD8AOrgLGY+1TPAybsAfVaC9Sk7Ps/4EZr+0bgdmv7TOB1QGHue/GZtX8AsNp6rra2q61jn1tllfXeMzq7Rg/acTxwGLBwT2hHtmsUoE23Yu5UmFp2gvWbKgXGWL81f2e/O+AZ4CJr+37gamv7GuB+a/si4OnOrtGF9gwBDrO2qzD3dp/g5e+pkzZ5+XtSQB9rOwh8Zn02BalHIdvaaTt60qHszgcwFXjT9fom4KY9oF5rSReHZcAQ149/mbX9J+Di1HLAxcCfXPv/ZO0bAix17U+Uy3aNHrZlNMkdaa+1I9s1CtCmW8nc6ST9njBL1E/N9rvDdADbgUDq79N+r7UdsMqpbNfowff1MnDq3vA9ZWjTXvE9ARXAF8CRhapHIdvaWd295FYaBmxwvd5o7ettNPCWUmq2MvfIBhistd5ibW8FBlvb2drQ2f6NGfZ3do1C0pvtKOb3fZ3lAnlYOe64rrZpILBLm/udpNYv8R7reKNVvmBtstwCh2JGpXvF95TSJvDw96SU8iul5gJ1wNuYkX6h6lHItmbFS+Kwp3Ks1vow4AzgWqXU8e6D2kh1UVPC5BpdYhqwL3AIsAW4o8jXKzhKqT7A88C/a62b3Me8+j1laJOnvyetdUxrfQjmtspHAON7uUpdxkvikOte1r2C1nqT9VwHvIj5IWxTSg0BsJ7rrOLZ2tDZ/uEZ9tPJNQpJb7ajKN+31nqb9ceNAw9ivq/utGkH0F8pFUjZn3Qu63g/q3yP26SUCmI60Se01i9Yuz39PWVqk9e/Jxut9S7gPYyLp1D1KGRbs+Ilcej0Xta9gVKqUilVZW8DpwELrXrZGSCXYvyoWPu/a2V4HAU0Wqb6m8BpSqlqy3w+DeMr3AI0KaWOUkop4Lsp58p0jULSm+3Ido0eYXdwFl/HfF/29S6ysjrGAOMwwdmMvztr9PwecH6WutttOh941yqf7Rr51l0BDwFLtNZ3ug559nvK1iaPf0+1Sqn+1nY5JoaypID1KGRbs9PdwFFvPDCZEcsx/rub94D6jMVkCswDFtl1wvjy/gmsAN4BBlj7FXCfVf8FwBTXuS4HVlqPy1z7p2D+GKuAe3EmLma8Rg/a8iTGfI9gfJXf7812dHaNHrbpcet8860/zBBX+Zut6y3DytLp7Hdnff+fW219Fii19pdZr1dax8fmukae7TkW486ZD8y1Hmd6+XvqpE1e/p4mAXOsui8Efl7oehSyrdkeMkNaEARBSMNLbiVBEARhNyHiIAiCIKQh4iAIgiCkIeIgCIIgpCHiIAiCIKQh4iAIFkqp6dbzaKXUJb1dH0HoTUQcBMFCa320tTkaEHEQ/qURcRAEC6VUi7X5G+A4Ze4j8B+9WSdB6C1kEpwgWCilWrTWfZRSJ2CWiz6rt+skCL2FWA6CIAhCGiIOgiAIQhoiDoKQTjPmlpWC8C+LiIMgpDMfiClzg3gJSAv/kkhAWhAEQUhDLAdBEAQhDREHQRAEIQ0RB0EQBCENEQdBEAQhDREHQRAEIQ0RB0EQBCENEQdBEAQhDREHQRAEIY3/Dz/NlqOVAVooAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import timeit\n",
    "from utils import dataset_helper\n",
    "from utils import custom_scores\n",
    "custom_scores.evalute_multiclass(Y_val.argmax(axis=-1), Y_val_.argmax(axis=-1))\n",
    "iteraction_log.index = iteraction_log.it\n",
    "iteraction_log.error_train.plot()\n",
    "iteraction_log.error_val.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "nclasses = 10\n",
    "nsamples = 60000\n",
    "nfeatures = 784\n",
    "eps = np.finfo(np.float32).eps\n",
    "X,  X_val, Y, Y_val = dataset_helper.get_toy_data_multiclass(nclasses, nsamples, nfeatures)\n",
    "\n",
    "Y = dataset_helper.one_hot_encode(Y, nclasses)\n",
    "Y_val = dataset_helper.one_hot_encode(Y_val, nclasses)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary\n",
      "-------------------------------\n",
      "H1      (input=784, neurons=64, activation=sigmoid)\n",
      "H2      (input=64, neurons=64, activation=sigmoid)\n",
      "H3      (input=64, neurons=24, activation=sigmoid)\n",
      "Output  (input=24, neurons=10, activation=sigmoid)\n",
      "-------------------------------\n",
      "\n",
      "Shuffled\n",
      "It: 10000 Batch: 10000 Epoch 0 Train Loss: 3.41618438 lr: 0.900000 Val Loss: 2.48568869\n",
      "It: 20000 Batch: 20000 Epoch 0 Train Loss: 1.91531834 lr: 0.900000 Val Loss: 1.67308536\n",
      "It: 30000 Batch: 30000 Epoch 0 Train Loss: 1.51279390 lr: 0.900000 Val Loss: 1.34227662\n",
      "It: 40000 Batch: 40000 Epoch 0 Train Loss: 1.18079804 lr: 0.900000 Val Loss: 0.86741301\n",
      "It: 50000 Batch: 2000 Epoch 1 Train Loss: 0.88554753 lr: 0.900000 Val Loss: 0.86815124\n",
      "It: 60000 Batch: 12000 Epoch 1 Train Loss: 0.88511472 lr: 0.900000 Val Loss: 0.86835478\n",
      "It: 70000 Batch: 22000 Epoch 1 Train Loss: 0.88346240 lr: 0.900000 Val Loss: 0.86759042\n",
      "It: 80000 Batch: 32000 Epoch 1 Train Loss: 0.56910750 lr: 0.900000 Val Loss: 0.51762576\n",
      "It: 90000 Batch: 42000 Epoch 1 Train Loss: 0.48551877 lr: 0.900000 Val Loss: 0.60640054\n",
      "It: 100000 Batch: 4000 Epoch 2 Train Loss: 0.48358990 lr: 0.810000 Val Loss: 0.47199104\n",
      "It: 110000 Batch: 14000 Epoch 2 Train Loss: 0.48122934 lr: 0.810000 Val Loss: 0.45978472\n",
      "It: 120000 Batch: 24000 Epoch 2 Train Loss: 0.48104607 lr: 0.810000 Val Loss: 0.46201458\n",
      "It: 130000 Batch: 34000 Epoch 2 Train Loss: 0.48102872 lr: 0.810000 Val Loss: 0.58554312\n",
      "It: 140000 Batch: 44000 Epoch 2 Train Loss: 0.48079526 lr: 0.810000 Val Loss: 0.46024875\n",
      "It: 150000 Batch: 6000 Epoch 3 Train Loss: 0.48176545 lr: 0.810000 Val Loss: 0.46057347\n",
      "It: 160000 Batch: 16000 Epoch 3 Train Loss: 0.48103512 lr: 0.810000 Val Loss: 0.46316781\n",
      "It: 170000 Batch: 26000 Epoch 3 Train Loss: 0.48122250 lr: 0.810000 Val Loss: 0.46208770\n",
      "It: 180000 Batch: 36000 Epoch 3 Train Loss: 0.48083981 lr: 0.810000 Val Loss: 0.52544094\n",
      "It: 190000 Batch: 46000 Epoch 3 Train Loss: 0.48094134 lr: 0.810000 Val Loss: 0.46451414\n",
      "It: 200000 Batch: 8000 Epoch 4 Train Loss: 0.47814886 lr: 0.729000 Val Loss: 0.46110046\n",
      "It: 210000 Batch: 18000 Epoch 4 Train Loss: 0.47739452 lr: 0.729000 Val Loss: 0.46817075\n",
      "It: 220000 Batch: 28000 Epoch 4 Train Loss: 0.47751882 lr: 0.729000 Val Loss: 0.47300781\n",
      "It: 230000 Batch: 38000 Epoch 4 Train Loss: 0.47753250 lr: 0.729000 Val Loss: 0.76673948\n",
      "It: 240000 Batch: 48000 Epoch 4 Train Loss: 0.47707606 lr: 0.729000 Val Loss: 0.48581925\n",
      "It: 250000 Batch: 10000 Epoch 5 Train Loss: 0.47743532 lr: 0.729000 Val Loss: 0.45968252\n",
      "It: 260000 Batch: 20000 Epoch 5 Train Loss: 0.47693334 lr: 0.729000 Val Loss: 0.46684686\n",
      "It: 270000 Batch: 30000 Epoch 5 Train Loss: 0.47741594 lr: 0.729000 Val Loss: 0.78374681\n",
      "It: 280000 Batch: 40000 Epoch 5 Train Loss: 0.47725823 lr: 0.729000 Val Loss: 0.46155424\n",
      "It: 290000 Batch: 2000 Epoch 6 Train Loss: 0.47658756 lr: 0.656100 Val Loss: 0.45815339\n",
      "It: 300000 Batch: 12000 Epoch 6 Train Loss: 0.47461598 lr: 0.656100 Val Loss: 0.46277969\n",
      "It: 310000 Batch: 22000 Epoch 6 Train Loss: 0.47396266 lr: 0.656100 Val Loss: 0.46680059\n",
      "It: 320000 Batch: 32000 Epoch 6 Train Loss: 0.47379142 lr: 0.656100 Val Loss: 0.45938858\n",
      "It: 330000 Batch: 42000 Epoch 6 Train Loss: 0.47435190 lr: 0.656100 Val Loss: 0.45900844\n",
      "It: 340000 Batch: 4000 Epoch 7 Train Loss: 0.47412785 lr: 0.656100 Val Loss: 0.46328424\n",
      "It: 350000 Batch: 14000 Epoch 7 Train Loss: 0.47428898 lr: 0.656100 Val Loss: 0.48289026\n",
      "It: 360000 Batch: 24000 Epoch 7 Train Loss: 0.47431236 lr: 0.656100 Val Loss: 0.45908402\n",
      "It: 370000 Batch: 34000 Epoch 7 Train Loss: 0.47433788 lr: 0.656100 Val Loss: 0.46001868\n",
      "It: 380000 Batch: 44000 Epoch 7 Train Loss: 0.47412129 lr: 0.656100 Val Loss: 0.45842282\n",
      "It: 390000 Batch: 6000 Epoch 8 Train Loss: 0.47283879 lr: 0.590490 Val Loss: 0.47216969\n",
      "It: 400000 Batch: 16000 Epoch 8 Train Loss: 0.47176320 lr: 0.590490 Val Loss: 0.45935103\n",
      "It: 410000 Batch: 26000 Epoch 8 Train Loss: 0.47154713 lr: 0.590490 Val Loss: 0.56137181\n",
      "It: 420000 Batch: 36000 Epoch 8 Train Loss: 0.47157157 lr: 0.590490 Val Loss: 0.45832013\n",
      "It: 430000 Batch: 46000 Epoch 8 Train Loss: 0.47161380 lr: 0.590490 Val Loss: 0.45786533\n",
      "It: 440000 Batch: 8000 Epoch 9 Train Loss: 0.47167369 lr: 0.590490 Val Loss: 0.46141281\n",
      "It: 450000 Batch: 18000 Epoch 9 Train Loss: 0.47175863 lr: 0.590490 Val Loss: 0.48158949\n",
      "It: 460000 Batch: 28000 Epoch 9 Train Loss: 0.47176634 lr: 0.590490 Val Loss: 0.45802654\n",
      "It: 470000 Batch: 38000 Epoch 9 Train Loss: 0.47152720 lr: 0.590490 Val Loss: 0.45749200\n",
      "It: 480000 Batch: 48000 Epoch 9 Train Loss: 0.47175893 lr: 0.590490 Val Loss: 0.49036018\n",
      "It: 490000 Batch: 10000 Epoch 10 Train Loss: 0.46923176 lr: 0.531441 Val Loss: 0.52285058\n",
      "It: 500000 Batch: 20000 Epoch 10 Train Loss: 0.46938429 lr: 0.531441 Val Loss: 0.46055177\n",
      "It: 510000 Batch: 30000 Epoch 10 Train Loss: 0.46945300 lr: 0.531441 Val Loss: 0.45884932\n",
      "It: 520000 Batch: 40000 Epoch 10 Train Loss: 0.46925783 lr: 0.531441 Val Loss: 0.51899756\n",
      "It: 530000 Batch: 2000 Epoch 11 Train Loss: 0.46943829 lr: 0.531441 Val Loss: 0.45716420\n",
      "It: 540000 Batch: 12000 Epoch 11 Train Loss: 0.46921272 lr: 0.531441 Val Loss: 0.45803102\n",
      "It: 550000 Batch: 22000 Epoch 11 Train Loss: 0.46931571 lr: 0.531441 Val Loss: 0.72502832\n",
      "It: 560000 Batch: 32000 Epoch 11 Train Loss: 0.46937170 lr: 0.531441 Val Loss: 0.46239398\n",
      "It: 570000 Batch: 42000 Epoch 11 Train Loss: 0.46919670 lr: 0.531441 Val Loss: 0.45980795\n",
      "It: 580000 Batch: 4000 Epoch 12 Train Loss: 0.46862492 lr: 0.478297 Val Loss: 0.48769600\n",
      "It: 590000 Batch: 14000 Epoch 12 Train Loss: 0.46734341 lr: 0.478297 Val Loss: 0.45924384\n",
      "It: 600000 Batch: 24000 Epoch 12 Train Loss: 0.46721951 lr: 0.478297 Val Loss: 0.46597704\n",
      "It: 610000 Batch: 34000 Epoch 12 Train Loss: 0.46739225 lr: 0.478297 Val Loss: 0.46083322\n",
      "It: 620000 Batch: 44000 Epoch 12 Train Loss: 0.46730554 lr: 0.478297 Val Loss: 0.45697577\n",
      "It: 630000 Batch: 6000 Epoch 13 Train Loss: 0.46714807 lr: 0.478297 Val Loss: 0.45939869\n",
      "It: 640000 Batch: 16000 Epoch 13 Train Loss: 0.46739946 lr: 0.478297 Val Loss: 0.55912606\n",
      "It: 650000 Batch: 26000 Epoch 13 Train Loss: 0.46743457 lr: 0.478297 Val Loss: 0.45702187\n",
      "It: 660000 Batch: 36000 Epoch 13 Train Loss: 0.46743321 lr: 0.478297 Val Loss: 0.45746310\n",
      "It: 670000 Batch: 46000 Epoch 13 Train Loss: 0.46733637 lr: 0.478297 Val Loss: 0.45984246\n",
      "It: 680000 Batch: 8000 Epoch 14 Train Loss: 0.46570597 lr: 0.430467 Val Loss: 0.46117946\n",
      "It: 690000 Batch: 18000 Epoch 14 Train Loss: 0.46560729 lr: 0.430467 Val Loss: 0.46792914\n",
      "It: 700000 Batch: 28000 Epoch 14 Train Loss: 0.46558143 lr: 0.430467 Val Loss: 0.45600891\n",
      "It: 710000 Batch: 38000 Epoch 14 Train Loss: 0.46553801 lr: 0.430467 Val Loss: 0.45905231\n",
      "It: 720000 Batch: 48000 Epoch 14 Train Loss: 0.46569435 lr: 0.430467 Val Loss: 0.45721958\n",
      "It: 730000 Batch: 10000 Epoch 15 Train Loss: 0.46580328 lr: 0.430467 Val Loss: 0.69157615\n",
      "It: 740000 Batch: 20000 Epoch 15 Train Loss: 0.46586433 lr: 0.430467 Val Loss: 0.45619131\n",
      "It: 750000 Batch: 30000 Epoch 15 Train Loss: 0.46550879 lr: 0.430467 Val Loss: 0.46253395\n",
      "It: 760000 Batch: 40000 Epoch 15 Train Loss: 0.46558527 lr: 0.430467 Val Loss: 0.46018933\n",
      "It: 770000 Batch: 2000 Epoch 16 Train Loss: 0.46536909 lr: 0.387420 Val Loss: 0.46091607\n",
      "It: 780000 Batch: 12000 Epoch 16 Train Loss: 0.46432937 lr: 0.387420 Val Loss: 0.45790242\n",
      "It: 790000 Batch: 22000 Epoch 16 Train Loss: 0.46425209 lr: 0.387420 Val Loss: 0.45617692\n",
      "It: 800000 Batch: 32000 Epoch 16 Train Loss: 0.46391188 lr: 0.387420 Val Loss: 0.45689158\n",
      "It: 810000 Batch: 42000 Epoch 16 Train Loss: 0.46390096 lr: 0.387420 Val Loss: 0.45926724\n",
      "It: 820000 Batch: 4000 Epoch 17 Train Loss: 0.46399938 lr: 0.387420 Val Loss: 0.46011771\n",
      "It: 830000 Batch: 14000 Epoch 17 Train Loss: 0.46385274 lr: 0.387420 Val Loss: 0.45546045\n",
      "It: 840000 Batch: 24000 Epoch 17 Train Loss: 0.46377655 lr: 0.387420 Val Loss: 0.45524713\n",
      "It: 850000 Batch: 34000 Epoch 17 Train Loss: 0.46400507 lr: 0.387420 Val Loss: 0.45978261\n",
      "It: 860000 Batch: 44000 Epoch 17 Train Loss: 0.46409258 lr: 0.387420 Val Loss: 0.45531825\n",
      "It: 870000 Batch: 6000 Epoch 18 Train Loss: 0.46303000 lr: 0.348678 Val Loss: 0.47907105\n",
      "It: 880000 Batch: 16000 Epoch 18 Train Loss: 0.46257545 lr: 0.348678 Val Loss: 0.49727593\n",
      "It: 890000 Batch: 26000 Epoch 18 Train Loss: 0.46276495 lr: 0.348678 Val Loss: 0.45881267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 900000 Batch: 36000 Epoch 18 Train Loss: 0.46236769 lr: 0.348678 Val Loss: 0.48196442\n",
      "It: 910000 Batch: 46000 Epoch 18 Train Loss: 0.46254630 lr: 0.348678 Val Loss: 0.45690293\n",
      "It: 920000 Batch: 8000 Epoch 19 Train Loss: 0.46264195 lr: 0.348678 Val Loss: 0.47622252\n",
      "It: 930000 Batch: 18000 Epoch 19 Train Loss: 0.46241354 lr: 0.348678 Val Loss: 0.46047159\n",
      "It: 940000 Batch: 28000 Epoch 19 Train Loss: 0.46281448 lr: 0.348678 Val Loss: 0.45577379\n",
      "It: 950000 Batch: 38000 Epoch 19 Train Loss: 0.46282657 lr: 0.348678 Val Loss: 0.45571953\n",
      "It: 960000 Batch: 48000 Epoch 19 Train Loss: 0.46264430 lr: 0.348678 Val Loss: 0.45527840\n",
      "It: 970000 Batch: 10000 Epoch 20 Train Loss: 0.46138038 lr: 0.313811 Val Loss: 0.45568523\n",
      "It: 980000 Batch: 20000 Epoch 20 Train Loss: 0.46149705 lr: 0.313811 Val Loss: 0.45538420\n",
      "It: 990000 Batch: 30000 Epoch 20 Train Loss: 0.46130968 lr: 0.313811 Val Loss: 0.45875608\n",
      "It: 1000000 Batch: 40000 Epoch 20 Train Loss: 0.46151363 lr: 0.313811 Val Loss: 0.46055580\n",
      "It: 1010000 Batch: 2000 Epoch 21 Train Loss: 0.46153935 lr: 0.313811 Val Loss: 0.45776036\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import timeit\n",
    "from utils import dataset_helper\n",
    "from utils import custom_scores\n",
    "reload(custom_scores)\n",
    "reload(dataset_helper)\n",
    "reload(loss_functions)\n",
    "reload(activation_functions)\n",
    "reload(network)\n",
    "reload(dataset_helper)\n",
    "\n",
    "\n",
    "\n",
    "eps = np.finfo(np.float128).eps\n",
    "lr = .9\n",
    "max_iter = 40 * X.shape[0]\n",
    "print_interval = 10*1000\n",
    "\n",
    "network.DEBUG = False\n",
    "\n",
    "eps = np.finfo(np.float32).eps\n",
    "\n",
    "# h1 = network.Layer(nfeatures, 128, 'sigmoid',label=\"H1\")\n",
    "# h2 = network.Layer(128, 256, 'sigmoid',label=\"H2\")\n",
    "# h3 = network.Layer(256, 32, 'sigmoid', label=\"H3\")\n",
    "# o = network.Layer(32, nclasses, 'sigmoid', label=\"Output\")\n",
    "h1 = network.Layer(nfeatures, 64, 'sigmoid', label=\"H1\")\n",
    "h2 = network.Layer(64, 64, 'sigmoid', label=\"H2\")\n",
    "h3 = network.Layer(64, 24, 'sigmoid', label=\"H3\")\n",
    "o = network.Layer(24, nclasses, 'sigmoid', label=\"Output\")\n",
    "\n",
    "model = network.NN(loss='smd')\n",
    "model.add_layer(h1)\n",
    "model.add_layer(h2)\n",
    "model.add_layer(h3)\n",
    "model.add_layer(o)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "start = time.process_time()\n",
    "model.fit(X, Y, max_iter=max_iter, \n",
    "          lr=lr, epsilon=eps, b_sz = 1,\n",
    "          decay_iteractions=2. * X.shape[0], decay_rate = 0.9,\n",
    "          X_val=X_val, Y_val=Y_val,\n",
    "          print_interval=print_interval)\n",
    "\n",
    "Y_ = np.array(model.predict(X))\n",
    "Y_ = Y_.argmax(axis=-1).flatten()\n",
    "Y = Y.argmax(axis=-1)\n",
    "mae = np.absolute(Y - Y_).mean()\n",
    "\n",
    "print(\"Time Spent \", time.process_time() - start)\n",
    "\n",
    "\n",
    "Y_val_ = np.array(model.predict(X_val))\n",
    "iteraction_log = network.get_iteration_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import timeit\n",
    "from utils import dataset_helper\n",
    "from utils import custom_scores\n",
    "custom_scores.evalute_multiclass(Y_val.argmax(axis=-1), Y_val_.argmax(axis=-1))\n",
    "iteraction_log.index = iteraction_log.it\n",
    "iteraction_log.error_train.plot()\n",
    "iteraction_log.error_val.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
