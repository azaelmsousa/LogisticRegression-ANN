{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows the development of an Artificial Neural Network with focus on classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.datasets as sk_datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from SGD import custom_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function:\n",
    "\n",
    "\t#\n",
    "\t# Sigmoid function for the activation\n",
    "\t# of a neuron, where h is the dot\n",
    "\t# product of X (input) and theta (weights)\n",
    "\t#\n",
    "\tdef act_Sigmoid(h):\n",
    "\t\tsig = 1. / (1. + np.exp(-h))\n",
    "\t\treturn sig\n",
    "\n",
    "\t#\n",
    "\t# Derivative of the sigmoid function. It\n",
    "\t# is used as part of the backpropagation\n",
    "\t# algorithm\n",
    "\t#\n",
    "\tdef act_Sigmoid_derivative(h):\n",
    "\t\tsig = 1. / (1. + np.exp(-h))\n",
    "\t\tderivative = sig*(1-sig)\n",
    "\t\treturn derivative\n",
    "\n",
    "\t#\n",
    "\t# Hyberbolic tangent function for the\n",
    "\t# acivation of a neuron, where h is the\n",
    "\t# dot product of X (input) and theta (weights)\n",
    "\t#\t\n",
    "\tdef act_Tanh(h):\n",
    "\t\ttanh = (2 / (1+np.exp(-2*h)))-1\n",
    "\n",
    "\t#\n",
    "\t# Deivative of the hyperbolic tangent\n",
    "\t# function. It is used as part of the\t\n",
    "\t# back propagation algorithm.\n",
    "\t#\n",
    "\tdef act_Tanh_derivative(h):\n",
    "\t\ttanh_l = (4*np.exp(-2*h))/((1+np.exp(-2*h))**2)\n",
    "\t\treturn tanh_l\n",
    "\n",
    "\t#\n",
    "\t# Cross entropy loss function, where h\n",
    "\t# is the activation of the last layer.\n",
    "\t# It computes the error of the predicted\n",
    "\t# class and the correct one.\n",
    "\t#\n",
    "\tdef err_CrossEntropy(predY,y):\n",
    "\t\teps = np.finfo(np.float128).eps\n",
    "\t\tpredY[predY < eps] = eps\n",
    "\t\tpredY[predY > 1.-eps] = 1.-eps\n",
    "\t\treturn -np.multiply(np.log(predY),y) - np.multiply((np.log(1-predY)),(1-y))\n",
    "\n",
    "\t#\n",
    "\t# Derivative of the SMD loss function.\n",
    "\t# It is used in the back propagation\n",
    "\t# algorithm.\n",
    "\t#\n",
    "\tdef err_CrossEntropy_derivative(X,predY,y):\n",
    "\t\terror = (predY - y)\n",
    "\t\tgrad = np.dot(X.transpose(),error)\n",
    "\t\treturn grad\n",
    "\n",
    "\t#\n",
    "\t# Sum of the squared differences (SMD) loss\n",
    "\t# function, where h is the activation of the\n",
    "\t# last layer. It computes the error of the\n",
    "\t# predicted class and the correct one.\n",
    "\t#\n",
    "\tdef err_SME(predY,y):\n",
    "\t\terror = np.square((predY - y)).sum(axis=1)\n",
    "\t\treturn error/(2*y.shape[1])\n",
    "\n",
    "\t#\n",
    "\t# Sum of the squared differences (SMD) loss\n",
    "\t# function, where h is the activation of the\n",
    "\t# last layer. It computes the error of the\n",
    "\t# predicted class and the correct one.\n",
    "\t#\n",
    "\tdef err_SME_derivative(predY,y):#X\n",
    "\t\tgrad = (predY - y)\n",
    "\t\t#error = (predY - y)\n",
    "\t\t#grad = np.dot(X.transpose(),error)\n",
    "\t\treturn grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "\n",
    "\tdef __init__(self, activation, error):\n",
    "\t\tself.act_func = Function.__dict__[activation]\n",
    "\t\tself.act_derivative = Function.__dict__[activation+\"_derivative\"]\n",
    "\t\tself.err_func = Function.__dict__[error]\n",
    "\t\tself.err_derivative = Function.__dict__[error+\"_derivative\"]\n",
    "\t\tself.activations = []\n",
    "\t\tself.dot_product = []\t\t\n",
    "\n",
    "\tdef initialize_random_weights(self, n_input, n_perceptron, n_classes):\n",
    "\t\tself.n_hidden_layers = len(n_perceptron)\n",
    "\n",
    "\t\tself.hidden_layers = []\t\t\n",
    "\n",
    "\t\tfor l in range(self.n_hidden_layers):\n",
    "\t\t\tif (l == 0):\n",
    "\t\t\t\tnp.random.seed(42)\n",
    "\t\t\t\tw = (np.random.rand(n_input+1,n_perceptron[l]))*0.001\n",
    "\t\t\telse:\n",
    "\t\t\t\tnp.random.seed(42)\n",
    "\t\t\t\tw = (np.random.rand(n_perceptron[l-1]+1,n_perceptron[l]))*0.001\n",
    "\t\t\tself.hidden_layers.append(w)\n",
    "\t\t\t\n",
    "\n",
    "\t\tif (self.n_hidden_layers == 0):\n",
    "\t\t\tnp.random.seed(42)\n",
    "\t\t\tself.output_layer = (np.random.rand(n_input+1,n_classes))*0.001\n",
    "\t\telse:\n",
    "\t\t\tnp.random.seed(42)\n",
    "\t\t\tself.output_layer = (np.random.rand(n_perceptron[self.n_hidden_layers-1]+1,n_classes))*0.001\n",
    "\n",
    "\n",
    "\tdef initialize_fixed_weights(self, w):\n",
    "\t\tself.hidden_layers = w[:-1]\n",
    "\t\tself.output_layer = w[-1]\n",
    "\t\tself.n_hidden_layers = len(w)-1\n",
    "\n",
    "\n",
    "\tdef show_weights(self):\n",
    "\n",
    "\t\tfor l in range(self.n_hidden_layers):\n",
    "\t\t\tprint(\"Hidden Layer \",str(l+1))\n",
    "\t\t\tprint(self.hidden_layers[l],\"\\n\")\n",
    "\n",
    "\t\tprint(\"Output Layer \")\n",
    "\t\tprint(self.output_layer,\"\\n\")\n",
    "\n",
    "\tdef show_activations(self):\n",
    "\n",
    "\t\tfor l in range(self.n_hidden_layers):\n",
    "\t\t\tprint(\"Hidden Layer \",str(l+1))\n",
    "\t\t\tprint(self.activations[l],\"\\n\")\n",
    "\n",
    "\t\tprint(\"Output Layer \")\n",
    "\t\tprint(self.activations[self.n_hidden_layers],\"\\n\")\n",
    "\n",
    "\tdef show_setup(self):\n",
    "\t\t\t\t\n",
    "\t\tprint(\"--- Input size: \",str(self.hidden_layers[0].shape[0]-1))\n",
    "\t\tprint(\"--- Number of hidden layers: \",str(self.n_hidden_layers))\n",
    "\t\tprint(\"--- Number of perceptrons at each layer: \")\n",
    "\t\tfor l in range(self.n_hidden_layers):\n",
    "\t\t\tprint(\"------ HL \"+str(l+1)+\": \"+str(self.hidden_layers[l].shape[1]))\n",
    "\t\tprint(\"--- Number of classes: \"+str(self.output_layer.shape[1]),\"\\n\")\n",
    "\n",
    "\tdef foward_propagation(self, X):\n",
    "\n",
    "\t\tdel self.activations[:]\n",
    "\n",
    "\t\tself.activations.append(X)\n",
    "\t\tinp = np.insert(X,0,1,axis=1)\n",
    "\n",
    "\t\tfor l in range(self.n_hidden_layers):\n",
    "\t\t\tout = np.matmul(inp, self.hidden_layers[l])\n",
    "\t\t\tself.dot_product.append(out)\n",
    "\t\t\tsig = self.act_func(out)\n",
    "\t\t\tself.activations.append(sig)\n",
    "\t\t\tinp = np.insert(sig,0,1,axis=1)\t\t\t\n",
    "\n",
    "\t\tout = np.matmul(inp, self.output_layer)\n",
    "\t\tself.dot_product.append(out)\n",
    "\t\tsig = self.act_func(out)\n",
    "\t\tself.activations.append(sig)\n",
    "\n",
    "\t\treturn sig\n",
    "\n",
    "\tdef backpropagation(self, x, y):\n",
    "\t\tsig = self.foward_propagation([x])\n",
    "\t\t#dErr/dAct * dAct/dDot\n",
    "\t\tdelta1 = self.err_derivative(sig,y) * self.act_derivative(self.dot_product[-1])\t\n",
    "\t\n",
    "\t\t#input current layer = output previous layer (inserting bias)\n",
    "\t\tact = np.insert(self.activations[-2],0,1,axis=1).transpose()\n",
    "\n",
    "\t\t#dErr/dAct * dAct/dDot * dDot/dWl\n",
    "\t\tgrad1 = np.matmul(act,delta1)\n",
    "\n",
    "\t\t#Layer (l-1)\n",
    "\n",
    "\t\t#dDotK/dAct (no bias)\n",
    "\t\tdot_act = self.output_layer[1:]\n",
    "\t\t#dAct/dDot\n",
    "\t\tact_dot = self.act_derivative(self.dot_product[-2])\n",
    "\t\tdelta2 = np.multiply(dot_act, act_dot.transpose())\n",
    "\t\tdelta12 = np.multiply(delta1,delta2)\n",
    "\t\tdelta12s = delta12.sum(axis=1)\n",
    "\t\tdelta12s = np.expand_dims(delta12s, axis=0)\n",
    "\n",
    "\t\tact = np.insert(self.activations[-3],0,1,axis=1).transpose()\n",
    "\t\tgrad2 = np.multiply(delta12s, act)\n",
    "\n",
    "\t\tgrad3 = None\n",
    "\t\t#Layer (l-2)\n",
    "\t\tif(len(self.hidden_layers) > 1):\n",
    "\t\t\t#dDotK/dAct (no bias)\n",
    "\t\t\tdot_act = self.hidden_layers[-1][1:]\n",
    "\t\t\t#dAct/dDot\n",
    "\t\t\tact_dot = self.act_derivative(self.dot_product[-3])\n",
    "\t\t\tdelta3 = np.multiply(dot_act, act_dot.transpose())\t\t\n",
    "\n",
    "\t\t\tdelta123 = np.array([])\n",
    "\t\t\tfor l,c in zip(delta12,delta3.transpose()):\n",
    "\t\t\t\tl = np.expand_dims(l,axis=0)\n",
    "\t\t\t\tc = np.expand_dims(c,axis=1)\n",
    "\t\t\t\tmatrix = np.matmul(c,l)\n",
    "\t\t\t\tdelta123 = delta123+matrix if delta123.size else matrix\n",
    "\t\t\tdelta123s = delta123.sum(axis=1)\n",
    "\n",
    "\t\t\tact = np.insert([x],0,1,axis=1).transpose()\n",
    "\t\t\tgrad3 = np.multiply(delta123s,act)\n",
    "\t\t\n",
    "\t\treturn grad1,grad2,grad3\n",
    "\n",
    "\n",
    "\tdef stochastic_training(self, X, Y, alpha, epochs):\n",
    "\t\tJ = []\n",
    "\t\tfor e in range(epochs):\n",
    "\t\t\tfor x,y in zip(X,Y):\n",
    "\t\t\t\tgrad1, grad2, grad3 = self.backpropagation(x,y)\t\t\t\n",
    "\t\t\t\n",
    "\t\t\t\t#update weights\n",
    "\t\t\t\tself.output_layer      = self.output_layer      - alpha*grad1\n",
    "\t\t\t\tself.hidden_layers[-1] = self.hidden_layers[-1] - alpha*grad2\n",
    "\t\t\t\tif(len(self.hidden_layers) > 1): self.hidden_layers[-2] = self.hidden_layers[-2] - alpha*grad3\n",
    "\n",
    "\t\t\t\tsig = self.foward_propagation([x])\n",
    "\t\t\t\terr = self.err_func(sig,y)\n",
    "\t\t\t\tJ.append(err)\n",
    "\t\t\t\t\n",
    "\t\tplt.plot(J)\t\n",
    "\t\tplt.ylabel('Error')\n",
    "\t\tplt.xlabel('iterations')\n",
    "\t\tplt.show()\n",
    "\n",
    "\tdef batch_training(self, X, Y, alpha, epochs):\n",
    "\t\tJ = []\n",
    "\t\tfor e in range(epochs):\n",
    "\t\t\tgrad1s, grad2s, grad3s = np.array([]),np.array([]),np.array([])\n",
    "\t\t\t\n",
    "\t\t\tfor x,y in zip(X,Y):\n",
    "\t\t\t\tgrad1, grad2, grad3 = self.backpropagation(x,y)\t\t\t\n",
    "\t\t\t\tgrad1s = grad1s+grad1 if grad1s.size else grad1\n",
    "\t\t\t\tgrad2s = grad2s+grad2 if grad2s.size else grad2\n",
    "\t\t\t\tif grad3: grad3s = grad3s+grad3 if grad3s.size else grad3\n",
    "\t\t\t\n",
    "\n",
    "\t\t\tgrad1s = grad1s/X.shape[0]\n",
    "\t\t\tgrad2s = grad2s/X.shape[0]\n",
    "\t\t\tgrad3s = grad3s/X.shape[0]\n",
    "\t\t\t\n",
    "\t\t\t#update weights\t\t\t\n",
    "\t\t\tself.output_layer      = self.output_layer      - alpha*grad1s\n",
    "\t\t\tself.hidden_layers[-1] = self.hidden_layers[-1] - alpha*grad2s\n",
    "\t\t\tif(len(self.hidden_layers) > 1): self.hidden_layers[-2] = self.hidden_layers[-2] - alpha*grad3s\n",
    "\t\t\tprint(self.hidden_layers[-1])\n",
    "\n",
    "\t\t\tsig = self.foward_propagation(X)\n",
    "\t\t\terr = self.err_func(sig,Y)\n",
    "\t\t\tj = err.sum()/X.shape[0]\t\t\t\n",
    "\t\t\tJ.append(j)\n",
    "\n",
    "\t\t\tprint(\"Epoch:\",e,\"error:\",j)\n",
    "\t\t\t\t\n",
    "\t\tplt.plot(J)\t\n",
    "\t\tplt.ylabel('Error')\n",
    "\t\tplt.xlabel('iterations')\n",
    "\t\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer  1\n",
      "[[0.84379971 0.15851025]\n",
      " [0.31700029 0.87052709]\n",
      " [0.02541348 0.99259214]] \n",
      "\n",
      "Output Layer \n",
      "[[0.50301023 0.78118819]\n",
      " [0.71366618 0.5281667 ]\n",
      " [0.30889682 0.87353862]] \n",
      "\n",
      "--- Input size:  2\n",
      "--- Number of hidden layers:  1\n",
      "--- Number of perceptrons at each layer: \n",
      "------ HL 1: 2\n",
      "--- Number of classes: 2 \n",
      "\n",
      "Y\n",
      " [0.79370361 0.88296932]\n",
      "\n",
      "a(Y)\n",
      " [0.80722329 0.89215366]\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------\n",
    "#        ANN Initialization\n",
    "#----------------------------------\n",
    "\n",
    "# Random Init\n",
    "teste = ANN(\"act_Sigmoid\", 'err_SMD')\n",
    "\n",
    "X,X_test,y,y_test = custom_SGD.get_data_multiclass(None)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "teste.initialize_random_weights(X_train.shape[1], [4], len(np.unique(y)))\n",
    "teste.show_weights()\n",
    "teste.show_setup()\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_train)\n",
    "y_train = lb.transform(y_train)\n",
    "\n",
    "teste.batch_training(X_train,y_train,1.2,5)\n",
    "\n",
    "sig = teste.foward_propagation(X_train)\n",
    "print(sig.shape)\n",
    "print(sig.head(10))\n",
    "y_pred = np.argmax(sig,axis=1)\n",
    "\n",
    "print(y_pred.shape)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "print(y_val)\n",
    "\n",
    "evalute_multiclass(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
