{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " Since we are dealing with logistic regression,\n",
    " the hypothesis is defined as:\n",
    "\n",
    "                    1\n",
    "       F(x) = ----------------\n",
    "                1 + exp^(-x)\n",
    "\n",
    " However, its implementation may result in overflow\n",
    " if x is too large, then, the version implemented \n",
    " here is more stable with similar results, and is\n",
    " defined as:\n",
    " \n",
    "                  exp^(x)\n",
    "       F(x) = ----------------, if x < 0\n",
    "                1 + exp^(x) \n",
    "                \n",
    "                    1\n",
    "       F(x) = ----------------, if x >= 0\n",
    "                1 + exp^(-x) \n",
    "'''\n",
    "def hypothesis(theta,X,stable=False):\n",
    "    \n",
    "    dot = np.dot(X,theta)\n",
    "    \n",
    "    #Regular Sigmoid Function        \n",
    "    if (stable == False):        \n",
    "        h = 1 / (1 + np.exp(-dot))\n",
    "    \n",
    "    else:\n",
    "    #Stable Sigmoid Function\n",
    "        num = (dot >= 0).astype(np.float128)\n",
    "        dot[dot >= 0] = -dot[dot >= 0]\t\n",
    "        exp = np.exp(dot)\n",
    "        num = np.multiply(num,exp)\n",
    "        h = num / (1 + exp)\n",
    "    \n",
    "    return h\n",
    "\n",
    "# Apply a multi class classification of the samples\n",
    "# regarding an optimized theta\n",
    "# ToDo\n",
    "def classify_multiclass(theta , X , th, nmodels):\n",
    "    # inserting the X^0 coeficient\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    # Given a theta collection calculate the hyphotesis\n",
    "    y = hypothesis(theta, X)\n",
    "    y[y >= th] = 1\n",
    "    y[y < th] = 0\n",
    "    X = np.delete(X, 0, axis=1)\n",
    "    return y\n",
    "\n",
    "# Given a threshold apply a \n",
    "# binary classification of the samples\n",
    "# regarding an optimized theta\n",
    "def classify(theta, X, th):\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    y = hypothesis(theta, X)\n",
    "    y[y >= th] = 1\n",
    "    y[y < th] = 0\n",
    "    X = np.delete(X, 0, axis=1)\n",
    "    return y\n",
    "\n",
    "def predict(theta,X):\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    y = hypothesis(theta, X)\n",
    "    X = np.delete(X, 0, axis=1)\n",
    "    return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------\n",
    "#   Evaluation Metrics and Loss Functions\n",
    "#-----------------------------------\n",
    "\n",
    "def cross_entropy_loss(h, y):\n",
    "    # y.log(h) + (1-log(h) . 1-y)\n",
    "    # log probability * inverse of the log probabality \n",
    "\teps = np.finfo(np.float).eps\n",
    "\th[h < eps] = eps\n",
    "\th[h > 1.-eps] = 1.-eps\n",
    "\treturn np.multiply(np.log(h),y) + np.multiply((np.log(1-h)),(1-y))\n",
    "\n",
    "def cost(theta, X, y):\n",
    "    h = hypothesis(theta, X)\n",
    "    cost = cross_entropy_loss(h, y)\n",
    "    mean_cost = cost.sum() / -y.shape[0]\n",
    "    return mean_cost\n",
    "\n",
    "def accuracy_score(predY, Y):\n",
    "    TP = ((predY == Y) & (predY == 1.)).sum()\n",
    "    TN = ((predY == Y) & (predY == 0.)).sum()\n",
    "    acc = (TP + TN) / predY.shape[0]\n",
    "    return acc\n",
    "\n",
    "def precision_score(predY,Y):\n",
    "    TP = ((predY == Y) & (predY == 1)).sum()\n",
    "    FP = ((predY != Y) & (predY == 1)).sum()\n",
    "    precision = TP / (TP + FP)\n",
    "    return precision\n",
    "\n",
    "def recall_score(predY, Y):\n",
    "    TP = ((predY == Y) & (predY == 1)).sum()\n",
    "    FN = ((predY != Y) & (predY == 0)).sum()\n",
    "    recall = TP / (TP + FN)\n",
    "    return recall\n",
    "\n",
    "def f1_score(predY, Y, beta):\n",
    "    precision = precision_score(predY, Y)\n",
    "    recall = recall_score(predY, Y)\n",
    "    fscore = (1 + beta * beta) * ((precision * recall) / ((beta * beta * precision) + recall))\n",
    "    return fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------\n",
    "#   Gradient Descent\n",
    "#-----------------------------------\n",
    "\n",
    "def BGD(X, y, alpha, iterations, lr_optimizer='invscaling', power_t=0.25, t=1.0):\n",
    "    \n",
    "    print (\"Starting BGD...\")\n",
    "    \n",
    "    print (\"Parameters lr_optimizer {}, power_t {}, t {}\".format(lr_optimizer, power_t, t))\n",
    "    \n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "    nsamples = X.shape[0]\n",
    "    nfeatures = X.shape[1]\n",
    "    theta = np.zeros(nfeatures)\n",
    "    J=[]\n",
    "\n",
    "    eta = alpha            \n",
    "    for i in range(iterations):\n",
    "\n",
    "        \n",
    "        if lr_optimizer:            \n",
    "            eta = alpha / ((i / y.shape[0]) + 1) * pow(t, power_t)\n",
    "        \n",
    "\n",
    "        h = hypothesis(theta,X)\n",
    "\n",
    "        error = h - y\n",
    "\n",
    "        grad = np.dot(X.transpose(),error)/nsamples\n",
    "\n",
    "        theta = theta - eta * grad\n",
    "\n",
    "        cost_  = cost(theta,X,y)\n",
    "        J.append(cost_)\n",
    "        if (i % 100) == 0:\n",
    "            print (\"Iteration {} cost {:.2f} error {:.2f} lr {:.6f}\".format(i, cost_, error.mean()**(1/2), eta))\n",
    "\n",
    "    X = np.delete(X,0,axis=1)\n",
    "    plt.figure()\n",
    "    plt.plot(J)    \n",
    "    plt.ylabel('Error')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.show()\n",
    "\n",
    "    return theta, J[iterations-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- X\n",
      "[[  74   74   74]\n",
      " [  36   36   36]\n",
      " [  93   93   93]\n",
      " ...\n",
      " [4900 4900 4900]\n",
      " [7000 7000 7000]\n",
      " [2400 2400 2400]]\n",
      "\n",
      "--- y\n",
      "[0. 0. 0. ... 1. 1. 1.]\n",
      "Starting BGD...\n",
      "Parameters lr_optimizer invscaling, power_t 0.25, t 1.0\n",
      "Iteration 0 cost 18.02 error 0.00 lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 cost 18.02 error 0.71 lr 0.098039\n",
      "Iteration 200 cost 18.02 error 0.71 lr 0.096154\n",
      "Iteration 300 cost 18.02 error 0.71 lr 0.094340\n",
      "Iteration 400 cost 18.02 error 0.71 lr 0.092593\n",
      "Iteration 500 cost 18.02 error 0.71 lr 0.090909\n",
      "Iteration 600 cost 18.02 error 0.71 lr 0.089286\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-9c539084a3df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Theta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-090c62169a2b>\u001b[0m in \u001b[0;36mBGD\u001b[0;34m(X, y, alpha, iterations, lr_optimizer, power_t, t)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mcost_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mJ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-3287de56707c>\u001b[0m in \u001b[0;36mcost\u001b[0;34m(theta, X, y)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmean_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmean_cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-3287de56707c>\u001b[0m in \u001b[0;36mcross_entropy_loss\u001b[0;34m(h, y)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#-----------------------------------\n",
    "#   Logistic Regression - Toy Example\n",
    "#-----------------------------------\n",
    "\n",
    "size=5000\n",
    "size2=size//2\n",
    "size10=size//10\n",
    "size102=size10//2\n",
    "\n",
    "X = np.random.randint(low=1, high=99, size=(size,1))\n",
    "X = np.concatenate((X, X, X), axis=1)\n",
    "X = np.multiply(X,np.concatenate((np.full((size2,1),1), np.full((size2,1),100))))\n",
    "\n",
    "y = np.empty((size))\n",
    "y[X[:,1] <= 99]=0\n",
    "y[X[:,1] >= 100]=1\n",
    "\n",
    "print(\"\\n--- X\")\n",
    "print(X)\n",
    "print(\"\\n--- y\")\n",
    "print(y)\n",
    "\n",
    "theta,error = BGD(X, y, 0.01, 10000)\n",
    "print(\"\\n--- Theta\")\n",
    "print(theta)\n",
    "print(\"\\n--- Error\")\n",
    "print(error)\n",
    "\n",
    "\n",
    "X_val = np.random.randint(low=1, high=9900, size=(size10,1))\n",
    "X_val = np.concatenate((X_val, X_val, X_val),axis=1)\n",
    "y_val = np.empty((size10))\n",
    "y_val[X_val[:,1] <= 99]=0\n",
    "y_val[X_val[:,1] >= 100]=1\n",
    "\n",
    "\n",
    "h = predict(theta,X_val)\n",
    "print(\"\\n--- Hypothesis\")\n",
    "print(h)\n",
    "predY = classify(theta,X_val,0.7)\n",
    "print(\"\\n--- Classification\")\n",
    "print(predY)\n",
    "print(\"\\n--- Expected Output\")\n",
    "print(y_val)\n",
    "\n",
    "acc = accuracy_score(predY, y_val)\n",
    "pre = precision_score(predY, y_val)\n",
    "recall = recall_score(predY, y_val)\n",
    "f = f1_score(predY, y_val, 1)\n",
    "\n",
    "print(\"\\n--- Accuracy\")\n",
    "print(acc)\n",
    "print(\"\\n--- Precision\")\n",
    "print(pre)\n",
    "print(\"\\n--- Recall\")\n",
    "print(recall)\n",
    "print(\"\\n--- F1Score\")\n",
    "print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------\n",
    "# MultiClass Classification - Toy Example\n",
    "#-----------------------------------\n",
    "\n",
    "# Not ready, still has a bug\n",
    "\n",
    "size=500\n",
    "size4=size//4\n",
    "size10=size//10\n",
    "size104=size10//4\n",
    "X = np.random.randint(low=0,high=7999, size=(size,1))\n",
    "X = np.concatenate((X,X,X),axis=1)\n",
    "y = np.empty((size),dtype=np.float128)\n",
    "y[X[:,1] <= 1999]=0\n",
    "y[((X[:,1] <= 3999) & (X[:,1]>1999))]=1\n",
    "y[((X[:,1] <= 5999) & (X[:,1]>3999))]=2\n",
    "y[X[:,1]>5999]=3\n",
    "\n",
    "classes = np.unique(y)\n",
    "print(classes)\n",
    "theta = {}\n",
    "\n",
    "for c in classes:\n",
    "\n",
    "\tcy = np.copy(y)\n",
    "\n",
    "\tcy[y != c] = 0\n",
    "\tcy[y == c] = 1\n",
    "\n",
    "\ttheta[c],acc = BGD(X,cy,0.001,500)\n",
    "\n",
    "X_val = np.random.randint(low=1,high=7999, size=(size10,1))\n",
    "X_val = np.concatenate((X_val,X_val,X_val),axis=1)\n",
    "y_val = np.empty((size10))\n",
    "y_val[X_val[:,1] <= 1999]=0\n",
    "y_val[((X_val[:,1] <= 3999) & (X_val[:,1]>1999))]=1\n",
    "y_val[((X_val[:,1] <= 5999) & (X_val[:,1]>3999))]=2\n",
    "y_val[X_val[:,1]>5999]=3\n",
    "\n",
    "print(\"\\n--- Theta\")\n",
    "print(theta)\n",
    "\n",
    "predY, pred = classify_multiclass(theta,X_val)\n",
    "print(\"\\n--- Hypothesis\")\n",
    "print(pred)\n",
    "print(\"\\n--- Classification\")\n",
    "print(predY)\n",
    "print(\"\\n--- Expected Output\")\n",
    "print(y_val)\n",
    "\n",
    "acc = accuracy_score(predY, y_val)\n",
    "pre = precision_score(predY, y_val)\n",
    "recall = recall_score(predY, y_val)\n",
    "f = f1_score(predY, y_val, 1)\n",
    "\n",
    "print(\"\\n--- Accuracy\")\n",
    "print(acc)\n",
    "print(\"\\n--- Precision\")\n",
    "print(pre)\n",
    "print(\"\\n--- Recall\")\n",
    "print(recall)\n",
    "print(\"\\n--- F1Score\")\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
