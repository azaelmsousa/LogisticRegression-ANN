{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows the development of an Artificial Neural Network with focus on classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.datasets as sk_datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function:\n",
    "\t\n",
    "\t#\n",
    "\t# Sigmoid function for the activation\n",
    "\t# of a neuron, where h is the dot\n",
    "\t# product of X (input) and theta (weights)\n",
    "\t#\n",
    "\tdef act_Sigmoid(h):\n",
    "\t\tsig = 1. / (1. + np.exp(-h))\n",
    "\t\treturn sig\n",
    "\n",
    "\t#\n",
    "\t# Derivative of the sigmoid function. It\n",
    "\t# is used as part of the backpropagation\n",
    "\t# algorithm\n",
    "\t#\n",
    "\tdef act_Sigmoid_derivative(h):\n",
    "\t\tsig = act_Sigmoid(h)\n",
    "\t\tderivative = sig*(1-sig)\n",
    "\t\treturn derivative\n",
    "\n",
    "\t#\n",
    "\t# Hyberbolic tangent function for the\n",
    "\t# acivation of a neuron, where h is the\n",
    "\t# dot product of X (input) and theta (weights)\n",
    "\t#\t\n",
    "\tdef act_Tanh(h):\n",
    "\t\ttanh = (2 / (1+np.exp(-2*h)))-1\n",
    "\n",
    "\t#\n",
    "\t# Deivative of the hyperbolic tangent\n",
    "\t# function. It is used as part of the\t\n",
    "\t# back propagation algorithm.\n",
    "\t#\n",
    "\tdef act_Tanh_derivative(h):\n",
    "\t\ttanh_l = (4*np.exp(-2*h))/((1+np.exp(-2*h))**2)\n",
    "\t\treturn tanh_l\n",
    "\n",
    "\t#\n",
    "\t# Cross entropy loss function, where h\n",
    "\t# is the activation of the last layer.\n",
    "\t# It computes the error of the predicted\n",
    "\t# class and the correct one.\n",
    "\t#\n",
    "\tdef err_CrossEntropy(predY,y):\n",
    "\t\teps = np.finfo(np.float128).eps\n",
    "\t\tpredY[predY < eps] = eps\n",
    "\t\tpredY[predY > 1.-eps] = 1.-eps\n",
    "\t\treturn -np.multiply(np.log(predY),y) - np.multiply((np.log(1-predY)),(1-y))\n",
    "\n",
    "\t#\n",
    "\t# Derivative of the SMD loss function.\n",
    "\t# It is used in the back propagation\n",
    "\t# algorithm.\n",
    "\t#\n",
    "\tdef err_CrossEntropy_derivative(X,predY,y):\n",
    "\t\terror = (predY - y)\n",
    "\t\tgrad = np.dot(X.transpose(),error)\n",
    "\t\treturn grad\n",
    "\n",
    "\t#\n",
    "\t# Sum of the squared differences (SMD) loss\n",
    "\t# function, where h is the activation of the\n",
    "\t# last layer. It computes the error of the\n",
    "\t# predicted class and the correct one.\n",
    "\t#\n",
    "\tdef err_SMD(predY,y):\n",
    "\t\terror = np.square((predY - y)).sum()\n",
    "\t\treturn error/2\n",
    "\n",
    "\t#\n",
    "\t# Sum of the squared differences (SMD) loss\n",
    "\t# function, where h is the activation of the\n",
    "\t# last layer. It computes the error of the\n",
    "\t# predicted class and the correct one.\n",
    "\t#\n",
    "\tdef err_SMD_derivative(X,predY,y):\n",
    "\t\terror = (predY - y)\n",
    "\t\tgrad = np.dot(X.transpose(),error)\n",
    "\t\treturn grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "\n",
    "\tdef __init__(self, activation):\n",
    "\t\tself.act_func = Function.__dict__[activation]\n",
    "\t\tself.act_derivative = Function.__dict__[activation+\"_derivative\"]\n",
    "\n",
    "\tdef initialize_random_weights(self, n_input, n_perceptron, n_classes):\n",
    "\t\tself.n_hidden_layers = len(n_perceptron)\n",
    "\n",
    "\t\tself.hidden_layers = []\n",
    "\t\tself.activation = []\n",
    "\n",
    "\t\tfor l in range(self.n_hidden_layers):\n",
    "\t\t\tif (l == 0):\n",
    "\t\t\t\tw = np.random.rand(n_input+1,n_perceptron[0])\n",
    "\t\t\telse:\n",
    "\t\t\t\tw = np.random.rand(n_perceptron[l-1]+1,n_perceptron[l])\n",
    "\t\t\tself.hidden_layers.append(w)\n",
    "\n",
    "\t\tif (self.n_hidden_layers == 0):\n",
    "\t\t\tself.output_layer = np.random.rand(n_input+1,n_classes)\n",
    "\t\telse:\n",
    "\t\t\tself.output_layer = np.random.rand(n_perceptron[self.n_hidden_layers-1]+1,n_classes)\n",
    "\n",
    "\n",
    "\tdef initialize_fixed_weights(self, w):\n",
    "\t\tself.hidden_layers = w[:-1]\n",
    "\t\tself.output_layer = w[-1]\n",
    "\t\tself.n_hidden_layers = len(w)-1\n",
    "\n",
    "\n",
    "\tdef show_weights(self):\n",
    "\n",
    "\t\tfor l in range(self.n_hidden_layers):\n",
    "\t\t\tprint(\"Hidden Layer \",str(l+1))\n",
    "\t\t\tprint(self.hidden_layers[l],\"\\n\")\n",
    "\n",
    "\t\tprint(\"Output Layer \")\n",
    "\t\tprint(self.output_layer,\"\\n\")\n",
    "\n",
    "\tdef show_setup(self):\n",
    "\t\t\t\t\n",
    "\t\tprint(\"--- Input size: \",str(self.hidden_layers[0].shape[0]-1))\n",
    "\t\tprint(\"--- Number of hidden layers: \",str(self.n_hidden_layers))\n",
    "\t\tprint(\"--- Number of perceptrons at each layer: \")\n",
    "\t\tfor l in range(self.n_hidden_layers):\n",
    "\t\t\tprint(\"------ HL \"+str(l+1)+\": \"+str(self.hidden_layers[l].shape[1]))\n",
    "\t\tprint(\"--- Number of classes: \"+str(self.output_layer.shape[1]),\"\\n\")\n",
    "\n",
    "\tdef foward_propagation(self, X):\n",
    "\n",
    "\t\tinp = np.insert(X,0,1,axis=1)\n",
    "\n",
    "\t\tfor l in range(self.n_hidden_layers):\n",
    "\t\t\tout = np.matmul(inp, self.hidden_layers[l])\n",
    "\t\t\tsig = self.act_func(out)\n",
    "\t\t\tinp = np.insert(sig,0,1,axis=1)\n",
    "\n",
    "\t\tout = np.matmul(inp, self.output_layer)\n",
    "\t\tsig = self.act_func(out)\n",
    "\n",
    "\t\treturn out,sig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer  1\n",
      "[[0.02814177 0.15377794]\n",
      " [0.94466117 0.16724318]\n",
      " [0.70835274 0.26635948]] \n",
      "\n",
      "Output Layer \n",
      "[[0.61027349 0.4320716 ]\n",
      " [0.58187071 0.94202937]\n",
      " [0.43493681 0.22798961]] \n",
      "\n",
      "--- Input size:  2\n",
      "--- Number of hidden layers:  1\n",
      "--- Number of perceptrons at each layer: \n",
      "------ HL 1: 2\n",
      "--- Number of classes: 2 \n",
      "\n",
      "Y\n",
      " [[1.44832431 1.45489365]\n",
      " [1.55913052 1.56433779]]\n",
      "\n",
      "a(Y)\n",
      " [[0.80974041 0.81075043]\n",
      " [0.82622855 0.82697492]]\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------\n",
    "#        ANN Initialization\n",
    "#----------------------------------\n",
    "\n",
    "# Random Init\n",
    "teste = ANN(\"act_Sigmoid\")\n",
    "teste.initialize_random_weights(2, [2], 2)\n",
    "teste.show_weights()\n",
    "teste.show_setup()\n",
    "\n",
    "'''\n",
    "# Fixed Init\n",
    "w1 = np.array([[1,2],[3,4],[5,6]])\n",
    "w2 = np.array([[7,8],[9,10],[11,12]])\n",
    "w = [w1,w2]\n",
    "\n",
    "teste = ANN(\"act_Sigmoid\")\n",
    "teste.initialize_fixed_weights(w)\n",
    "teste.show_weights()\n",
    "teste.show_setup()\n",
    "'''\n",
    "\n",
    "#----------------------------------\n",
    "#           Toy Examples\n",
    "#----------------------------------\n",
    "\n",
    "X = np.array([[1,2],[3,4]])\n",
    "Y, aY = teste.foward_propagation(X)\n",
    "print(\"Y\\n\",Y)\n",
    "print(\"\\na(Y)\\n\",aY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
